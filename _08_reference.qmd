# Metrics to Evaluate Machine Model Performance

| Technique/Metric | Description | Purpose/Formula | Scenario: Cancer prediction |
|------------------|-------------|-----------------|-------------------|
| 1. Train-Test Split | Split the dataset into training and testing subsets | Assess model performance on unseen data to detect overfitting and ensure generalizability | Always used; crucial for unbiased evaluation of model performance |
| 2. Cross-Validation | Divide data into k subsets and train the model k times, using a different subset as test set each time | Provides robust estimate of model performance by averaging results over multiple splits | Useful for smaller datasets or when data collection is expensive (e.g., rare cancer types) |
| 3. Confusion Matrix | Table comparing predicted and actual values in classification | Metrics: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN) | Fundamental for understanding model performance in classification tasks, like cancer detection |
| 4. Accuracy | Ratio of correctly predicted instances to total instances | $\frac{TP + TN}{TP + TN + FP + FN}$ | Used when classes are balanced; less suitable for rare cancer detection due to class imbalance |
| 5a. Precision | Ratio of correctly predicted positive observations to total predicted positives | $\frac{TP}{TP + FP}$ | Important when false positives are costly (e.g., unnecessary biopsies or treatments) |
| 5b. Recall (Sensitivity) | Ratio of correctly predicted positive observations to all actual positive observations | $\frac{TP}{TP + FN}$ | Critical in cancer detection to minimize false negatives (missed cancer cases) |
| 5c. F1-Score | Harmonic mean of Precision and Recall | $2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ | Balances precision and recall; useful when seeking a compromise between false positives and false negatives |
| 6. ROC Curve and AUC | ROC: Graph of true positive rate vs false positive rate at various thresholds. AUC: Area under ROC curve | Higher AUC indicates better model performance | Useful for comparing models and choosing optimal threshold, especially in diagnostic tests |
| 7. Mean Absolute Error (MAE) | Average of absolute differences between predicted and actual values | $\frac{1}{n} \sum_{i=1}^{n} \|y_i - \hat{y}_i\|$ | Used in regression tasks, e.g., predicting survival time; less sensitive to outliers than MSE |
| 8a. Mean Squared Error (MSE) | Average of squared differences between predicted and actual values | $\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ | Used in regression; penalizes large errors more, suitable when large errors are particularly undesirable |
| 8b. Root Mean Squared Error (RMSE) | Square root of MSE | $\sqrt{\text{MSE}}$ | Same as MSE, but in the original unit of the target variable, making it more interpretable |
| 9. R-squared | Proportion of variance in dependent variable predictable from independent variables | $1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$ | Used in regression to assess overall fit; indicates how well the model explains the variance in the data |
| 10a. Akaike Information Criterion (AIC) | Measures relative quality of statistical model for given data | $2k - 2\ln(L)$ where $k$ is number of parameters and $L$ is likelihood | Used for model selection; helps prevent overfitting by penalizing complex models |
| 10b. Bayesian Information Criterion (BIC) | Similar to AIC but with stronger penalty term for number of parameters | $k\ln(n) - 2\ln(L)$ where $n$ is number of observations | Also used for model selection; tends to favor simpler models compared to AIC |