{
  "hash": "76b63eb3f428815efb7afffe7737befe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle-block-banner: true\ntitle: \"Unveiling the Drivers of Clean Air\"\nsubtitle: \"Analysis of Portland Air Quality (AQI)\"\ndate: \"2024-07-30\"\nformat:\n  html:\n    code-fold: true\n    code-tools:\n      source: repo\n      toggle: true\n      caption: none\n    smooth-scroll: true\n    toc: true\n    toc-title: Contents\n    number-sections: false\n    other-links:\n      - text: Airnow Open Data\n        href: https://www.airnow.gov\n    code-links:\n      - text: Sweetviz Analysis Report\n        icon: file-code\n        href: SWEETVIZ_REPORT.html\n    html-math-method: mathjax\n    css: styles.css\nexecute:\n  echo: false\n  warning: false\njupyter: python3\n---\n\n# Introduction\n\nOn October 30, 1948, the Donora High School Football team played through a dense smog to complete the game with hundreds of fans in the audience, despite very poor visibility. The team, fueled by resilience, took pride in playing through poor conditions, a testament to the high spirit of the small town. Soon after, calls to the town’s medical offices began flooding in, complaining of difficulty breathing and respiratory issues. Donora, Pennsylvania, was a town of metalworks, built by the American Steel and Wire company and the Donora Zinc Works company, which made up major parts of the town’s economy. The heavy smog and pollution clouds that covered the sky had been viewed as a sign of prosperity, owing to the industrial might that powered their economy. Within just twelve hours, seventeen people would be dead, 1440 seriously affected, and 4470 with mild to moderate conditions---almost half the town’s working population (Jacobs, Burgess, Abbott). \n\nThis event, known as the Donora Smog of 1948, prompted the country into taking a closer look at the negative impacts of air pollution. Widespread debate surrounding the event led to the first legislation aimed at regulating the air quality within the United States, ushering in a new era of tracking, combatting, and reversing the ill effects of poor air quality. \n\nThe quality of air we breathe has direct impacts on our health. We must understand the factors that contribute to poor air quality and how we individually and collectively contribute to these changes. Until we can visualize the impact we have on our atmosphere, we will continue behavior that negatively impacts the air around us. \n\nIn this project, we will focus on the key factors influencing air quality in Portland, Oregon. We aim to understand the complex interplay between various environmental and human-made factors that contribute to high air pollution levels. Initially, we were surprised to discover that Portland, the city we reside in, has some of the best air quality for a city of its size in the United States. This led us to narrow our focus to understanding the factors that contribute to these favorable outcomes in this city.\n\nOur project aims to develop and validate machine learning models to analyze the various factors influencing air quality. By focusing on Portland in comparison to other large US cities, we hope to find things Portland does that lead to the greater AQI outcomes. Our approach will consider a range of variables, including meteorological conditions, pollution sources, and transit systems. Through this analysis, we aim to provide actionable insights and recommendations for sustaining and improving air quality. By examining both the contributors to clean air and the sources of pollution, we can understand the factors affecting air quality and develop comprehensive strategies for enhancement.\n\n\n# Background\n\nFederal regulation of air quality in the United States began in 1955 with the Air Pollution Control Act. This new piece of legislation provided funding for initial research into air quality and pollution in the US. Building off this and privately funded research, Congress passed the Clean Air Act of 1963, establishing the first federal regulation for controlling air pollution. This act established a new federal program within the US Public Health Service, dedicated to the monitoring and control of air quality. In 1967, Congress passed the Air Quality Act, which introduced more federal oversight and enforcement policies, allowing extensive monitoring of interstate air pollution. This all led to the passage of the 1970 Clean Air Act, aimed at restricting and regulating emissions, measuring and reducing pollutant particles, and addressing upcoming pollution threats (Environmental Protection Agency).\n\nAlso established in 1970, the Environmental Protection Agency (EPA) implemented and monitored the requirements established by these rulings. The EPA's authority extended beyond federal lands and roads to include all companies operating within the United States. Enforcement authority was expanded to allow upholding these established standards, and prevent companies from circumventing the law.  Much of the improvement in the quality of air in the US over the past fifty years can be attributed to these regulations. In 1990, when deaths due to air quality were first measured, an estimated 135,000 Americans died. By 2010, that number had dropped to 71,000 (Zhang et al.). Despite the significant improvements led by the federal guidelines of the late 70s, nearly four in ten Americans still live in places where they are exposed to unhealthy air (American Lung Association).\n\nIn 1999, the EPA developed the Air Quality Index (AQI), creating an easily understood measurement of air quality. The AQI measures air pollution levels on a scale from 0 to 500, divided into six categories. A score of 0 to 50 represents good air quality which poses little or no risk to those breathing it in, while a score above 300 signifies emergency conditions, an extremely high risk which impacts everyone. This measurement is mainly derived from five major pollutants: ozone, particulate matter (2.5μm and 10μm), carbon monoxide, nitrogen dioxide, and sulfur dioxide (Airnow.gov). Poor air quality has been linked to a variety of diseases including respiratory infections, stroke, heart disease, lung cancer, and chronic obstructive pulmonary disease, among others (World Health Organization). An estimated seven million premature deaths annually can be attributed to air pollution, which equates to a global mean loss of life expectancy of 2.9 years, making it the largest environmental risk factor for disease and premature death (Fuller, Landrigan, Balakrishnan, et. al.). Thus, it is important to understand factors that contribute to poor air quality, and outcomes that can be attributed to the state of the AQI.\n\nThese harmful factors can originate from a variety of sources. Anything that releases a foreign substance into the air can lower the quality of the air. This includes smoking, vehicle exhaust, combustion processes for production and manufacturing, household cleaning products, appliances, central air and heating systems, agriculture pesticides, livestock, shipping and transportation, and much more. Individually, we can reduce our individual contributions by lowering our reliance on personal vehicles, watching our power usage, supporting companies that monitor and address their emissions, and more. However, there are many factors beyond our control. Larger pollutant sources, such as manufacturing and transportation, are often regulated to some extent but may still release significant amounts of pollutants into the atmosphere which we as individuals have no say over (Manisalidis, Stavropoulou, Stavropoulos, Bezirtzoglou). It is challenging to restrict and watch our personal contributions to the polluting of the environment without worrying about what others are doing. Measuring and analyzing the impact these pollutants have on air quality is a crucial step towards addressing these issues.\n\n\n# Methods\n\n## Tools Deployed\nPython will be the primary programming language used to conduct this analysis. We will also use R language in statistical applications where necessary.\n\nTo perform our analysis, we will employ NumPy and Pandas for data manipulation. Matplotlib and Seaborn for visualization, and Time Series forecasting algorithms such as Prophet and SARIMAX.\n\nWe will address data inconsistencies, missing values and ensure that data is in a tidy format.\n\nWe may need to normalize or standardize data if necessary and create new features through aggregation to enhance the model’s performance.\n\n## What is Prophet?\n\nProphet is an open-source forecasting tool developed by Meta, designed for forecasting time series data. It is suited for datasets with strong seasonal, monthly, weekly, or daily patterns, and it handles missing data and outliers well. We utilized prophet to gain a quick understanding of our AQI patterns, seeking to understand basic trends before conducting a more thorough analysis.\n\nKey features of Prophet include seasonality detection and holiday incorporation, while providing easy use and understanding for users. \nWe can use this software to get complex understanding from simple applications.\n\nTo conduct this analysis, we prepare data into a two column table, date and AQI. Prophet uses the trends of past data to highlight similarities over days of the year, weeks, months, and seasons. From this, prophet is able to generate its predictions, cross validate, and give performance metrics such as mean absolute percentage error to quantify the accuracy of the results.\n\n## What is SARIMAX algorithm?​​​​​​​​​​​​​​​\nThe most common method used in time series forecasting is known as the ARIMA model. We will use an extended version called SARIMAX (*Seasonal Auto Regressive Integrated Moving Averages with exogenous factor*)\n\n- The SARIMAX model is used when the data sets have seasonal cycles. \n- In the dataset concerning the air quality/AQI there is a seasonal pattern which we have explained in the above section.\n- SARIMAX is a model that can be fitted to time series data in order to better understand or predict future points in the time series\n- SARIMAX is particularly useful for forecasting time series data that exhibits both trends and seasonality.\n\nHere's a breakdown of its components:\n\nThere are three distinct integers (p,d,q) that are used to parametrize SARIMAX models. Because of that, ARIMA models are denoted with the notation SARIMAX(p,d,q).\n\nTogether these three parameters account for seasonality, trend, and noise in datasets:\n\n1. *Seasonality (S)*: Accounts for recurring patterns or cycles in the data.\n2. *AutoRegressive (AR)*: Uses past values to predict future values.\n3. *Integrated (I)*: Applies differencing to make the time series stationary.\n4. *Moving Average (MA)*: Uses past forecast errors in the prediction.\n5. *eXogenous factors (X)*: Incorporates external variables that may influence the forecast.\n\nWe are trying to find the right p, d, q hyperparameters to correctly forecast and predict the AQI values.\n\n# Metrics to Evaluate Machine Model Performance\n\n| Technique/Metric | Description | Purpose/Formula | Scenario: Cancer prediction |\n|------------------|-------------|-----------------|-------------------|\n| 1. Train-Test Split | Split the dataset into training and testing subsets | Assess model performance on unseen data to detect overfitting and ensure generalizability | Always used; crucial for unbiased evaluation of model performance |\n| 2. Cross-Validation | Divide data into k subsets and train the model k times, using a different subset as test set each time | Provides robust estimate of model performance by averaging results over multiple splits | Useful for smaller datasets or when data collection is expensive (e.g., rare cancer types) |\n| 3. Confusion Matrix | Table comparing predicted and actual values in classification | Metrics: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN) | Fundamental for understanding model performance in classification tasks, like cancer detection |\n| 4. Accuracy | Ratio of correctly predicted instances to total instances | $\\frac{TP + TN}{TP + TN + FP + FN}$ | Used when classes are balanced; less suitable for rare cancer detection due to class imbalance |\n| 5a. Precision | Ratio of correctly predicted positive observations to total predicted positives | $\\frac{TP}{TP + FP}$ | Important when false positives are costly (e.g., unnecessary biopsies or treatments) |\n| 5b. Recall (Sensitivity) | Ratio of correctly predicted positive observations to all actual positive observations | $\\frac{TP}{TP + FN}$ | Critical in cancer detection to minimize false negatives (missed cancer cases) |\n| 5c. F1-Score | Harmonic mean of Precision and Recall | $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Balances precision and recall; useful when seeking a compromise between false positives and false negatives |\n| 6. ROC Curve and AUC | ROC: Graph of true positive rate vs false positive rate at various thresholds. AUC: Area under ROC curve | Higher AUC indicates better model performance | Useful for comparing models and choosing optimal threshold, especially in diagnostic tests |\n| 7. Mean Absolute Error (MAE) | Average of absolute differences between predicted and actual values | $\\frac{1}{n} \\sum_{i=1}^{n} \\|y_i - \\hat{y}_i\\|$ | Used in regression tasks, e.g., predicting survival time; less sensitive to outliers than MSE |\n| 8a. Mean Squared Error (MSE) | Average of squared differences between predicted and actual values | $\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ | Used in regression; penalizes large errors more, suitable when large errors are particularly undesirable |\n| 8b. Root Mean Squared Error (RMSE) | Square root of MSE | $\\sqrt{\\text{MSE}}$ | Same as MSE, but in the original unit of the target variable, making it more interpretable |\n| 9. R-squared | Proportion of variance in dependent variable predictable from independent variables | $1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$ | Used in regression to assess overall fit; indicates how well the model explains the variance in the data |\n| 10a. Akaike Information Criterion (AIC) | Measures relative quality of statistical model for given data | $2k - 2\\ln(L)$ where $k$ is number of parameters and $L$ is likelihood | Used for model selection; helps prevent overfitting by penalizing complex models |\n| 10b. Bayesian Information Criterion (BIC) | Similar to AIC but with stronger penalty term for number of parameters | $k\\ln(n) - 2\\ln(L)$ where $n$ is number of observations | Also used for model selection; tends to favor simpler models compared to AIC |\n\n# Machine Learning AQI Time Series\n## How can we use Akaike Information Criteria (AIC)?\nUsed to measure of a statistical model, it quantifies:\n\n- The goodness of fit\n- The simplicity of the model into a single statistic\n- When comparing two models, the one with the lower AIC is generally \"better\"\n\nThe Akaike Information Criterion (AIC) is a measure used to compare different statistical models. It helps in model selection by balancing the goodness of fit and the complexity of the model. Here's how to interpret the AIC value:\n\n- *Lower AIC is Better*: A lower AIC value indicates a better-fitting model. It means the model has a good balance between accuracy and complexity.\n- *Comparative Measure*: AIC is most useful when comparing multiple models. The model with the lowest AIC among a set of candidate models is generally preferred.\n- *Penalty for Complexity*: AIC includes a penalty for the number of parameters in the model. This discourages overfitting by penalizing models that use more parameters without a corresponding improvement in fit.\n\n\n\n\n\n## Data Explaination\nThe data for this project was initially scattered across multiple sources and required significant organization and compilation. The focus of this project is on the air quality in Portland, Oregon, so various data sources were aggregated and processed to compare a variety of air quality indicators.\n\n## Air Quality Data:\nAir quality data, specifically AQI values, were obtained from the United States Environmental Protection Agency (EPA) pre-generated data files. The AQI values are calculated daily, based on a variety of factors including criteria gasses and measured pollutant concentrations, and measures how harmful breathing the air is. AQI is classified into one of six categories from ‘good’ to ‘hazardous’, each having long term health effects associated with it. The files were given daily on a county wide basis, separated into different files by year. \n\n## Meteorological Data:\nHistorical weather data was also sourced from the EPA database, measured by thousands of weather stations across the country. Measurements tracked include temperature, wind speed, air pressure, and humidity. Temperature is measured in degrees fahrenheit. Wind speed is measured in knots, which are defined as one nautical mile per hour (equivalent to approximately 1.15mph). Wind speed is important in air quality as winds can blow different pollutants around and move and spread wildfires. Pressure is measured in millibars, where 1013.25 millibars is the standard atmospheric pressure (Earth’s pressure at mean sea level). Finally, humidity is measured in percent relative humidity. This is the amount of water vapor in the air as a percentage of the maximum amount of water vapor possible at a given temperature. Humidity can make it more difficult to breathe and sweat, make the air feel hotter than it is, and prevent air pollutants from dispersing as easily. Indoors, high humidity can trap air, leading to the growth of mold and harmful bacteria. This data was given daily by city, separated into different files by year. Measurements were taken hourly, but pre-calculated in the source database, giving an average value over the twenty four hours and a maximum value.\n\n## Pollution Source Data:\nPollution data was again sourced from the EPA database, separated by criteria gasses (CO, NO2, O3, SO2), Toxins (lead), and particulate matter (PM2.5 and PM10). Criteria gas Carbon Monoxide is measured in parts per million, and is especially dangerous as it is both colorless and odorless. CO binds to hemoglobin in the blood, making the transportation of oxygen around the body more difficult. Nitrogen Dioxide is dangerous to breathe in at high levels. It can cause swelling in the throat, burning, reduced oxygenation of body tissues, and fluid build up in the lungs. It is released in many common combustion reactions including in cars, coal plants, and cigarettes. It is measured in parts per billion. Ozone can harm our ability to breathe, especially in older people, children, and people with asthma. It is measured in parts per million. Sulfur Dioxide, measured in parts per billion, can irritate the eyes, mucous membranes, skin, and respiratory tract. Lead is a toxin which can increase the risk of high blood pressure, cardiovascular problems, and complications during pregnancy. While exposure has gone down significantly in the recent decades after use in gasoline, it still remains a dangerous toxin to breathe in. It is measured in micrograms per cubic meter. PM2.5 and PM10 are particulate matter, small inhalable particles with diameters of 2.5 microns or smaller, and 10 microns or smaller respectively. PM2.5 includes all sorts of common particles, metals, and organic compounds. PM10 includes dust, pollen, molds, and other larger (but still very small) particles. Due to the variability of particles included in the PM classification, there are a wide range of negative health impacts that come from breathing in these particles. PM2.5 and PM10 are measured in micrograms per cubic meter.\n\nThis data was given daily by city, separated into different files by year. They are sourced from thousands of individual sources, which measure various selections of these pollution sources. Because of the variety of different pollutants being measured, there was a significant amount of missing data, especially from small towns. Measurements were taken hourly, pre-compiled into a daily average and maximum. \n\n## Transit Data:\nInformation on motor buses taken from the National Transit Database, produced by the Federal Transit Administration. Includes information of bus systems and ridership by city, separated by year. Data is recorded yearly, encompassing annual totals for information such as number of buses, total revenue, passengers, and miles driven for the respective city transit systems. Information was given in yearly CSVs, separated by the city transit system. For cities with multiple systems, data was combined. Only motorbus data was used, which may not be reflective of cities with other large methods of public transportation, such as the New York subway system.\n\n## Population Data:\nData on population and population density sourced from the Simplemaps United States Cities database, which is built from multiple sources including the U.S. Geological Survey and the U.S. Census Bureau. Data is updated as of May 6, 2024, reflecting very up to date information. \n\n# Data Processing\nThe data was downloaded in R. For information given in yearly CSV files, data was stacked vertically to include all years in our time frame. In all tables, relevant columns were selected and renamed, reducing the information being brought into our initial SQL database. R was connected and imported to PostgreSQL using the RPostgres package, and used to read, stack, select columns, and rename columns before being written into a PostgreSQL database. \n\n## Data Organization\nGiven the raw data available, the table structure was simplified compared to the original data sources. Data was organized in a star schema centered on the air_quality fact table. This table tracks AQI, pollutant, weather and toxin data daily for each location. The first dimension table is the dates table, a serialized list of dates from January 1st, 2015 to December 31st, 2022. Next, we have a locations dimension table, a serialized list of over 1400 cities and towns from around the country. These are labeled by the state, county, and city name, as well as the population and population density, allowing connection to information based on what is given. The aqi_category dimension table is a short list of AQI value categories (Good, Unhealthy, Hazardous, etc.) with their respective AQI value range as minimum and maximum values. \n\nFinally, the yearly_transit dimension table gives the information for the transit system of the respective city during the specified year attached in the fact table. This table seems counterproductive to not include the location or year of the specified line or even a reference id, but in keeping with star schema, it was decided that this was the best way to reference this information. Understanding the context of a specified line requires joining the table back to the fact table, and joining the location and date tables to that as well. \n\nEach table has a unique serialized primary key, and all dimension tables are connected via foreign key. Several additional indexes are included on columns that will be queried often. Finally, constraints have been added to limit unusual or impossible data.\n\nTracking these identifiers independently allows for accurate analysis of changes over time and across different areas, and allows adding new information should we need to update the database. (Figure 1) illustrates the resulting ERD structure using drawSQL.\n\n\nFigure 1.  \n\n![ERD Diagram](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/aqi_erd_final.png?raw=true)\n\n\n```{python}\n#| label: import-package\nfrom sqlalchemy import create_engine, text\nimport dotenv\nimport datetime\nimport time\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport missingno as msno\nfrom summarytools import dfSummary\nimport sweetviz as sv\nfrom ydata_profiling import ProfileReport\n```\n\n## Initial Exploratory Data Analysis (EDA)\nWe have a new dataset named metro_1mil.csv. This file was created using a SQL statement that joins all relevant tables, filtering for metropolitan areas with populations less than or equal to 1 million. This approach limits our EDA to mid-sized metropolitan cities, such as Portland, Oregon.\n\n```{python}\n#| label: read-data\ndf = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')\n```\n\n## Visualization AQI Distribution\nLet's plot the AQI data distribution\n```{python}\n#| label: plot-aqi\nplt.figure(figsize=(10, 6))\nsns.histplot(df['aqi'], kde=True)\n\nplt.title('Distribution of Air Quality Index')\nplt.xlabel('AQI')\nplt.show()\n```\n\n## Dataframe Shape\n```{python}\n#| label: describe-shape\nnum_rows, num_columns = df.shape\n```\nThe DataFrame contains `{python} num_rows` rows and `{python} num_columns` columns.\n\n# Exploring Oregon State: \n```{python}\n#| label: filter-df\ndf = df[df['state'] == 'Oregon']\nnum_rows_oregon = df.shape[0]\n```\nBy filtering our Dataframe for Oregon state, our DataFrame contains `{python} num_rows_oregon` rows.\n\n# Features Engineering\n\n```{python}\n#| label: convert-dt\ndf['date'] = pd.to_datetime(df['date'])\n```\n\n```{python}\n#| label: df-drop-col\ndf = df.drop(['aqi_range', 'st_abbv'], axis=1)\ndf = df.drop(['mean_lead_micrograms_per_cubic_meter', 'max_lead_micrograms_per_cubic_meter'], axis=1)\ndf = df.drop(['mean_pm100_micrograms_per_cubic_meter', 'max_pm100_micrograms_per_cubic_meter'], axis=1)\ndf = df.drop(['mean_pm25_micrograms_per_cubic_meter', 'max_pm25_micrograms_per_cubic_meter'], axis=1)\n```\n\n```{python}\n#| label: df-ampute\ncolumns_to_impute = [\n    'mean_temperature_fahrenheit', 'max_temperature_fahrenheit',\n    'mean_pressure_millibars', 'max_pressure_millibars',\n    'mean_humidity_percent_relative_humidity', 'max_humidity_percent_relative_humidity',\n    'mean_wind_knots', 'max_wind_knots',\n    'mean_co_ppm', 'max_co_ppm',\n    'mean_no2_ppb', 'max_no2_ppb',\n    'mean_ozone_ppm', 'max_ozone_ppm',\n    'mean_so2_ppb', 'max_so2_ppb'\n]\n\nmeans = {col: df[col].mean() for col in columns_to_impute}\ndf = df.assign(**{col: df[col].fillna(means[col]) for col in columns_to_impute})\n```\n\nDate Column Preprocessing:\n\n- Converted the date column to DateTime objects for easier manipulation and analysis.\n- Extracted additional time-based features: year, month, day of week, and quarter.\n\n\nFeature Selection:\n\n- Removed irrelevant columns to focus the analysis on pertinent variables.\n- Retained features: pollutant, aqi, wind\n\n\nMissing Value Treatment:\n\n- Identified columns with missing values: most all of them\n- Applied mean() imputation method for numerical columns.\n- For categorical columns: n/a\n\n\nData Types and Memory Usage:\n\n- Optimized data types to reduce memory usage (e.g., using categories for low-cardinality strings, int8/int16 for small integers).\n\nBasic Statistics:\n\n- Generated summary statistics for numerical columns using df.describe().\n- Calculated frequency distributions for categorical variables.\n\n\nDistribution Analysis:\n\n- Plotted histograms and kernel density estimates for main numerical features.\n\n\nTime Series Components:\n\n- Decomposed time series data into trend, seasonality, and residual components for relevant variables.\n\n# Sweetviz Data Report\n```{python}\n#| label: sweetvis-report\nmy_report = sv.analyze(df)\nmy_report.show_html()\n```\nWe have generated a complete statistical report confirming the quality of EDA steps.\n\n# Advanced Exploratory Data Analysis\nWe have also employed the ydata-profiling package, a powerful Time Series Analysis EDA package that offers more detailed analysis.\n\nWe have unlocked time series-specific features using ydata-profiling:\n- Set tsmode=True when creating the ProfileReport\n- Ensure our DataFrame is sorted or specify the sortby parameter\n- Time Series Feature Identification\n\nThe ydata identifies time-dependent features using autocorrelation analysis.  \nFor recognized time series features:\n- Histograms are replaced with line plots\n- Feature details include new autocorrelation and partial autocorrelation plots\n- Two additional warnings may appear: `NON STATIONARY` and `SEASONAL`\n\nHandling Multi-Entity Time Series Data, In our case, with category_id:\n\n- Each pollutants represents a distinct time series\n- For optimal analysis, we filter and profile each pollutant separately\n\n```{python}\n#| label: ydata-report\n#for group in df.groupby(\"category_id\"):\n#    # Running 1 profile per station\n#    profile = ProfileReport(\n#        group[1],\n#        minimal=True,\n#        sortby=\"date\",\n#        # title=f\"Air Quality profiling - Site Num: {group[0]}\"\n#    )\n\n #   profile.to_file(f\"Ts_Profile_{group[0]}.html\")\n```\n\n```{python}\n#| label: ydata-render\n#profile = ProfileReport(\n#    group[1],\n#    tsmode=True,\n#    sortby=\"date\",\n#    # title=f\"Air Quality profiling - Site Num: {group[0]}\"\n#)\n#profile.to_file(\"your_report2.html\")\n```\n\n\n```{python}\n#| label: quarto-cross-ref pickle\n#df.to_pickle('/data/df.pkl')\n```\n\nOur exploratory data analysis (EDA) process consisted of two complementary approaches:\n\n- *Manual Investigation*: We conducted an in-depth, hands-on examination of the dataset.\n- *Automated Analysis*: We leveraged two powerful EDA packages:\n\n- *Sweetviz*: For quick, visual data summaries\n- *ydata-profiling*: For more detailed, customizable reports  \n\nThese methods allowed us to thoroughly evaluate key data quality aspects, including:\n\n- Class balance in categorical variables\n- Presence and distribution of missing values (NaN)\n- Feature distributions and correlations\n- Potential time-series characteristics\n\nThis multi-faceted approach ensures a robust understanding of our dataset's structure, quality, and potential challenges before proceeding with further analysis.\n\n```{python}\n#| label: df-aqi\ndf_aqi = df[['date', 'aqi']]\ndf_aqi = df_aqi.set_index('date')\n```\n\n# Time Series Visualization for CO Pollutant, Wind and AQI\nCO pollutant refers to carbon monoxide, which is a colorless, odorless, and tasteless gas that can be harmful to human health and the environment. Here's some key information about CO as a pollutant:\n\nPrimarily produced by incomplete combustion of carbon-containing fuels\nMajor sources include vehicle exhaust, industrial processes, and some natural sources like volcanoes\n\n- Slightly less dense than air\n- Highly flammable\n\n```{python}\n#| label: ts-wind-co-aqi\nsns.set_theme(style=\"darkgrid\")\n\n# Prepare wind data\ndf_wind = df[['date', 'mean_wind_knots', 'max_wind_knots']]\ndf_wind = df_wind.set_index('date')\n\ndf_wind_year = df_wind.resample('YE').mean().assign(Resample='Year')\ndf_wind_month = df_wind.resample('ME').mean().assign(Resample='Month')\ndf_wind_day = df_wind.resample('D').mean().assign(Resample='Day')\n\ndf_wind_combined = pd.concat([df_wind_year, df_wind_month, df_wind_day])\ndf_wind_combined.ffill(inplace=True)\ndf_wind_combined.reset_index(inplace=True)\n\n# Prepare CO data\ndf_co = df[['date', 'mean_co_ppm', 'max_co_ppm']]\ndf_co = df_co.set_index('date')\n\ndf_co_year = df_co.resample('YE').mean().assign(Resample='Year')\ndf_co_month = df_co.resample('ME').mean().assign(Resample='Month')\ndf_co_day = df_co.resample('D').mean().assign(Resample='Day')\n\ndf_co_combined = pd.concat([df_co_year, df_co_month, df_co_day])\ndf_co_combined.ffill(inplace=True)\ndf_co_combined.reset_index(inplace=True)\n\n# Prepare AQI data\ndf_aqi = df[['date','aqi']]\ndf_aqi = df_aqi.set_index('date')\n\ndf_aqi_year = df_aqi.resample('YE').mean().assign(Resample='Year')\ndf_aqi_month = df_aqi.resample('ME').mean().assign(Resample='Month')\ndf_aqi_day = df_aqi.resample('D').mean().assign(Resample='Day')\n\ndf_aqi_combined = pd.concat([df_aqi_year, df_aqi_month, df_aqi_day])\ndf_aqi_combined.ffill(inplace=True)\ndf_aqi_combined.reset_index(inplace=True)\n\n# Merge the three DataFrames\ndf_combined = pd.merge(df_wind_combined, df_co_combined, on=['date', 'Resample'], suffixes=('_wind', '_co'))\ndf_combined = pd.merge(df_combined, df_aqi_combined, on=['date', 'Resample'])\n\n# Melt the DataFrame for FacetGrid\ndf_melted = df_combined.melt(id_vars=['date', 'Resample'], value_vars=['mean_wind_knots', 'mean_co_ppm', 'aqi'], \n                             var_name='Variable', value_name='Value')\n\n# Set the figure size\nplt.figure(figsize=(10, 18))\n\n# Plot using seaborn\ng = sns.FacetGrid(df_melted, row='Resample', col='Variable', hue='Resample', sharex=True, sharey=False, height=4, aspect=3)\ng.map(sns.lineplot, 'date', 'Value')\n\n# Adjust the plot\ng.add_legend()\ng.set_axis_labels('Date', 'Value')\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Mean Wind Knots, CO PPM, and AQI Resampled by Year, Month, and Day')\nplt.show()\n```\n\n# Time Series Visualization for SO2, NO2 and Ozone\nNO2 (nitrogen dioxide) is an important air pollutant. Here's a concise overview of it:  \n- Reddish-brown gas with a pungent odor\n- Part of a group of pollutants known as nitrogen oxides (NOx)\n\nSO2 (sulfur dioxide) is an important air pollutant. Here's a concise overview of SO2 as a pollutant:\n\n- Colorless gas with a sharp, pungent odor\n- Highly soluble in water\n\nOzone (O₃) as a pollutant is a complex topic, as it can be both beneficial and harmful depending on its location in the atmosphere. \nHere's a concise overview of ozone as a ground-level pollutant:\n\n- Colorless to pale blue gas with a distinctive smell\n- Highly reactive molecule composed of three oxygen atoms\n```{python}\n#| label: ts-so2-no2-ozone\n#| warning: false\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"darkgrid\")\n\n# Filter and resample SO2 data\ndf_so2 = df[['date', 'mean_so2_ppb', 'max_so2_ppb']]\ndf_so2 = df_so2.set_index('date')\n\ndf_so2_year = df_so2.resample('YE').mean().assign(Resample='Year')\ndf_so2_month = df_so2.resample('ME').mean().assign(Resample='Month')\ndf_so2_day = df_so2.resample('D').mean().assign(Resample='Day')\n\ndf_so2_combined = pd.concat([df_so2_year, df_so2_month, df_so2_day])\ndf_so2_combined.ffill(inplace=True)\ndf_so2_combined.reset_index(inplace=True)\n\n# Filter and resample NO2 data\ndf_no2 = df[['date', 'mean_no2_ppb', 'max_no2_ppb']]\ndf_no2 = df_no2.set_index('date')\n\ndf_no2_year = df_no2.resample('YE').mean().assign(Resample='Year')\ndf_no2_month = df_no2.resample('ME').mean().assign(Resample='Month')\ndf_no2_day = df_no2.resample('D').mean().assign(Resample='Day')\n\ndf_no2_combined = pd.concat([df_no2_year, df_no2_month, df_no2_day])\ndf_no2_combined.ffill(inplace=True)\ndf_no2_combined.reset_index(inplace=True)\n\n# Filter and resample ozone data\ndf_ozone = df[['date', 'mean_ozone_ppm', 'max_ozone_ppm']]\ndf_ozone = df_ozone.set_index('date')\n\ndf_ozone_year = df_ozone.resample('YE').mean().assign(Resample='Year')\ndf_ozone_month = df_ozone.resample('ME').mean().assign(Resample='Month')\ndf_ozone_day = df_ozone.resample('D').mean().assign(Resample='Day')\n\ndf_ozone_combined = pd.concat([df_ozone_year, df_ozone_month, df_ozone_day])\ndf_ozone_combined.ffill(inplace=True)\ndf_ozone_combined.reset_index(inplace=True)\n\n# Merge the three DataFrames\ndf_combined = pd.merge(df_so2_combined, df_no2_combined, on=['date', 'Resample'], suffixes=('_so2', '_no2'))\ndf_combined = pd.merge(df_combined, df_ozone_combined, on=['date', 'Resample'])\n\n# Melt the DataFrame for FacetGrid\ndf_melted = df_combined.melt(id_vars=['date', 'Resample'], \n                             value_vars=['mean_so2_ppb', 'mean_no2_ppb', 'mean_ozone_ppm'], \n                             var_name='Variable', value_name='Value')\n\n# Set the figure size\nplt.figure(figsize=(15, 20))\n\n# Plot using seaborn\ng = sns.FacetGrid(df_melted, row='Resample', col='Variable', hue='Resample',sharex=True, sharey=False, height=6, aspect=2)\ng.map(sns.lineplot, 'date', 'Value')\n\n# Adjust the plot\ng.add_legend()\n#g.set_axis_labels('Date', 'Value (PPB/PPM)')\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Mean SO2, NO2, and Ozone Resampled by Year, Month, and Day')\nplt.show()\n```\nWe finally completed the exploratory data analysis.\n\n\n---\njupyter: ir\n---\n# Prophet AQI Trend Forecasting and Hypothesis Testing\n\n\n\n\n\nTo gain an understanding of how Portland's AQI differs from other large cities, we can start by running a hypothesis test to determine the significance.\n\n\n\nInitially we can run a two sample t-test to show that Portland's AQI average is statistically greater than the average AQI of all large metropolitan areas within the US. We compare the sample of Portland AQI datapoints in the time period with the sample of all metropolitan areas with a population greater than one million. \n\n**Null Hypothesis**: The Portland AQI is greater than or equal to the AQI of all large metro cities in the US.\n\n**Alternate Hypothesis**: The Portland AQI is less than the AQI of all large metro cities in the US.\n\n**Assumptions**: \n\n1. Simple Random Sample: Data is a simple random sample. We have selected 10% of values from each population set.\n\n2. Independence: AQI in Portland does not affect AQI in the rest of the country. However, the dataset for the large metro areas does include Portland, so Portland AQI will be repeated within both population groups. Both groups are largely independent, though this should be noted.\n\n3. Normal Distribution: \n\n::: {#19e0d385 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](capstone_files/figure-html/cell-5-output-1.png){width=420 height=420}\n:::\n:::\n\n\nFrom the QQ plots and histograms, we can see both datasets are clearly not normally distributed. They have long tails to the right. However, since the tails have such low frequencies and the samples are very large, this likely will not impact the results.\n\nThe partial violations of the assumptions in the t test in our analysis suggest that the conclusions should be considered with a degree of caution. \n\n**Two Sample T Test Portland AQI vs Large Metro Areas AQI**\n\n::: {#ac116c61 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n```\n\n\tWelch Two Sample t-test\n\ndata:  pdx_aqi_sample and lmaqi_sample\nt = -5.8489, df = 298.71, p-value = 6.49e-09\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -6.926287\nsample estimates:\nmean of x mean of y \n 38.61644  48.26444 \n```\n:::\n:::\n\n\nBased on the low P-value of < 2.2*10^-16, we can safely reject the null hypothesis. We conclude that Portland's AQI mean is not greater than or equal to the AQI of all large metropolitan areas. This agrees with our initial observations on the greater AQI outcomes of Portland, and specifies the significance of this statistically.\n\n\n# Prophet Data Forecasting\n\nProphet by Meta is used as time series prediction to estimate and map trends based on our data. We use this to see how AQI trends vary by day, month, and year. It is also able to give us a forecast for a given period after the end of our data which we can analyze and use to anticipate future AQI values.\n\n\n## Data Forecasting\n\nTo use the package, data must be in the format of a two column graph, with the first column being the date data, and the second being the variable being mapped and predicted. In this case, this predicted variable is AQI. \n\n\n\nThe package allows a future period to be generated, which can be specified and added to the end of the time data. It can then predict future AQI values for the new data period.\n\n\n\nThe graph below (figure 4324), shows the supplied data with the future prediction over the future period. The actual values are shown with the black datapoints, and the blue line represents the prediction. The upper and lower bounds of the error are represented with the transparent blue area. Each year, the data spikes during the late summer to early fall. Even more significant is the large spike in September 2020. What caused it? How significant was it?\n\n::: {#9b8bcc49 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](capstone_files/figure-html/cell-11-output-1.png){width=420 height=420}\n:::\n:::\n\n\n## 2020 Wildfires\n\nIn September 2020, a wildfire ravaged the state of Oregon, as well as many other areas of the United States and Canada. The fires burned more than one million acres of land, destroying thousands of homes, and killing 11 people. 500,000 Oregonians were on evacuation alert, and 40,000 were actually forced to leave (Oregon Department of Emergency Management). Anyone around during that time will recall the orange skies, thick atmosphere, and strong smoke smell, but how unusual was this period actually?\n\nTo understand, we will conduct a two sample t-test to see whether or not this month had greater than usual AQI.\n\n**Null Hypothesis**: September 2020 AQI less than or equal to AQI of entire period in Portland\n\n**Alternate Hypothesis**: September 2020 AQI greater AQI of entire period in Portland\n\n**Assumptions**:\n\n1. Simple Random Sample: Data is not a simple random sample. Since there are so few datapoints for the September 2020 population (30 datapoints), taking a 10% sample may not be suitable to accurately capture the variance of this month. Thus, it was decided to use the entire population as the sample. A 10% sample will be taken of the entire Portland data population. \n\n2. Independence: AQI in September 2020 does not affect AQI in the rest of the timeframe. \n\n3. Normal Distribution: \n\n::: {#cfd6eb9f .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](capstone_files/figure-html/cell-12-output-1.png){width=420 height=420}\n:::\n:::\n\n\nFrom the QQ plots and histograms, we can see both datasets are clearly not normally distributed. They have long tails to the right, and the September 2020 dataset is oddly shaped due to lack of data.\n\nThe partial violations of the assumptions in the t test in our analysis suggest that the conclusions should be considered with a degree of caution. \n\n**Two Sample T Test September 2020 vs Full Period of Portland AQI**\n\n::: {#136805e9 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n```\n\n\tWelch Two Sample t-test\n\ndata:  pdx_sep_20_aqi and pdx_data_sample\nt = 3.1638, df = 29.133, p-value = 0.001814\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 38.86371      Inf\nsample estimates:\nmean of x mean of y \n122.66667  38.73288 \n```\n:::\n:::\n\n\nBased on the low p value, we reject the null. We conclude that the September AQI in Portland is not less than or equal to the AQI for the entire period. The wildfire had a significant effect on the air quality, making it much more difficult to breathe. This aligns with the observation of the large spike during this period. We must do more to address and combat the wildfires that not only harm the air we breathe, but cause long lasting damage to the local environment. \n\n\n## AQI Trends\n\nProphet's plot component function allows us to see specific trends including the total (entire eight years), weekly, and yearly trends in figure 21111. This allows us to see how AQI changes by day. In the top full time span graph, it shows the potential spread of data for the predicted period with the transparent blue area. We can also see that day of the week tends to have very little impact on the trend(it may look significant but it is only moving up and down less than 1.5 AQI between days). Finally, from the yearly graph we cab see the consistent increase during the late summer to mid fall each year. \n\n::: {#e7c61301 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](capstone_files/figure-html/cell-14-output-1.png){width=420 height=420}\n:::\n:::\n\n\n## Cross Validation\n\nHow accurate are these predictions? A cross validation predicts over the period from the cutoff date up to specified date in the ds column date.\n\nCross validating the predictions allows us to create many estimates from a starting date up to an end date within out timeframe. In this instance, the tool predicts using start dates every half a year. It will predict AQI for that date to every day up to a year after the start date, giving us 365 estimates per start date. We then compare all estimations seeing how accurate the prediction is for a number of days after the start date as shown in figure 348483 below. \n\nThe tool allows us to measure the difference in predicted y and actual y with a variety of different measurements. In the figure below, we have selected mean absolute percent error, to show how if the error on average increased as the prediction got further from the starting point.\n\n\n\n\n::: {#e109c226 .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](capstone_files/figure-html/cell-17-output-1.png){width=420 height=420}\n:::\n:::\n\n\nThe light grey datapoints represent the mean absolute percent error. A datapoint with a x value 200 and y value 1 means it was predicted for a period of 200 days after the cutoff date, and was 1% off the actual value. We can see that most predictions are under 1%, making this a pretty decent estimation. The fact that they do not increase over time allows us to have a certain degree of confidence in the prediction even a year out. The blue line shows the mean of these datapoints.\n\n\n## Portland's Transit System\n\nAs shown in the graphs and tests above, Portland has greater than normal AQI outcomes when compared with other cities of high populations. The AQI follows certain yearly trends, with the notable exception in September 2020 due to the large wildfires. Using Prophet, we are able to predict AQI up to a year out of the final datapoint in the time period. What exactly is the reason for these outcomes in this city?\n\nOne hypothesis for Portland's better air quality outcomes is due to the increased focus on public transportation. Portland has placed high emphasis on utilizing public transit, with rates comparable to larger cities and higher than most other cities of its size. We will run a two sample t test to see how Portland's rate of ridership compares with other large cites.\n\nWe have standardized rates by passenger miles ridden per person. This is the total amount of miles ridden in a given year divided by the population. This allows us to properly compare cities with different population sizes.\n\n**Null Hypothesis**: Transit ridership in Portland is less than or equal to transit ridership across the country (large metro areas only).\n\n**Alternate Hypothesis**: Transit ridership in Portland is greater than transit ridership across the country (large metro areas only).\n\n**Assumptions**:\n\n1. Simple Random Sample: Data is not a simple random sample. Since there are so few datapoints for the Portland transit ridership population (8 datapoints), taking a 10% sample may not be suitable to accurately capture the variance of this month. Thus, it was decided to use the entire population as the sample. A 10% sample will be taken of the large cities transit population. \n\n2. Independence: Transit ridership in Portland does not affect transit ridership in the rest of the country. \n\n3. Normal Distribution: \n\n::: {#8402b3d2 .cell execution_count=17}\n\n::: {.cell-output .cell-output-display}\n![](capstone_files/figure-html/cell-18-output-1.png){width=420 height=420}\n:::\n:::\n\n\nFrom the QQ plots and histograms, we can see both datasets are clearly not normally distributed. The Portland data has so few datapoints that it is hard to tell if it has a normal distribution or not. The large city transit data seems normal with the exception of one datapoint. \n\nThe partial violations of the assumptions in the t test in our analysis suggest that the conclusions should be considered with a degree of caution. \n\n**Two Sample T Test Portland Transit Ridership vs Large Metro Area Ridership**\n\n::: {#8afaa6e4 .cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n```\n\n\tWelch Two Sample t-test\n\ndata:  pdx_transit$pass_miles_per_person and large_metro_transit_sample\nt = 1.4209, df = 25.248, p-value = 0.0838\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -5.556724       Inf\nsample estimates:\nmean of x mean of y \n 95.90601  68.36166 \n```\n:::\n:::\n\n\nBased on the low p value, we reject the null. Portland transit ridership is not less than or equal to the average transit ridership of large metro areas across the country. Portland's higher transit numbers could be an influence in the better air outcomes of the city. However, because transit ridership is better in some other cites such as Baltimore and New York but not reflected in their AQI outcomes, this leads us to believe that transit ridership does not have very significant influence in the air quality. We will explore which features have a greater effect on AQI in the upcoming Machine Learning section.\n\n\n\n\n# Results\n\n## Sci-Kit Learn Machine Learning\nUltimately, we want to see which variables have the greatest impact on AQI. To do this we must perform a machine learning analysis and create a prediction algorithm. \n\n```{python}\n#| label: import-packages\n#| include: false\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n```\n\nFirst, missing data must be addressed. This can be done in a variety of ways, but for this we replace NA values with the mean for the respective city and group by week. If there is no data to take a mean of, that row will be dropped. \n\n```{python}\n#| include: false \n#| #| label: read-csv\ndf = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')\nlen(df)\n```\n\n```{python}\n#| label: check-isna\n#| include: false\ndf.isna().sum()\n```\n\n```{python}\n#| label: met-dataframe\n#| include: false\nmet = df[[\n    'date',\n    'state',\n    'county',\n    'city',\n    'population',\n    'density',\n    'aqi',\n    'category',\n    'mean_temperature_fahrenheit',\n    'mean_pressure_millibars',\n    'mean_humidity_percent_relative_humidity',\n    'mean_wind_knots',\n    'mean_co_ppm',\n    'mean_no2_ppb',\n    'mean_ozone_ppm',\n    'mean_so2_ppb',\n    'mean_pm100_micrograms_per_cubic_meter',\n    'mean_pm25_micrograms_per_cubic_meter',\n    'mean_lead_micrograms_per_cubic_meter',\n    'num_busses',\n    'revenue',\n    'operating_expense',\n    'passenger_trips',\n    'operating_hours',\n    'passenger_miles',\n    'operating_miles'\n    ]]\n\nmet = met.rename(columns={'mean_temperature_fahrenheit': 'temp', \n                          'mean_pressure_millibars': 'pressure', \n                          'mean_humidity_percent_relative_humidity': 'humidity',\n                          'mean_wind_knots': 'wind_speed', \n                          'mean_co_ppm': 'co', \n                          'mean_no2_ppb': 'no2',\n                          'mean_ozone_ppm': 'o3', \n                          'mean_so2_ppb': 'so2', \n                          'mean_pm100_micrograms_per_cubic_meter': 'pm100',\n                          'mean_pm25_micrograms_per_cubic_meter': 'pm25', \n                          'mean_lead_micrograms_per_cubic_meter': 'lead'})\n```\n\n\n```{python}\n#| include: false\n#| label: check-NaN\nmet.isna().sum()\n```\n\n```{python}\n#| include: false\n#| label: set-index-ts\nmet['date'] = pd.to_datetime(met['date'])\nmet.set_index('date', inplace=True)\n#group by week, drop lead for too much missing data\nmet = met.groupby([pd.Grouper(freq='W'), 'state', 'county', 'city']).agg({\n    'population': 'first',\n    'density': 'first',\n    'aqi': 'mean',\n    'temp': 'mean',\n    'pressure': 'mean',\n    'humidity': 'mean',\n    'wind_speed': 'mean',\n    'co': 'mean',\n    'no2': 'mean',\n    'o3': 'mean',\n    'so2': 'mean',\n    'pm100': 'mean',\n    'pm25': 'mean',\n    'num_busses': 'mean',\n    'revenue': 'mean',\n    'operating_expense': 'mean',\n    'passenger_trips': 'mean',\n    'operating_hours': 'mean',\n    'passenger_miles': 'mean',\n    'operating_miles': 'mean'\n}).reset_index()\n```\n\nSince AQI is the dependent variable being measured, all rows without AQI data are dropped. Certain cities have very little data and will be dropped out of necessity.\n\n```{python}\n#| include: false\n#| label: drop-missNaN-col\nmet = met[met['city'] != 'Virginia Beach']\n```\n\n```{python}\n#| include: false\n#| label: drop-null-aqi\nmet = met.dropna(subset=['aqi'])\n```\n\n```{python}\n#| include: false\n#| label: check-NaN-again\nmet.isna().sum()\n```\n\nThe data collected has separate information for the city of New York City. NYC is divided into five boroughs, each within its own county. These values are grouped and averaged out to make NYC have the same amount of datapoints as every other city. This will also address data present in some New York boroughs but not others. Likewise, Kansas City spans two states and two counties, so those values are grouped together.\n\n```{python}\n#| include: false\n#| label: group-nyc-1-city\nnyc = met[met['city'] == 'New York']\ncolumns = ['date', 'aqi', 'temp', 'pressure','humidity',\n           'wind_speed','co','no2','o3',\n           'so2','pm100','pm25', 'num_busses',\n           'revenue', 'operating_expense', \n           'passenger_trips', 'operating_hours', \n           'passenger_miles', 'operating_miles']\n\nnyc = nyc[columns].groupby('date').mean().reset_index()\n#Add data that was dropped\nnyc['state'] = 'New York'\nnyc['county'] = 'Multiple'\nnyc['city'] = 'New York City'\nnyc['population'] = 18908608\nnyc['density'] = 11080.3\n\nnyc = nyc[['date', 'state', 'county', 'city', 'population', \n     'density', 'aqi', 'temp', 'pressure','humidity',\n     'wind_speed','co','no2','o3', 'so2','pm100',\n     'pm25', 'num_busses', 'revenue', 'operating_expense', \n     'passenger_trips', 'operating_hours', \n     'passenger_miles', 'operating_miles']]\n\nprint(nyc)\n```\n\n```{python}\n#| include: false\n#| label: replace-nyc-in-met\nmet = met[met['city'] != 'New York']\nmet = pd.concat([met, nyc], ignore_index=True)\n```\n\n```{python}\n#| include: false\n#| label: dataframe-Kansas-City\n#Do same with Kansas City\nkc = met[met['city'] == 'Kansas City']\ncolumns = ['date', 'aqi', 'temp', 'pressure','humidity',\n           'wind_speed','co','no2','o3',\n           'so2','pm100','pm25', 'num_busses',\n           'revenue', 'operating_expense', \n           'passenger_trips', 'operating_hours', \n           'passenger_miles', 'operating_miles']\n\nkc = kc[columns].groupby('date').mean().reset_index()\n\ncolumns = ['temp', 'pressure','humidity',\n           'wind_speed','co','no2','o3',\n           'so2','pm100','pm25', 'num_busses',\n           'revenue', 'operating_expense', \n           'passenger_trips', 'operating_hours', \n           'passenger_miles', 'operating_miles']\nmet[columns] = met[columns].fillna(met.groupby('city')[columns].transform('mean'))\n\nkc['state'] = 'Missouri'\nkc['county'] = 'Multiple'\nkc['city'] = 'Kansas City'\nkc['population'] = 1689556\nkc['density'] = 620.7\n\nkc = kc[['date', 'state', 'county', 'city', 'population', \n     'density', 'aqi', 'temp', 'pressure','humidity',\n     'wind_speed','co','no2','o3', 'so2','pm100',\n     'pm25', 'num_busses', 'revenue', 'operating_expense', \n     'passenger_trips', 'operating_hours', \n     'passenger_miles', 'operating_miles']]\n\nmet = met[met['city'] != 'Kansas City']\nmet = pd.concat([met, kc], ignore_index=True)\n```\n\nAfter this data cleaning, we are left with a total of eighteen cities across the country with a total metropolitan area population of greater than one million. The table below (figure 24324) shows all eighteen cities being used in the prediction algorithm and their respective row counts.\n\n```{python}\n#| echo: false\n#| label: count-cities\nds = met.dropna()\n\nds.groupby('city').size().reset_index(name='count')\n```\n\nTo perform a ML prediction algorithm, the predicted variable (AQI) must be discrete. To achieve this, we bin AQI data into discrete groups. Initially, we thought to bin them based on the existing AQI categories, but found that most of the data is grouped in the sub 100 range. Therefore, we expanded the bins, focusing the prediction on outcomes in the double digits. The bins chosen are: \n\n- 0-30\n- 31-40\n- 41-50 \n- 51-60\n- 61-70\n- 71-80\n- 81-90\n- 91-100\n- 101-150\n- 151+\n\n```{python}\n#| include: false\n#| label: AQI-bin-ML\nbins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 150, 500]\n\nlabels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\nds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n\nds.head()\n```\n\n```{python}\n#| include: false\n#| #| label: label-bin\nlabels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\nds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n\nds.head()\n```\n\n```{python}\n#| include: false\n#| #| label: convert-year-month-datetime\nds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year\nds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month\n```\n\nA feature selector is run on the set of all variables. This chooses the best predictors of the dependent variable AQI.\n\n```{python}\n#| include: false\n#| #| label: aqi-X-y\n#dependent variable aqi, all others (except aqi category) as independent variables\ny = ds['aqi_discrete']\nX = ds[['city',\n        'population',\n        'density',\n        'temp',\n        'pressure',\n        'humidity',\n        'wind_speed',\n        'co',\n        'no2',\n        'o3',\n        'so2',\n        'pm100',\n        'pm25',\n        'num_busses', \n        'revenue', \n        'operating_expense', \n        'passenger_trips', \n        'operating_hours', \n        'passenger_miles', \n        'operating_miles',\n        'year',\n        'month'\n        ]]\n\n```\n\nThe following tools are used:\n\n- Train Test Split\n- One Hot Encoder\n- Transformer\n- Pipeline\n- Standard Scaler \n\n```{python}\n#| include: false\n#| #| label: split-train\n#Split into train and test datasets, define model and encoder\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nencoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')\n```\n\n```{python}\n#| include: false\n#| #| label: def-transformer\n#define transformer with encoder and standardscaler\n\ntransformer = ColumnTransformer(\n        [\n            ('categories', encoder, ['city','year','month']),\n            ('scaled_air_quality', StandardScaler(), [\n                'population',\n                'density',\n                'temp',\n                'pressure',\n                'humidity',\n                'wind_speed',\n                'co',\n                'no2',\n                'o3',\n                'so2',\n                'pm100',\n                'pm25',\n                'num_busses', \n                'revenue', \n                'operating_expense', \n                'passenger_trips', \n                'operating_hours', \n                'passenger_miles', \n                'operating_miles'\n                ]\n            )\n        ],\n        remainder='drop', verbose_feature_names_out=False)\n#fit transformer\ntransformer.fit(X_train, y_train)\n```\n\nFeature selection is done on the data. From this we are given the following variables: \n\n- City\n- Temperature\n- Humidity\n- Carbon Monoxide\n- Nitrogen Dioxide\n- Ozone\n- PM10\n- PM2.5\n\nThese features are used in the final model.\n\n```{python}\n#| include: false\n#| #| label: feature-select\n#feature selection\nfeature_selector = SelectKBest(k=10)\nX_train_trans = transformer.transform(X_train)\nX_train_trans_df = pd.DataFrame(\n    X_train_trans, \n    columns = transformer.get_feature_names_out(),\n    index = X_train.index)\nfeature_selector.fit(X_train_trans_df, y_train)\nfeature_selector.get_support()\nfeature_selector.scores_[feature_selector.get_support()]\nX_train_trans_df.columns[feature_selector.get_support()]\n```\n\n\n\n```{python}\n#| include: false\n#| #| label: use-features\n#Use selected features\ny = ds['aqi_discrete']\nX = ds[[\n    'city', 'temp', 'humidity', 'co', 'no2', 'o3', 'pm100', 'pm25'\n    ]]\n#Create train/test data with selected features\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train_enc = pd.DataFrame(encoder.fit_transform(X_train[['city']]),\n                           columns = encoder.get_feature_names_out(), index = X_train.index)\nX_train_trans = X_train[[\n    'temp', 'humidity', 'co', 'no2','o3', 'pm100', 'pm25'\n    ]].merge(X_train_enc, left_index = True, right_index = True)\nX_test_enc = pd.DataFrame(encoder.fit_transform(X_test[['city']]),\n                           columns = encoder.get_feature_names_out(), index = X_test.index)\nX_test_trans = X_test[[\n    'temp', 'humidity', 'co', 'no2','o3', 'pm100', 'pm25'\n    ]].merge(X_test_enc, left_index = True, right_index = True)\n```\n\nWith these features, we test various models with default parameters to see which is the most accurate at predicting AQI from this dataset. The five models tested are: \n\n- K nearest neighbors\n- Tree model \n- Random Forest model\n- Logistic Regression \n- Naive Bayes \n\nRunning the models gives the output in figure 234324. Random Forest model is the most accurate and will be used for the final model.\n\n```{python}\n#| echo: false\n#| warning: false\n#| label: model-select\nmodel1 = KNeighborsClassifier(n_neighbors=5)\nmodel2 = tree.DecisionTreeClassifier()\nmodel3 = RandomForestClassifier(n_estimators=10, random_state=12)\nmodel4 = LogisticRegression()\nmodel5 = GaussianNB()\n\nresults = []\n\nfor model, label in zip([model1, model2, model3, model4, model5], ['KNN', 'Tree', 'Random Forest', 'Logistic', 'naive Bayes']):\n    model.fit(X_train_trans, y_train)\n    y_pred = model.predict(X_test_trans)\n    model = label\n    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n    accuracy = sum(y_test == y_pred)/len(y_test)\n    \n    results.append({\n        'Model': label,\n        'Cohen Kappa Score': cohen_kappa,\n        'Accuracy': accuracy\n    })\n\ndisplay(pd.DataFrame(results))\n```\n\n```{python}\n#| include: false\n#| #| label: def-best-model\n#Create model from most accurate, RandomForest\nmodel = RandomForestClassifier(n_estimators=10)\nmodel.fit(X_train_trans, y_train)\n\n#predict on test data\ny_pred = model.predict(X_test_trans)\n\n#Print results\nprint(\"cohen kappa score: \", cohen_kappa_score(y_test, y_pred))\nprint(\"accuracy: \", sum(y_test == y_pred)/len(y_test))\nConfusionMatrixDisplay(\n    confusion_matrix = confusion_matrix(\n        y_test, y_pred, \n        labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n        ), \n        display_labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n        ).plot(xticks_rotation='vertical')\n```\n\n```{python}\n#| include: false\n#| label: improve-transformer\n#redefined transformer\ntransformer = ColumnTransformer(\n        [\n            ('categories', encoder, ['city']),\n            ('scaled_air_quality', StandardScaler(), [\n                'temp',\n                'humidity',\n                'co',\n                'no2',\n                'o3',\n                'pm100',\n                'pm25'\n                ]\n            )\n        ],\n        remainder='drop', verbose_feature_names_out=False)\n#Pipeline for simplification\n\nclassification_pipeline = Pipeline([('aqi_transformer', transformer),\n                                    ('RF_model', RandomForestClassifier())\n                                    ])\nclassification_pipeline.fit(X_train, y_train)\n```\n\nHyperparameter optimization will be done to further improve the model. A randomized search is run with 100 iterations. The following hyperparameters are optimized:\n\n- Max Categories\n- Min Frequency\n- Max Depth\n- Max Features\n- Min Samples Leaf\n- Min Samples Split\n- Num Estimators\n- Bootstrap\n\n```{python}\n#| include: false\n#| label: hyper-parameters-optim\n#hyperparameter optimization\nclassification_pipeline.get_params()\n```\n\n\n\n```{python}\n#| include: false\n#| label: def-parameters\nparameters = {\n    'aqi_transformer__categories__max_categories': randint(3,30),\n    'aqi_transformer__categories__min_frequency': randint(2,20),\n    'RF_model__max_depth': randint(3, 30),\n    'RF_model__max_features': [None, 'sqrt', 'log2'],\n    'RF_model__min_samples_leaf': randint(2, 10),\n    'RF_model__min_samples_split': randint(2, 10),\n    'RF_model__n_estimators': randint(50, 200),\n    'RF_model__bootstrap': [True, False]\n}\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(classification_pipeline, param_distributions=parameters,\n                                   n_iter=n_iter_search, n_jobs=-1, cv=5)\nrandom_search.fit(X_train, y_train)\nprint(random_search.best_score_)\n```\n\nFigure 242442 below shows the selected hyperparameters.\n\n```{python}\n#| echo: false\n#| label: randsearch-get-params\nrandom_search.best_params_\n#random_search.best_estimator_.get_params()\nparams_df = pd.DataFrame(list(random_search.best_params_.items()), columns=['Parameter', 'Value'])\n\n# Print the DataFrame\ndisplay(params_df)\n```\n\nUsing these hyperparameters, we reach an accuracy of about 63%.\n\n```{python}\n#| echo: false\n#| warning: false\n#| label: predict-X_test\ny_pred=random_search.predict(X_test)\nprint(\"Cohen Kappa Score: \", cohen_kappa_score(y_test, y_pred))\nprint(\"Accuracy: \", sum(y_pred == y_test)/len(y_test))\n```\n\n```{python}\n#| include: false\nConfusionMatrixDisplay(\n    confusion_matrix = confusion_matrix(\n        y_test, y_pred, \n        labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n        ), \n        display_labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n        ).plot(xticks_rotation='vertical')\n```\n\n\n![AirQuality Confusion Matrix](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/AirQuality-ConfusionMatrix.png?raw=true)\n\nWe can use this model for the prediction of AQI, exploring the features that predict it, and use that to generate conclusions for what particles to reduce or systems to increase.\n\n\n# ML Time Series with SARIMAX Model\n\n```{python}\n#| label: import-lib\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n```\n\n```{python}\n#| label: read-data2\nimport pandas as pd\ndf = pd.read_pickle('/data/df.pkl')\n```\n\n```{python}\n#| label: define-df_aqi\nsns.set_theme(style=\"darkgrid\")\ndf_aqi = df[['date','aqi']]\ndf_aqi = df_aqi.set_index('date')\n\ndf_aqi = df_aqi.resample('ME').mean()\ndf_aqi.ffill(inplace=True)\n#df_aqi.plot(figsize=(15,8))\n```\nAny time series is decomposed of two things:\n\n- Seasonality\n- Trends\n\nBy the help of `statsmodel` package we can break the time series into its seasonal pattern and trends.\nThis will helps us to understand the data clearly and will help us to make more sense of the data.\n\n# Decomposing the Time Series With Additive Method\n```{python}\n#| label: additive-decomp\nsns.set_theme(style=\"darkgrid\")\ndecomposition = sm.tsa.seasonal_decompose(df_aqi, model='additive')\nfig = decomposition.plot()\nplt.show()\n```\n\nThere are three components to a time series: \n\n1. `Trend`: Trend tells you how things are overall changing  \n2. `Seasonality`: Seasonality shows you how things change within a given period (e.g. year,month, week, day)  \n3. `Residual`: The Error/residual/irregular activity are the anomalies whitch cannot be explained by the trend or the seasonal value  \n\nIn a additive time series, the components add together to make the time series. If you have an increasing trend, you still\nsee roughly the same size peaks and troughs throughout the time series. This is often seen in indexed time series where the\nabsolute value is growing but changes stay relative.\n\n\n## Time Series Prediction\nFor this project, we have used an extended version of *ARIMA* model knows as *SARIMAX* model as we have explained in the methods section.\nThe *SARIMAX* model is used when the data sets have seasonal cycles. In our dataset concerning air quality/AQI there is a seasonal pattern\nwhich we can see in the above visualization.\n\nWe need to find the right p,d and q parameters to correctly forecast and predict the AQI value.\n\n- *p* is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model.\n- *d* is the integrated part of the model. This includes terms in the model that incorporate the amount of diferencing (the number of past time points to subtract from the current value) to apply the time series.\n- *q* is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.  \n\nWe use a tuning technique called `grid search method` that attempts to compute the optimum values of hyperparameters. We are trying to find the right p,d,q values that would be given as an input to the SARIMAX time series model.\n```{python}\n#| label: grid-search\n#| cache: true\nimport itertools\n\n# Define the p, d, q parameters to take any value between 0 to 1\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q)) # generate all possible combinations of p,d,q\n\n# This creates combinations of seasonal parameters with a seasonality period of 12 (e.g., monthly data).\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in pdq] \n\n# Outer Loop: Iterates over all combinations of pdq.\n# Inner Loop: Iterates over all combinations of seasonal_pdq.\n#  Try Block:\n#  Creates a SARIMAX model with the current combination of parameters.\n#  Fits the model to the data.\n#  Prints the AIC (Akaike Information Criterion) for the model, which is a measure of model quality.\n# Except Block:\n#  Catches and prints any errors that occur during model fitting, allowing the loop to continue with the next set of parameters.\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(df_aqi,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n        except Exception as e:\n            print(f\"Error with parameters {param} and {param_seasonal}: {e}\")\n            continue\n```\n\nWe have to find the lowest AIC values which would have the best corresponding p,d,q values to have the best forecast of AQI values.  \n\n## Summary of SARIMAX\n```{python}\n#| label: mod-fit\nmod = sm.tsa.statespace.SARIMAX(df_aqi,order=(1, 1, 1),seasonal_order=(0,1, 1, 12),enforce_stationarity=False,enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])\n```\n\n# How Fit the SARIMAX model\n## Print the summary which includes AIC\n```{python}\n#| label: fit-summary-AIC\nmod = sm.tsa.statespace.SARIMAX(df_aqi, order=(1, 1, 1), seasonal_order=(0, 1, 1, 12), enforce_stationarity=False, enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary())\n```\n\n```{python}\n#| label: Extract-AIC-value\naic_value = results.aic\nprint(f\"The AIC value is: {aic_value}\")\n# Additional performance metrics can be calculated as needed\n```\n\n# Plot Diagnostics\n```{python}\n#| label: plot-diag\nresults.plot_diagnostics(figsize=(12, 10))\n```\n\n# Train and Test\nRigorous validation is paramount to establishing the model's reliability and practical application. To ensure the model's generalizability, we will employ a train-test split. This approach safeguards against overfitting by exposing the model to unseen data, allowing for a more accurate assessment of its predictive capabilities.\n\nBy partitioning the dataset, we can:\n\n- Evaluate performance: Measure the model's accuracy on unseen data.\n- Detect overfitting: Identify discrepancies between training and testing performance.\n- Assess generalization: Determine the model's ability to handle new data.\n- Quantify reliability: Calculate confidence intervals for prediction accuracy.\n- Iteratively improve: Use insights to refine the model.\n\nThis rigorous process underpins the credibility and utility of our research findings.\n\nTo split the data, we follow the recommended `70:30` ratio, 70% of the data is the training data, and 30% of the data is the testing data.\n\n```{python}\n#| label: check-min-date\n# Check the minimum date in the 'date' column\n#print(f\"Start date of the data:\", df_aqi.index.min())\n```\n\n```{python}\n#| label: check-max-date\n#print(f\"End date of the data:\", df_aqi.index.max())\n```\n\nOnce the model is created, predicted values are generated using the .get_prediction() method, with datetime as input\n```{python}\n#| label: check-prediction\npred = results.get_prediction(start=pd.to_datetime('2023-01-01 00:00:00'), dynamic=False)\npred_ci = pred.conf_int()\n```\n\nThe graph indicates overlapping patterns in the testing and training data, suggesting strong potential for the forecasting model's performance.\n```{python}\n#| label: plot-prediction\nax = df_aqi['2015-01-31 00:00:00':].plot(label='Observed') # plot the observed data \npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n\nax.fill_between(pred_ci.index,pred_ci.iloc[:, 0],pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Days')\nax.set_ylabel('AQI index')\nplt.legend()\nplt.show()\n```\n\nTo facilitate comparison of true and predicted test values, we will create a separate DataFrame. \nMean Error Estimation will be used for analysis.\n```{python}\n#| label: define-forecast-truth\ny_forecasted = pred.predicted_mean\ny_truth = df_aqi['2022-12-31 00:00:00':]\n```\n\nTo evaluate model performance, we calculate the MSE\n```{python}\n#| label: calculate-mse\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nmse = np.sqrt(mean_squared_error(y_truth, y_forecasted))   \nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n```\n\n## Forecasting Future Values\n\nAs we conclude our modeling process, we generate predictions for the next 7 data points:\n\n1. **Model Information**: The `result` variable contains our fitted model's details.\n\n2. **Forecasting Method**: We use the `.get_forecast()` method on our model results.\n\n3. **Prediction Generation**: This method analyzes observed patterns in our data to project future values.\n\n4. **Output**: We obtain forecasts for the next 7 time points, representing predicted air quality levels.\n\nThis step transforms our analytical work into actionable insights for air quality management.\n```{python}\n#| label: recheck-forecast\npred_uc = results.get_forecast(steps=7)\npred_ci = pred_uc.conf_int()\n```\n\n## Visualizing Our Results: The Culmination of Our Analysis\n\nThe final and crucial step of our project is the creation of a comprehensive plot that encapsulates our complex analysis. This visualization serves as the key to understanding and interpreting our findings.\n\n### Interpreting the Forecast Plot\n\nOur plot consists of several key elements:\n\n1. **Observed Values (Blue Line)**\n   - Represents the actual, historical air quality measurements\n   - Provides a baseline for comparing our predictions\n\n2. **Forecasted Values (Orange Line)**\n   - Depicts the future air quality levels predicted by our SARIMAX Time Series Model\n   - Allows us to visualize potential trends and patterns in air quality\n\n3. **Confidence Interval (Shaded Region)**\n   - The shaded area around the forecast line represents the 95% Confidence Interval (CI)\n   - Indicates the range within which we can be 95% confident that the true future values will fall\n   - Wider intervals suggest greater uncertainty in the prediction\n\nThis visual representation not only summarizes our extensive data analysis but also provides a powerful tool for understanding potential future air quality trends. It bridges the gap between complex statistical models and actionable insights, making our findings accessible and meaningful to a broader audience.\n\n```{python}\n#| label: plot-forecast\nax = df_aqi.plot(label='Observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,pred_ci.iloc[:, 0],pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Days')\nax.set_ylabel('AQI index')\nplt.legend()\nplt.show()\n```\n\n\n# Conclusion\n\nAfter conducting our thorough research, we have landed on these specific recommendations. We found there are high seasonal trends where late summer/early fall tends to have the worst air quality. These numbers consistently show up each year, exasperated by the dry heat and lack of rainfall. As climate change raises temperatures and water sources dry up, wildfire season will continue to get worse over time. We must be aware of the nature of air quality and how it differs at different parts of the year. We should understand how the AQI works, and avoid being outside for too long when it reaches more dangerous levels.\n\nAir quality is significantly affected by various natural and unpredictable elements, including:\n\n- Weather conditions\n- Wind speed and direction\n- Temperature fluctuations\n- Humidity levels\n- Atmospheric pressure\n- Solar radiation intensity\n\nWith multiple factors we have no control over, it is important to do what we can for those factors we can impact.\n\nWe must focus on the variables that have the greatest impact on the AQI. These are city, temperature, humidity, CO, NO2, O3, PM10, and PM2.5. City, temperature, and humidity for the most part are out of our control. That leaves us with three criteria gasses and all particulate matter. The largest source of carbon monoxide, nitrogen dioxide, and ozone is the cars, trucks, and other vehicles we use daily (Environmental Protection Agency). We can lower our reliance on personal vehicles by utilizing public transportation, carpooling, walking, biking, increasing work from home to lower commutes when available, and overall be more considerate about if driving a car is necessary. We often drive unnecessarily, out of convenience or impatience. \n\nFor many Americans, personal vehicles are required. Many urban and suburban areas lack proper public transportation, or existing public transportation systems are inadequate, unreliable, and infrequent. Many towns and cities in the United States were designed for cars instead of for people. Fixing this will require a substantial overhaul, necessitating millions or billions of dollars in spending. This is not to say that we shouldn’t bother---any investment that increases the public transportation system’s usage decreases the amount of cars on the road. A small change is the first step in addressing the personal vehicle issue.\n\nIndustrial manufacturing processes and agriculture are significant polluters of the environment. We should invest in the research of more environmentally friendly manufacturing methods, working with materials that require less combustion, or are recyclable. Agricultural reduction starts with less of a reliance on red meat and the dairy industry, mainstays of the American diet. This will be a huge shift, taking combined efforts of the citizens, government, and food industry. As red meat and dairy consumption goes down, possibly due to the replacement with artificial or lab grown meat, less livestock will need to be kept, and less feeding crops will need to be grown (Congressional Budget Office). It will be a difficult transition but a necessary one.\n\nWildfires not only increase the particle matter in the air, but burn forests, causing long term damage to the soil. Particulate matter in the air makes it more difficult to breathe, which is reflected by the increased AQI levels. Not all forest fires are started by man made sources, but many are. Therefore, when in the woods, one should always obey fire restrictions, especially in the middle of the summer when it’s most dry. If fires are allowed, they should always be watched and never left unattended. They must always be properly extinguished and all embers must be cool to the touch before leaving. Campsites should be properly cleaned, and all tools used correctly. One should also stay on marked trails, avoiding trampling vegetation which can increase the risk of wildfire spreading (Oregon Wildfire Response and Recovery).\n\nOne of the largest limitations is Algorithm Dependence. This is the reliability of forecasts which are inherently tied to the chosen predictive algorithms. Different models may yield varying results, emphasizing the importance of algorithm selection and validation.\n\nWe must also be wary of geographical considerations. Our data analysis couldn't fully quantify the unique geographical features of Portland and the broader Willamette Valley region. In further analysis we should seek to understand: \n\n- The protective influence of surrounding mountain ranges\n- The impact on wind patterns and air circulation\n- The potential effects of wildfires on air quality\n\nThese geographical factors play a significant role in local air quality dynamics but were beyond the scope of our current data set. The impact of the geological features can be seen in the image below (figure 1414)\n\n![AirNow Map](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/AirNow-Map.png?raw=true)\n\nBy recognizing these limitations, we can better interpret and apply our forecasting results, while also identifying areas for future research and data collection to enhance prediction accuracy.\n\nUltimately, we have a responsibility to take care of our planet and combat climate change. The worse climate conditions get, the more wildfires will spread, and the worse the air quality will become. As time goes on, with worsening air conditions, more people will catch and even die from preventable conditions sparked by poor air quality.  Water, pollution, food, and financial problems will get worse. One should look at what they can do to make a difference, support those who vouch to make larger changes, and encourage people they know to do the same. While the situation may seem dire, there is hope for progress through concerted and informed efforts.\n\n\n# Bibliography\n\nAmerican Lung Association, https://www.lung.org/research/sota/key-findings  \n\nAirly, https://airly.org/en/how-does-humidity-affect-air-quality-all-you-need-to-know/  \n\nAirnow.gov, https://www.airnow.gov/aqi/aqi-basics/using-air-quality-index  \n\nCalifornia Air Resources Board, https://ww2.arb.ca.gov/resources/carbon-monoxide-and-health  \n\nCenters for Disease Control and Prevention, Agency for Toxic Substances and Disease Registry, https://wwwn.cdc.gov/TSP/ToxFAQs/ToxFAQsDetails.aspx?faqid=396&toxid=69  \n\nCenters for Disease Control and Prevention, Agency for Toxic Substances and Disease Registry, https://wwwn.cdc.gov/TSP/MMG/MMGDetails.aspx?mmgid=249&toxid=46 \n\nCongressional Budget Office, https://www.cbo.gov/publication/60030\n\nEnvironmental Protection Agency, https://www.epa.gov/pm-pollution/particulate-matter-pm-basics \n\nEnvironmental Protection Agency, https://www.epa.gov/ground-level-ozone-pollution/health-effects-ozone-pollution \n\nEnvironmental Protection Agency, https://www.epa.gov/clean-air-act-overview/evolution-clean-air-act \n\nFederal Transit Administration, https://www.apta.com/research-technical-resources/transit-statistics/ntd-data-tables/  \n\nFuller, Landrigan, Balakrishnan, et al., https://www.thelancet.com/journals/lanplh/article/PIIS2542-5196(22)00090-0/fulltext \n\nJacobs, Burgess, Abbott, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5922205/  \n\nManisalidis, Stavropoulou, Stavropoulos, Bezirtzoglou, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7044178/ \n\nNational Oceanic and Atmospheric Administration, https://oceanservice.noaa.gov/facts/nautical-mile-knot.html  \n\nNational Weather Service, https://www.weather.gov/source/zhu/ZHU_Training_Page/winds/pressure_winds/Pressure.htm \n\nOregon Wildfire Response and Recovery, https://wildfire.oregon.gov/prevention \n\nWorld Health Organization, https://www.who.int/news-room/fact-sheets/detail/lead-poisoning-and-health \n\nZhang Et. Al. https://acp.copernicus.org/articles/18/15003/2018/  \n\nWorld Health Organization, https://www.who.int/news/item/25-03-2014-7-million-premature-deaths-annually-linked-to-air-pollution\n\n",
    "supporting": [
      "capstone_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}