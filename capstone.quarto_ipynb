{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title-block-banner: true\n",
        "title: \"Unveiling the Drivers of Clean Air\"\n",
        "subtitle: \"Analysis of Portland Air Quality (AQI)\"\n",
        "date: \"2024-08-01\"\n",
        "author:\n",
        "  - name: Stéphane Berhault\n",
        "    orcid: 0009-0003-5357-629X\n",
        "    email: sgberhault@willamette.edu\n",
        "    affiliation: \n",
        "      - name: Willamette University\n",
        "        city: Portland\n",
        "        state: OR\n",
        "        url: https://willamette.edu\n",
        "  - name: Jake Warmflash\n",
        "    email: jwarmflash@willamette.edu\n",
        "    affiliation: \n",
        "      - name: Willamette University\n",
        "        city: Portland\n",
        "        state: OR\n",
        "        url: https://willamette.edu\n",
        "keywords:\n",
        "  - AQI\n",
        "  - Time Series\n",
        "  - Prophet\n",
        "  - SARIMAX\n",
        "  - Scikit-Learn\n",
        "execute:\n",
        "  echo: false\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "format:\n",
        "  html:\n",
        "    theme: cerulean\n",
        "    fig-width: 8\n",
        "    fig-height: 6\n",
        "    code-fold: true\n",
        "    code-tools:\n",
        "      source: false\n",
        "      toggle: false\n",
        "      caption: none\n",
        "    smooth-scroll: true\n",
        "    toc: true\n",
        "    toc-location: left\n",
        "    toc-title: Contents\n",
        "    number-sections: false\n",
        "    other-links:\n",
        "      - text: Airnow Open Data\n",
        "        href: https://www.airnow.gov\n",
        "    code-links:\n",
        "      - text: Sweetviz Analysis Report\n",
        "        icon: file-code\n",
        "        href: SWEETVIZ_REPORT.html\n",
        "      - text: ydata Analysis Report\n",
        "        icon: file-code\n",
        "        href: your_report2.html\n",
        "    html-math-method: mathjax\n",
        "    css: styles.css\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "In their journal article published in The American Journal of Public Health, Jacobs, Burgess, and Abbott tell the story of the Donora Pennsylvania Smog crisis. On October 30, 1948, the Donora High School Football team played through a dense smog to complete the game with hundreds of fans in the audience, despite very poor visibility. The team, fueled by resilience, took pride in playing through poor conditions, a testament to the high spirit of the small town. Soon after, calls to the town’s medical offices began flooding in, complaining of difficulty breathing and respiratory issues. Donora, Pennsylvania, was a town of metalworks, built by the American Steel and Wire company and the Donora Zinc Works company, which made up major parts of the town’s economy. The heavy smog and pollution clouds that covered the sky had been viewed as a sign of prosperity, owing to the industrial might that powered their economy. Within just twelve hours, seventeen people would be dead, 1440 seriously affected, and 4470 with mild to moderate conditions---almost half the town’s working population (Jacobs, Burgess, Abbott). \n",
        "\n",
        "This event, known as the Donora Smog of 1948, prompted the country into taking a closer look at the negative impacts of air pollution. Widespread debate surrounding the event led to the first legislation aimed at regulating the air quality within the United States, ushering in a new era of tracking, combatting, and reversing the ill effects of poor air quality. The quality of air we breathe has direct impacts on our health. We must understand the factors that contribute to poor air quality and how we individually and collectively contribute to these changes. Until we can visualize the impact we have on our atmosphere, we will continue behavior that negatively impacts the air around us. \n",
        "\n",
        "In this project, we will focus on the key factors influencing air quality in Portland, Oregon. We aim to understand the complex interplay between various environmental and human-made factors that contribute to high air pollution levels. Initially, we were surprised to discover that Portland, the city we reside in, has some of the best air quality for a city of its size in the United States. This led us to narrow our focus to understanding the factors that contribute to these favorable outcomes in this city.\n",
        "\n",
        "Our project aims to develop and validate machine learning models to analyze the various factors influencing air quality. By focusing on Portland in comparison to other large US cities, we hope to find things Portland does that lead to the greater AQI outcomes. Our approach will consider a range of variables, including meteorological conditions, pollution sources, transit systems, park land, and livestock. Through this analysis, we aim to provide actionable insights and recommendations for sustaining and improving air quality. By examining both the contributors to clean air and the sources of pollution, we can understand the factors affecting air quality and develop comprehensive strategies for enhancement.\n",
        "\n",
        "\n",
        "# Background\n",
        "Federal regulation of air quality in the United States began in 1955 with the Air Pollution Control Act. This new piece of legislation provided funding for initial research into air quality and pollution in the US. Building off this and privately funded research, Congress passed the Clean Air Act of 1963, establishing the first federal regulation for controlling air pollution. This act established a new federal program within the US Public Health Service, dedicated to the monitoring and control of air quality. In 1967, Congress passed the Air Quality Act, which introduced more federal oversight and enforcement policies, allowing extensive monitoring of interstate air pollution. This all led to the passage of the 1970 Clean Air Act, aimed at restricting and regulating emissions, measuring and reducing pollutant particles, and addressing upcoming pollution threats (Environmental Protection Agency).\n",
        "\n",
        "Also established in 1970, the Environmental Protection Agency (EPA) implemented and monitored the requirements established by these rulings. The EPA's authority extended beyond federal lands and roads to include all companies operating within the United States. Enforcement authority was expanded to allow upholding these established standards, and prevent companies from circumventing the law.  Much of the improvement in the quality of air in the US over the past fifty years can be attributed to these regulations. In 1990, when deaths due to air quality were first measured, an estimated 135,000 Americans died. By 2010, that number had dropped to 71,000 (Zhang et al.). Despite the significant improvements led by the federal guidelines of the late 70s, nearly four in ten Americans still live in places where they are exposed to unhealthy air (American Lung Association).\n",
        "\n",
        "In 1999, the EPA developed the Air Quality Index (AQI), creating an easily understood measurement of air quality. The AQI measures air pollution levels on a scale from 0 to 500, divided into six categories. A score of 0 to 50 represents good air quality which poses little or no risk to those breathing it in, while a score above 300 signifies emergency conditions, an extremely high risk which impacts everyone. This measurement is mainly derived from five major pollutants: ozone, particulate matter (2.5μm and 10μm), carbon monoxide, nitrogen dioxide, and sulfur dioxide (Airnow.gov). Poor air quality has been linked to a variety of diseases including respiratory infections, stroke, heart disease, lung cancer, and chronic obstructive pulmonary disease, among others (World Health Organization). An estimated seven million premature deaths annually can be attributed to air pollution, which equates to a global mean loss of life expectancy of 2.9 years, making it the largest environmental risk factor for disease and premature death (Fuller, Landrigan, Balakrishnan, et. al.). Thus, it is important to understand factors that contribute to poor air quality, and outcomes that can be attributed to the state of the AQI.\n",
        "\n",
        "These harmful factors can originate from a variety of sources. Anything that releases a foreign substance into the air can lower the quality of the air. This includes smoking, vehicle exhaust, combustion processes for production and manufacturing, household cleaning products, appliances, central air and heating systems, agriculture pesticides, livestock, shipping and transportation, and much more. Individually, we can reduce our individual contributions by lowering our reliance on personal vehicles, watching our power usage, supporting companies that monitor and address their emissions, and more. However, there are many factors beyond our control. Larger pollutant sources, such as manufacturing and transportation, are often regulated to some extent but may still release significant amounts of pollutants into the atmosphere which we as individuals have no say over (Manisalidis, Stavropoulou, Stavropoulos, Bezirtzoglou). It is challenging to restrict and watch our personal contributions to the polluting of the environment without worrying about what others are doing. Measuring and analyzing the impact these pollutants have on air quality is a crucial step towards addressing these issues.\n",
        "\n",
        "## Data Ethics\n",
        "The ethical implications of conducting this analysis should always be considered and addressed. Air quality has historically impacted lower income communities significantly more. Marginalized communities may be built near major pollution sources such as factories or highways. In Portland, for example, Interstate 5 completed construction in 1966. It was part of an Oregon State Highway Department (OSHD) development, a project run by the state as a part of the Federal Aid Highway Act. The highway was built on the existing Minnesota Avenue, cutting North Portland, the heart of Portland’s African American community in half. It was decided to avoid affecting the higher income downtown properties. In fact, the OSHD contracted work on the Minnesota Ave portion of the highway to private contractors, in an effort to avoid a political battle (Oregon Encyclopedia). Today, the neighborhoods surrounding the large highway have worse AQI outcomes than those further away, all other factors equal (Shandas and George). Any recommendations given by this analysis should properly consider the ramifications of those impacted. \n",
        "\n",
        "Another important ethical issue to be aware of is data transparency. Data can be used to push a specific narrative without faking or editing the specific data points. By leaving out important information, pushing correlated data as causation to push an agenda, or by purposely misconstruing what the data mean, one can create a narrative that can misinform or deceive readers. All analysis methodology must be openly discussed to ensure full understanding by audiences. This leads to credibility, reproducibility, and trust in the methods and conclusions of any piece of data analysis. All data is sourced from credible online sources that are open about their collection methods. Data privacy is addressed and no personalized information is given. Limitations are also addressed allowing readers to understand where any uncertainty may be.\n",
        "\n",
        "\n",
        "# Methods\n",
        "Python will be the primary programming language used to conduct this analysis. We will also use R language in statistical applications. To perform our analysis, we will employ the NumPy and Pandas libraries for data manipulation, Matplotlib and Seaborn for visualization, and Time Series forecasting algorithms such as Prophet and SARIMAX. We will address data inconsistencies, missing values and ensure that data is in a tidy format. We may need to normalize or standardize data if necessary and create new features through aggregation to enhance the model’s performance.\n",
        "\n",
        "## Prophet\n",
        "Prophet is an open-source forecasting tool developed by Meta, designed for forecasting time series data. It is suited for datasets with strong seasonal, monthly, weekly, or daily patterns, and it handles missing data and outliers well. We utilized prophet to gain a quick understanding of our AQI patterns, seeking to understand basic trends before conducting a more thorough analysis.\n",
        "\n",
        "Key features of Prophet include seasonality detection and holiday incorporation, while providing easy use and understanding for users. We can use this software to get complex understanding from simple applications.\n",
        "\n",
        "To conduct this analysis, we prepare data into a two column table, date and AQI. Prophet uses the trends of past data to highlight similarities over days of the year, weeks, months, and seasons. From this, prophet is able to generate its predictions, cross validate, and give performance metrics such as mean absolute percentage error to quantify the accuracy of the results\n",
        "\n",
        "## SARIMAX​​\n",
        "The most common method used in time series forecasting is known as the ARIMA model. We will use an extended version called SARIMAX (*Seasonal Auto Regressive Integrated Moving Averages with exogenous factor*)\n",
        "\n",
        "- *(S) Seasonality*: Accounts for recurring patterns or cycles in the data.\n",
        "- *(AR) AutoRegressive*: Uses past values to predict future values.\n",
        "- *(I) Integrated*: Applies differencing to make the time series stationary.\n",
        "- *(MA) Moving Average*: Uses past forecast errors in the prediction.\n",
        "- *(X) eXogenous factors*: Incorporates external variables that may influence the forecast.\n",
        "\n",
        "The SARIMAX model is used when the data sets have seasonal cycles. In the dataset concerning the air quality/AQI there is a seasonal pattern. SARIMAX is a model that can be fitted to time series data in order to better understand or predict future points. SARIMAX is particularly useful for forecasting time series data that exhibits both trends and seasonality.\n",
        "There are three distinct integers (p,d,q) that are used to parametrize SARIMAX models. Because of that, SARIMAX models are denoted with the notation SARIMAX(p,d,q). Together these three parameters account for seasonality, trend, and noise in datasets:\n",
        "\n",
        "- *p* is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model.\n",
        "- *d* is the integrated part of the model. This includes terms in the model that incorporate the amount of differencing (the number of past time points to subtract from the current value) to apply the time series.\n",
        "- *q* is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.  \n",
        "\n",
        "We use a tuning technique called grid search method that attempts to compute the optimum values of the hyperparameters. We are trying to find the right p,d,q values that would be given as an input to the SARIMAX time series model.\n",
        "\n",
        "## Akaike Information Criteria (AIC)\n",
        "The Akaike Information Criterion (AIC) is a measure used to compare different statistical models. It helps in model selection by balancing the goodness of fit and the complexity of the model. When comparing two models, the one with the lower AIC is generally \"better\".\n",
        "\n",
        "Here’s how to interpret the AIC value:\n",
        "\n",
        "- *AIC Value*: A lower AIC value indicates a better-fitting model. It means the model has a good balance between accuracy and complexity.\n",
        "- *Comparative Measure*: AIC is most useful when comparing multiple models. The model with the lowest AIC among a set of candidate models is generally preferred.\n",
        "- *Penalty for Complexity*: AIC includes a penalty for the number of parameters in the model. This discourages overfitting by penalizing models that use more parameters without a corresponding improvement in fit.\n",
        "\n",
        "## Train-Test-Split\n",
        "Rigorous validation is paramount to establishing the model's reliability and practical application. To ensure the model's generalizability, we will employ a train-test split. This approach safeguards against overfitting by exposing the model to unseen data, allowing for a more accurate assessment of its predictive capabilities.\n",
        "\n",
        "By partitioning the dataset, we can:\n",
        "\n",
        "- *Evaluate performance*: Measure the model's accuracy on unseen data.\n",
        "- *Detect overfitting*: Identify discrepancies between training and testing performance.\n",
        "- *Assess generalization*: Determine the model's ability to handle new data.\n",
        "- *Quantify reliability*: Calculate confidence intervals for prediction accuracy.\n",
        "- *Iteratively improve*: Use insights to refine the model.\n",
        "\n",
        "This rigorous process underpins the credibility and utility of our research findings.\n",
        "To split the data, we follow the recommended 70:30 ratio, 70% of the data is the training data, and 30% of the data is the testing data.\n",
        "\n",
        "## Hypothesis Testing\n",
        "Hypothesis tests for significance are conducted to understand the significance of differences in datasets. Statistics are done in R using a variety of statistical methods and measurements:\n",
        "\n",
        "- *Null Hypothesis*: This serves as the baseline of a hypothesis test. It represents the idea that there is no effect, difference, or relationship between tested variables. Any observed differences are due to random chance. \n",
        "- *Alternate Hypothesis*: On the other hand, the alternative hypothesis suggests a potential effect, difference, or relationship between tested variables exists. It reflects what is hoped to be found by the data.\n",
        "- *Independence*: The idea that data in one sample or population has no impact on the data in another sample or population.\n",
        "- *Normal Distribution*: A continuous probability distribution symmetric around the mean. It is characterized by mean and standard deviation.\n",
        "- *Normal QQ Plot*: A plot to measure the normality of a distribution. It compares the quantiles of a dataset against the quantiles of a theoretical normal distribution. Data is mapped along a y=x trendline. Deviations of the data from the trendline indicate non normality. An “S” shaped bend suggests data with heavy or light tails. A convex or concave bend suggests skewness in the distribution.\n",
        "- *Histogram*: Plot that displayed data frequencies of values within specified intervals known as bins. Histograms are used to visualize the shape and spread of a distribution.\n",
        "- *Welch Two Sample t-test*: This type of hypothesis test measures the statistical significance of the difference in mean of the two samples. Unlike the Student’s  two sample t-test, the Welch two-sample t-test is used in cases where the groups being compared have unequal variances. A Welch two sample t-test includes the following:\n",
        "\t- *t-Statistic (t)*: Difference between sample means relative to variance of samples. Larger t-statistics indicate larger differences between means.\n",
        "\t- *Degrees of Freedom (df)*: Amount of information available to estimate variance of sample means.\n",
        "\t- *p-Value*: The probability of observing the sample data or something more extreme if the null hypothesis is true. A small p-value (< 0.05) suggests the observed difference is statistically significant.\n",
        "\t- *95 Percent Confidence Interval*: Range in which the true difference in means is 95% likely to lie within.\n",
        "\n",
        "## Machine Learning\n",
        "Machine learning is done in scikit-learn, an open source ML library built in Python. It is built on top of other common Python libraries such as NumPy, Pandas, and Matplotlib. The following tools are used:\n",
        "\n",
        "- *One Hot Encoder*: Transforms categorical data into a binary matrix. Each category is represented as a vector where one element is tagged with a 1 and all others are tagged as 0. This is done for certain machine learning algorithms that require all discrete data.\n",
        "- *Standard Scaler*: Used to normalize or standardize numerical data. Certain machine learning algorithms are affected by unscaled data and therefore require normalized data.\n",
        "- *Transformer*: Can be used to change multiple variables in one step in methods such as One Hot Encoder and Standard Scaler.\n",
        "- *Pipeline*: Allows multiple functions in a series of steps that can include transformers and models. Incorporates chaining to supply the output of one step as the input of the next step.\n",
        "- *Feature Selection*: Chooses a subset of most relevant features, with the ability to specify how many features.\n",
        "- *Hyperparameter Optimization*: Selects the best set of hyperparameters in a model. These hyperparameters are set before training and control various aspects of the training process and model behavior. The optimization method used is a Randomized search. This evaluates a fixed number of random combinations from a specified distribution. This is far more efficient than a Grid search which systematically checks every single combination of the specified hyperparameter distribution.\n",
        "- *Cohen Kappa Score*: A statistical measure used to assess agreement between two classifiers when they categorize items into classes. In ML specifically, the Cohen Kappa Score is used to evaluate agreement between predicted and actual labels in classification algorithms. It is measured on a scale from -1 to 1, where 1 represents perfect agreement in classifiers, 0 represents agreement being no better or worse than random chance, and -1 represents perfect disagreement.\n",
        "- *Random Forest Model*: An ensemble model used in prediction tasks. It combines multiple decision trees and aggregates their predictions to improve performance and reduce overfitting.\n",
        "- *Confusion Matrix*: Measures and visualizes the accuracy of a classification model. It breaks down raw numbers of correctly and incorrectly classified values.\n",
        "- *Classification Report*: A detailed evaluation of the prediction model. Includes: \n",
        "    - *Precision*: The ratio of correctly predicted positive observations to the total predicted positives\n",
        "    - *Recall*: The ratio of correctly predicted positive observations to all observations\n",
        "    - *F1 score*: Mean of precision and recall\n",
        "    - *Support*: Number of observations\n",
        "\n",
        "Reference Machine Learning Metrics Table in Appendix for additional information.\n",
        "\n",
        "\n",
        "# Data\n",
        "The data for this project was initially scattered across multiple sources and required significant organization and compilation. The focus of this project is on the air quality in Portland, Oregon, so various data sources were aggregated and processed to compare a variety of air quality indicators. There is a significant amount of missing data due to incomplete collection which will be addressed via filling in and row dropping.\n",
        "\n",
        "### Air Quality Data\n",
        "Air quality data, specifically AQI values, were obtained from the United States Environmental Protection Agency (EPA) pre generated data files. The AQI values are calculated daily, based on a variety of factors including criteria gasses and measured pollutant concentrations, and measures how harmful breathing the air is. AQI is classified into one of six categories from ‘good’ to ‘hazardous’, each having long term health effects associated with it. The files were given daily on a county wide basis, separated into different files by year. After stacking year outputs together in R, columns regarding date, location (state, county), AQI, and AQI category were brought into the database.\n",
        "\n",
        "### Meteorological Data\n",
        "Historical weather data was also sourced from the EPA database, measured by thousands of weather stations across the country. Measurements tracked include temperature, wind speed, air pressure, and humidity. Temperature is measured in degrees fahrenheit. Wind speed is measured in knots, which are defined as one nautical mile per hour (equivalent to approximately 1.15mph). Wind speed is important in air quality as winds can blow different pollutants around and move and spread wildfires. Pressure is measured in millibars, where 1013.25 millibars is the standard atmospheric pressure (Earth’s pressure at mean sea level). Finally, humidity is measured in percent relative humidity. This is the amount of water vapor in the air as a percentage of the maximum amount of water vapor possible at a given temperature. Humidity can make it more difficult to breathe and sweat, make the air feel hotter than it is, and prevent air pollutants from dispersing as easily. This data was given daily by city, separated into different files by year. Measurements were taken hourly, but pre-aggregated in the source database, giving an average value over the twenty four hours and a maximum value. After stacking years in R, columns regarding date, location (state, city), weather observation average and maximum, and observation unit were brought into the database.\n",
        "\n",
        "### Pollution Source Data\n",
        "Pollution data was again sourced from the EPA database, separated by criteria gasses (CO, NO2, O3, SO2), Toxins (lead), and particulate matter (PM2.5 and PM10). Criteria gas Carbon Monoxide (CO) is measured in parts per million, and is especially dangerous as it is both colorless and odorless. CO binds to hemoglobin in the blood, making the transportation of oxygen around the body more difficult. Nitrogen Dioxide (NO2)  is dangerous to breathe in at high levels. It can cause swelling in the throat, burning, reduced oxygenation of body tissues, and fluid build up in the lungs. It is released in many common combustion reactions including in cars, coal plants, and cigarettes. It is measured in parts per billion. Ozone (O3) can harm our ability to breathe, especially in older people, children, and people with asthma. It is measured in parts per million. Sulfur Dioxide (SO2), measured in parts per billion, can irritate the eyes, mucous membranes, skin, and respiratory tract. Lead is a toxin which can increase the risk of high blood pressure, cardiovascular problems, and complications during pregnancy. While exposure has gone down significantly in the recent decades after use in gasoline, it still remains a dangerous toxin to breathe in. It is measured in micrograms per cubic meter. PM2.5 and PM10 are particulate matter, small inhalable particles with diameters of 2.5 microns or smaller, and 10 microns or smaller respectively. PM2.5 includes all sorts of common particles, metals, and organic compounds. PM10 includes dust, pollen, molds, and other larger (but still very small) particles. Due to the variability of particles included in the PM classification, there are a wide range of negative health impacts that come from breathing in these particles. PM2.5 and PM10 are measured in micrograms per cubic meter.\n",
        "\n",
        "This data was given daily by city, separated into different files by year. They are sourced from thousands of individual sources, which measure various selections of these pollution sources. Because of the variety of different pollutants being measured, there was a significant amount of missing data, especially from small towns. Measurements were taken hourly, pre-compiled into a daily average and maximum. After stacking years in R, columns regarding date, location (state, city), pollutant observation average and maximum, and observation unit were brought into the database.\n",
        "\n",
        "### Public Transit Data\n",
        "Information on motor buses taken from the National Transit Database, produced by the Federal Transit Administration. Includes information of bus systems and ridership by city, separated by year. Data is recorded yearly, encompassing annual totals for information such as number of buses, total revenue, passengers, and miles driven for the respective city transit systems. Information was given in yearly CSVs, separated by the city transit system. For cities with multiple systems, data was combined. Only motorbus data was compiled, which may not be reflective of cities with other large methods of public transportation, such as the New York subway system. Only public transit information was able to be collected. Accessible data on cars and trucks could not be found. This will be further discussed in the limitations section of the conclusion. After stacking years in R, columns regarding date, location (state, city), transport mode, number of buses, passenger trips in hours and miles, and bus operations in hours and miles were brought into the database.\n",
        "\n",
        "### Land Area Data \n",
        "City and county area data was collected from the 2020 U.S. Census, collected by the U.S. Census Bureau. It is measured in km^2^. City, county, state, city area, and county area columns brought into the database.\n",
        "\n",
        "### Park Area Data\n",
        "Park areas data is sourced from the Trust for Public Land’s 2024 annual City Park Survey. This information is collected via survey from a variety of park agencies including Individual Parks and Recreation agencies, the National Park Service, Bureau of Land Management, Regional Park authorities, Private Conservancy, Watershed Management Agencies, private park services, and more. Totals are compiled by the Trust for Public Land, and measured in acres. Acreage is initially given in integers. Only location (city, state) and park area columns are brought into the database. \n",
        "\n",
        "### Livestock Data\n",
        "Information on livestock numbers collected on a county wide basis from the 2017 United States Census of Agriculture by the National Agricultural Statistics Service within the United States Department of Agriculture. Cattle include cows and bulls raised for meat consumption, cows for milk production, and calves. Hogs includes all pigs, adult and adolescent. Although there are colloquial differences for the animals called hogs and pigs, they refer to the same species, though ‘hogs’ generally refers to the larger members of the species. Sheep includes sheep raised for wool production and lambs. Numbers may have slight inaccuracies for areas with low numbers to prevent disclosing identifying information and are marked with a ‘D’. All of these data points will be replaced with zero. Location (county, state) and livestock varieties (cattle and calves, hogs, sheep and lambs) columns are collected and added to the database.\n",
        "\n",
        "### Population Data\n",
        "Data on population and population density was sourced from the Simplemaps United States Cities database, which is built from multiple sources including the U.S. Geological Survey and the U.S. Census Bureau. Data was last updated on May 6, 2024, reflecting very up to date information. \n",
        "\n",
        "## Data Organization\n",
        "Given the raw data available, the table structure was simplified compared to the original data sources. Data was organized around the air_quality table. This table tracks AQI, pollutant, weather and toxin data daily for each location. It includes all 1438 locations with a line for each of the 2922 days in the eight year time period, reaching a total of over 4.2 million rows. Location is split into a separate table to reduce repeated data. This table lists cities, labeled with their city name, county, and state. It includes metropolitan area population data given in raw numbers and as a density. Additionally, this table includes the city area in square kilometers and park area in acres. The aqi_category table is a short list of AQI value categories (Good, Unhealthy, Hazardous, etc.) with their respective AQI value range as minimum and maximum values. The yearly_transit table is connected to the location table and gives the information for the transit system of the respective city during the specified year. Finally, the livestock table, also connected via locations, includes counts for cattle, hogs, and sheep. \n",
        "\n",
        "The central table has a compound primary key composed of location_id and date. Each other table has a serialized primary key, which are used to connect to each other. Several additional indexes are included on columns that will be queried often. Finally, constraints have been added to limit unusual or impossible data. For example, an AQI value less than 0 or greater than 500 would be impossible and thus would be caught by the constraint.\n",
        "\n",
        "Tracking these identifiers independently allows for accurate analysis of changes over time and across different areas, and allows adding new information should we need to update the database. Figure 1 illustrates the resulting ERD structure using drawSQL.\n",
        "\n",
        "![Figure 1: ERD diagram, structure of database](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/erd_diagram_final_extended.png?raw=true)\n",
        "\n",
        "\n",
        "# Prophet AQI Trend Forecasting and Statistical Testing\n",
        "\n",
        "To gain an understanding of how Portland's AQI differs from other large cities, we can start by running a hypothesis test to determine the significance.\n",
        "\n",
        "Initially we can run a two sample t-test to show that Portland's AQI average is statistically greater than the average AQI of all large metropolitan areas within the US. We compare the sample of Portland AQI datapoints in the time period with the sample of all metropolitan areas with a population greater than one million.\n",
        "\n",
        "**Null Hypothesis**: The Portland AQI is greater than or equal to the AQI of all large metro cities in the US.\n",
        "\n",
        "**Alternate Hypothesis**: The Portland AQI is less than the AQI of all large metro cities in the US.\n",
        "\n",
        "**Assumptions**:\n",
        "\n",
        "1.  Simple Random Sample: Data is a simple random sample. We have selected 10% of values from each population set.\n",
        "\n",
        "2.  Independence: AQI in Portland does not affect AQI in the rest of the country. However, the dataset for the large metro areas does include Portland, so Portland AQI will be repeated within both population groups. Both groups are largely independent, though this should be noted.\n",
        "\n",
        "3.  Normal Distribution:\n",
        "\n",
        "![Figure 3334566: PDX Vs Large Metro Areas Distribution](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/normal_distribution_1_pdx_vs_lma.png?raw=true)\n",
        "\n",
        "From the QQ plots and histograms, we can see both datasets are clearly not normally distributed. They have long tails to the right. However, since the tails have such low frequencies and the samples are very large, this likely will not impact the results.\n",
        "\n",
        "The partial violations of the assumptions in the t-test in our analysis suggest that the conclusions should be considered with a degree of caution.\n",
        "\n",
        "**Two Sample T Test Portland AQI vs Large Metro Areas AQI**\n",
        "\n",
        "![](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/hypothesis_test_results_1_pdx_vs_lma.png?raw=true)\n",
        "\n",
        "Based on the low P-value of 6.862\\*10^-4^, we can safely reject the null hypothesis. We conclude that Portland's AQI mean is not greater than or equal to the AQI of all large metropolitan areas. This agrees with our initial observations on the greater AQI outcomes of Portland, and specifies the significance of this statistically.\n",
        "\n",
        "## Data Forecasting with Prophet\n",
        "\n",
        "Prophet by Meta is used as time series prediction to estimate and map trends based on our data. We use this to see how AQI trends vary by day, month, and year. It is also able to give us a forecast for a given period after the end of our data which we can analyze and use to anticipate future AQI values.\n",
        "\n",
        "To use the package, data must be in the format of a two column graph, with the first column being the date data, and the second being the variable being mapped and predicted. In this case, this predicted variable is AQI.\n",
        "\n",
        "The package allows a future period to be generated, which can be specified and added to the end of the time data. It can then predict future AQI values for the new data period.\n",
        "\n",
        "The graph below (figure 4324), shows the supplied data with the future prediction over the future period. The actual values are shown with the black datapoints, and the blue line represents the prediction. The upper and lower bounds of the error are represented with the transparent blue area. Each year, the data spikes during the late summer to early fall. Even more significant is the large spike in September 2020. What caused it? How significant was it?\n",
        "\n",
        "![Figure 4324: Prophet Prediction](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/prophet_pdx_trend_and_prediction.png?raw=true)\n",
        "\n",
        "## 2020 Wildfires\n",
        "\n",
        "In September 2020, a wildfire ravaged the state of Oregon, as well as many other areas of the United States and Canada. The fires burned more than one million acres of land, destroying thousands of homes, and killing 11 people. 500,000 Oregonians were on evacuation alert, and 40,000 were actually forced to leave (Oregon Department of Emergency Management). Anyone around during that time will recall the orange skies, thick atmosphere, and strong smoke smell, but how unusual was this period actually?\n",
        "\n",
        "To understand, we will conduct a two sample t-test to see whether or not this month had greater than usual AQI.\n",
        "\n",
        "**Null Hypothesis**: September 2020 AQI less than or equal to AQI of entire period in Portland\n",
        "\n",
        "**Alternate Hypothesis**: September 2020 AQI greater AQI of entire period in Portland\n",
        "\n",
        "**Assumptions**:\n",
        "\n",
        "1.  Simple Random Sample: Data is not a simple random sample. Since there are so few datapoints for the September 2020 population (30 datapoints), taking a 10% sample may not be suitable to accurately capture the variance of this month. Thus, it was decided to use the entire population as the sample. A 10% sample will be taken of the entire Portland data population.\n",
        "\n",
        "2.  Independence: AQI in September 2020 does not affect AQI in the rest of the timeframe.\n",
        "\n",
        "3.  Normal Distribution:\n",
        "\n",
        "![Figure 4444: Wildfire Distribution](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/normal_distribution_2_wildfires.png?raw=true)\n",
        "\n",
        "\n",
        "From the QQ plots and histograms, we can see both datasets are clearly not normally distributed. They have long tails to the right, and the September 2020 dataset is oddly shaped due to lack of data.\n",
        "\n",
        "The partial violations of the assumptions in the t test in our analysis suggest that the conclusions should be considered with a degree of caution.\n",
        "\n",
        "**Two Sample T Test September 2020 vs Full Period of Portland AQI**\n",
        "\n",
        "![](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/hypothesis_test_results_2_wildfires.png?raw=true)\n",
        "\n",
        "Based on the low p value of 1.85\\*10^-3^, we reject the null. We conclude that the September AQI in Portland is not less than or equal to the AQI for the entire period. The wildfire had a significant effect on the air quality, making it much more difficult to breathe. This aligns with the observation of the large spike during this period. We must do more to address and combat the wildfires that not only harm the air we breathe, but cause long lasting damage to the local environment.\n",
        "\n",
        "## AQI Trends\n",
        "\n",
        "Prophet's plot component function allows us to see specific trends including the total (entire eight years), weekly, and yearly trends in figure 21111. \n",
        "\n",
        "![Figure 21111: Prophet Component Trend Aggregations](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/prophet_components_plot_aqi.png?raw=true)\n",
        "\n",
        "This allows us to see how AQI changes by day. In the top full time span graph, it shows the potential spread of data for the predicted period with the transparent blue area. We can also see that day of the week tends to have very little impact on the trend (it may look significant but it is only moving up and down less than 1.5 AQI between days). Finally, from the yearly graph we can see the consistent increase during the late summer to mid fall each year.\n",
        "\n",
        "## Cross Validation\n",
        "\n",
        "How accurate are these predictions? A cross validation predicts over the period from the cutoff date up to specified date in the ds column date.\n",
        "\n",
        "Cross validating the predictions allows us to create many estimates from a starting date up to an end date within out timeframe. In this instance, the tool predicts using start dates every half a year. It will predict AQI for that date to every day up to a year after the start date, giving us 365 estimates per start date. We then compare all estimations seeing how accurate the prediction is for a number of days after the start date.\n",
        "\n",
        "The tool allows us to measure the difference in predicted y and actual y with a variety of different measurements. In figure 559955 below, we have selected mean absolute percent error, to show how if the error on average increased as the prediction got further from the starting point.\n",
        "\n",
        "![Figure 559955: Prophet Error on Cross Validated Estimates](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/prophet_cross_val_aqi.png?raw=true)\n",
        "\n",
        "The light grey datapoints represent the mean absolute percent error. A datapoint with a x value 200 and y value 1 means it was predicted for a period of 200 days after the cutoff date, and was 1% off the actual value. We can see that most predictions are under 1%, making this a pretty decent estimation. The fact that they do not increase over time allows us to have a certain degree of confidence in the prediction even a year out. The blue line shows the mean of these datapoints.\n",
        "\n",
        "## Portland's Transit System\n",
        "\n",
        "As shown in the graphs and tests above, Portland has greater than normal AQI outcomes when compared with other cities of high populations. The AQI follows certain yearly trends, with the notable exception in September 2020 due to the large wildfires. Using Prophet, we are able to predict AQI up to a year out of the final datapoint in the time period. What exactly is the reason for these outcomes in this city?\n",
        "\n",
        "One hypothesis for Portland's better air quality outcomes is due to the increased focus on public transportation. Portland has placed high emphasis on utilizing public transit, with rates comparable to larger cities and higher than most other cities of its size. We will run a two sample t test to see how Portland's rate of ridership compares with other large cites.\n",
        "\n",
        "We have standardized rates by passenger miles ridden per person. This is the total amount of miles ridden in a given year divided by the population. This allows us to properly compare cities with different population sizes.\n",
        "\n",
        "**Null Hypothesis**: Transit ridership in Portland is less than or equal to transit ridership across the country (large metro areas only).\n",
        "\n",
        "**Alternate Hypothesis**: Transit ridership in Portland is greater than transit ridership across the country (large metro areas only).\n",
        "\n",
        "**Assumptions**:\n",
        "\n",
        "1.  Simple Random Sample: Data is not a simple random sample. Since there are so few datapoints for the Portland transit ridership population (8 datapoints), taking a 10% sample may not be suitable to accurately capture the variance of this month. Thus, it was decided to use the entire population as the sample. A 10% sample will be taken of the large cities transit population.\n",
        "\n",
        "2.  Independence: Transit ridership in Portland does not affect transit ridership in the rest of the country.\n",
        "\n",
        "3.  Normal Distribution:\n",
        "\n",
        "![Figure 300000: Transit Distribution](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/normal_distribution_3_pdx_transit.png?raw=true)\n",
        "\n",
        "From the QQ plots and histograms, we can see both datasets are clearly not normally distributed. The Portland data has so few datapoints that it is hard to tell if it has a normal distribution or not. The large city transit data seems normal with the exception of one datapoint.\n",
        "\n",
        "The partial violations of the assumptions in the t test in our analysis suggest that the conclusions should be considered with a degree of caution.\n",
        "\n",
        "**Two Sample T Test Portland Transit Ridership vs Large Metro Area Ridership**\n",
        "\n",
        "![](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/hypothesis_test_results_3_transit.png?raw=true)\n",
        "\n",
        "Based on the high p value of 0.2469, we fail to reject the null. We cannot conclude that Portland's transit ridership is different from the average transit ridership of other large metro areas across the country. Transit ridership may have some impact on AQI but not enough to make a statistical difference by itself. Likely, this is a situation of correlated variables. Cities that invest more in public infrastructure are more likely to make a concerned effort into being environmentally conscious.\n",
        "\n",
        "It should also be noted that some cities have higher public transit numbers not reflected in the data. For example, in New York City, subway ridership is almost double that of its motor bus numbers. In this data only motor bus numbers were included which may contribute to the lack of evidence for transit numbers influence on AQI. We will explore which features have a greater effect on AQI in the upcoming Machine Learning section. \n",
        "\n",
        "\n",
        "# Results Machine Learning Scikit-Learn\n",
        "Ultimately, we want to see which variables have the greatest impact on AQI. To do this we perform a machine learning analysis and create a prediction algorithm. As the AQI is defined by the four criteria gasses and particulate matter (PM10 and PM2.5), those should not be included in the algorithm. Thus we start with all the other variables."
      ],
      "id": "9d42773f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: import-library\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import classification_report"
      ],
      "id": "import-library",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: import-data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil_v2.csv')"
      ],
      "id": "import-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "len(df)\n",
        "df.head()"
      ],
      "id": "35e59492",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: met-dataframe\n",
        "#| echo: false\n",
        "\n",
        "met = df[[\n",
        "    'date',\n",
        "    'state',\n",
        "    'county',\n",
        "    'city',\n",
        "    'population',\n",
        "    'density',\n",
        "    'city_area_km2',\n",
        "    'park_acreage',\n",
        "    'aqi',\n",
        "    'category',\n",
        "    'mean_temperature_fahrenheit',\n",
        "    'mean_pressure_millibars',\n",
        "    'mean_humidity_percent_relative_humidity',\n",
        "    'mean_wind_knots',\n",
        "    'mean_co_ppm',\n",
        "    'mean_no2_ppb',\n",
        "    'mean_ozone_ppm',\n",
        "    'mean_so2_ppb',\n",
        "    'mean_pm100_micrograms_per_cubic_meter',\n",
        "    'mean_pm25_micrograms_per_cubic_meter',\n",
        "    'mean_lead_micrograms_per_cubic_meter',\n",
        "    'num_busses',\n",
        "    'revenue',\n",
        "    'operating_expense',\n",
        "    'passenger_trips',\n",
        "    'operating_hours',\n",
        "    'passenger_miles',\n",
        "    'operating_miles',\n",
        "    'cattle',\n",
        "    'hogs',\n",
        "    'sheep'\n",
        "    ]]\n",
        "\n",
        "met = met.rename(columns={'city_area_km2': 'area',\n",
        "                          'mean_temperature_fahrenheit': 'temp', \n",
        "                          'mean_pressure_millibars': 'pressure', \n",
        "                          'mean_humidity_percent_relative_humidity': 'humidity',\n",
        "                          'mean_wind_knots': 'wind_speed', \n",
        "                          'mean_co_ppm': 'co', \n",
        "                          'mean_no2_ppb': 'no2',\n",
        "                          'mean_ozone_ppm': 'o3', \n",
        "                          'mean_so2_ppb': 'so2', \n",
        "                          'mean_pm100_micrograms_per_cubic_meter': 'pm100',\n",
        "                          'mean_pm25_micrograms_per_cubic_meter': 'pm25', \n",
        "                          'mean_lead_micrograms_per_cubic_meter': 'lead'})"
      ],
      "id": "met-dataframe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: check-NaN\n",
        "\n",
        "met.isna().sum()"
      ],
      "id": "check-NaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: set-index-ts\n",
        "\n",
        "met['date'] = pd.to_datetime(met['date'])\n",
        "met.set_index('date', inplace=True)\n",
        "#group by week, drop lead for too much missing data\n",
        "met = met.groupby([pd.Grouper(freq='W'), 'state', 'county', 'city']).agg({\n",
        "    'population': 'first',\n",
        "    'density': 'first',\n",
        "    'area': 'first',\n",
        "    'park_acreage': 'first',\n",
        "    'aqi': 'mean',\n",
        "    'temp': 'mean',\n",
        "    'pressure': 'mean',\n",
        "    'humidity': 'mean',\n",
        "    'wind_speed': 'mean',\n",
        "    'co': 'mean',\n",
        "    'no2': 'mean',\n",
        "    'o3': 'mean',\n",
        "    'so2': 'mean',\n",
        "    'pm100': 'mean',\n",
        "    'pm25': 'mean',\n",
        "    'num_busses': 'mean',\n",
        "    'revenue': 'mean',\n",
        "    'operating_expense': 'mean',\n",
        "    'passenger_trips': 'mean',\n",
        "    'operating_hours': 'mean',\n",
        "    'passenger_miles': 'mean',\n",
        "    'operating_miles': 'mean',\n",
        "    'cattle': 'first',\n",
        "    'hogs': 'first',\n",
        "    'sheep': 'first'\n",
        "}).reset_index()"
      ],
      "id": "set-index-ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: drop-missNaN-col\n",
        "met = met[met['city'] != 'Virginia Beach']"
      ],
      "id": "drop-missNaN-col",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: drop-null-aqi\n",
        "met = met.dropna(subset=['aqi'])"
      ],
      "id": "drop-null-aqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: check-NaN-again\n",
        "met.isna().sum()"
      ],
      "id": "check-NaN-again",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: group-nyc-1-city\n",
        "nyc = met[met['city'] == 'New York']\n",
        "columns = ['date', 'area', 'park_acreage', 'aqi',\n",
        "           'temp', 'pressure','humidity',\n",
        "           'wind_speed','co','no2','o3',\n",
        "           'so2','pm100','pm25', 'num_busses',\n",
        "           'revenue', 'operating_expense', \n",
        "           'passenger_trips', 'operating_hours', \n",
        "           'passenger_miles', 'operating_miles',\n",
        "           'cattle', 'hogs', 'sheep']\n",
        "\n",
        "nyc = nyc[columns].groupby('date').mean().reset_index()\n",
        "#Add data that was dropped\n",
        "nyc['state'] = 'New York'\n",
        "nyc['county'] = 'Multiple'\n",
        "nyc['city'] = 'New York City'\n",
        "nyc['population'] = 18908608\n",
        "nyc['density'] = 11080.3\n",
        "\n",
        "nyc = nyc[['date', 'state', 'county', 'city', 'population', \n",
        "     'density', 'area', 'park_acreage','aqi', \n",
        "     'temp', 'pressure','humidity',\n",
        "     'wind_speed','co','no2','o3', 'so2','pm100',\n",
        "     'pm25', 'num_busses', 'revenue', 'operating_expense', \n",
        "     'passenger_trips', 'operating_hours', \n",
        "     'passenger_miles', 'operating_miles',\n",
        "     'cattle', 'hogs', 'sheep']]\n",
        "\n",
        "print(nyc)"
      ],
      "id": "group-nyc-1-city",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: replace-nyc-in-met\n",
        "met = met[met['city'] != 'New York']\n",
        "met = pd.concat([met, nyc], ignore_index=True)"
      ],
      "id": "replace-nyc-in-met",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: dataframe-Kansas-City\n",
        "#Do same with Kansas City\n",
        "kc = met[met['city'] == 'Kansas City']\n",
        "columns = ['date', 'area', 'park_acreage', 'aqi',\n",
        "           'temp', 'pressure','humidity',\n",
        "           'wind_speed','co','no2','o3',\n",
        "           'so2','pm100','pm25', 'num_busses',\n",
        "           'revenue', 'operating_expense', \n",
        "           'passenger_trips', 'operating_hours', \n",
        "           'passenger_miles', 'operating_miles',\n",
        "           'cattle', 'hogs', 'sheep']\n",
        "\n",
        "\n",
        "kc = kc[columns].groupby('date').mean().reset_index()\n",
        "\n",
        "columns = ['date', 'area', 'park_acreage', 'aqi',\n",
        "           'temp', 'pressure','humidity',\n",
        "           'wind_speed','co','no2','o3',\n",
        "           'so2','pm100','pm25', 'num_busses',\n",
        "           'revenue', 'operating_expense', \n",
        "           'passenger_trips', 'operating_hours', \n",
        "           'passenger_miles', 'operating_miles',\n",
        "           'cattle', 'hogs', 'sheep']\n",
        "\n",
        "met[columns] = met[columns].fillna(met.groupby('city')[columns].transform('mean'))\n",
        "\n",
        "kc['state'] = 'Missouri'\n",
        "kc['county'] = 'Multiple'\n",
        "kc['city'] = 'Kansas City'\n",
        "kc['population'] = 1689556\n",
        "kc['density'] = 620.7\n",
        "\n",
        "kc = kc[['date', 'state', 'county', 'city', 'population', \n",
        "     'density', 'area', 'park_acreage','aqi', \n",
        "     'temp', 'pressure','humidity',\n",
        "     'wind_speed','co','no2','o3', 'so2','pm100',\n",
        "     'pm25', 'num_busses', 'revenue', 'operating_expense', \n",
        "     'passenger_trips', 'operating_hours', \n",
        "     'passenger_miles', 'operating_miles',\n",
        "     'cattle', 'hogs', 'sheep']]\n",
        "\n",
        "met = met[met['city'] != 'Kansas City']\n",
        "met = pd.concat([met, kc], ignore_index=True)"
      ],
      "id": "dataframe-Kansas-City",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: count-cities\n",
        "ds = met.dropna()\n",
        "\n",
        "ds.groupby('city').size().reset_index(name='count')"
      ],
      "id": "count-cities",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: AQI-bin-ML\n",
        "#bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 150, 500]\n",
        "#labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "\n",
        "bins = [0, 50, 100, 150, 200, 300, 500]\n",
        "labels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-500']\n",
        "\n",
        "ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "ds.head()"
      ],
      "id": "AQI-bin-ML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| label: convert-year-month-datetime\n",
        "ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year\n",
        "ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month"
      ],
      "id": "convert-year-month-datetime",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After data is cleaned, we are left with a total of eighteen cities across the country with a total metropolitan area population of greater than one million. To perform the ML prediction algorithm, AQI will be predicted. We will use existing AQI cateogories as classifiers. A feature selector is run on the set of all variables except for the six that define AQI (CO, NO2, O3, SO2, PM10, PM2.5). This chooses the best predictors of the dependent variable AQI."
      ],
      "id": "bc8bc359"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: aqi-X-y\n",
        "#dependent variable aqi, all others (except aqi category and aqi definition vars) as independent variables\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[['city',\n",
        "        'population',\n",
        "        'density',\n",
        "        'area',\n",
        "        'park_acreage',\n",
        "        'temp',\n",
        "        'pressure',\n",
        "        'humidity',\n",
        "        'wind_speed',\n",
        "        'num_busses', \n",
        "        'revenue', \n",
        "        'operating_expense', \n",
        "        'passenger_trips', \n",
        "        'operating_hours', \n",
        "        'passenger_miles', \n",
        "        'operating_miles',\n",
        "        'cattle',\n",
        "        'hogs',\n",
        "        'sheep',\n",
        "        'year',\n",
        "        'month'\n",
        "        ]]"
      ],
      "id": "aqi-X-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: split-train\n",
        "#Split into train and test datasets, define model and encoder\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')"
      ],
      "id": "split-train",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: def-transformer\n",
        "#define transformer with encoder and standardscaler\n",
        "\n",
        "transformer = ColumnTransformer(\n",
        "        [\n",
        "            ('categories', encoder, ['city','year','month']),\n",
        "            ('scaled_air_quality', StandardScaler(), [\n",
        "                'population',\n",
        "                'density',\n",
        "                'area',\n",
        "                'park_acreage',\n",
        "                'temp',\n",
        "                'pressure',\n",
        "                'humidity',\n",
        "                'wind_speed',\n",
        "                'num_busses', \n",
        "                'revenue', \n",
        "                'operating_expense', \n",
        "                'passenger_trips', \n",
        "                'operating_hours', \n",
        "                'passenger_miles', \n",
        "                'operating_miles',\n",
        "                'cattle',\n",
        "                'hogs',\n",
        "                'sheep'\n",
        "                ]\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop', verbose_feature_names_out=False)\n",
        "#fit transformer\n",
        "transformer.fit(X_train, y_train)"
      ],
      "id": "def-transformer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features Selected in Initial Model:\n",
        "\n",
        "- City\n",
        "- Month\n",
        "- Population\n",
        "- City Area\n",
        "- Park Area\n",
        "- Temperature\n",
        "- Humidity\n",
        "- Cattle\n"
      ],
      "id": "b6142e2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: feature-select\n",
        "#feature selection\n",
        "feature_selector = SelectKBest(k=10)\n",
        "X_train_trans = transformer.transform(X_train)\n",
        "X_train_trans_df = pd.DataFrame(\n",
        "    X_train_trans, \n",
        "    columns = transformer.get_feature_names_out(),\n",
        "    index = X_train.index)\n",
        "feature_selector.fit(X_train_trans_df, y_train)\n",
        "feature_selector.get_support()\n",
        "feature_selector.scores_[feature_selector.get_support()]\n",
        "X_train_trans_df.columns[feature_selector.get_support()]"
      ],
      "id": "feature-select",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: use-features\n",
        "#Use selected features\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[[\n",
        "    'city', 'month', 'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'\n",
        "    ]]\n",
        "#Create train/test data with selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "X_train_enc = pd.DataFrame(encoder.fit_transform(X_train[['city','month']]),\n",
        "                           columns = encoder.get_feature_names_out(), index = X_train.index)\n",
        "X_train_trans = X_train[[\n",
        "    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'\n",
        "    ]].merge(X_train_enc, left_index = True, right_index = True)\n",
        "X_test_enc = pd.DataFrame(encoder.fit_transform(X_test[['city','month']]),\n",
        "                           columns = encoder.get_feature_names_out(), index = X_test.index)\n",
        "X_test_trans = X_test[[\n",
        "    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'\n",
        "    ]].merge(X_test_enc, left_index = True, right_index = True)"
      ],
      "id": "use-features",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| warning: false\n",
        "#| label: model-select\n",
        "model1 = tree.DecisionTreeClassifier()\n",
        "model2 = RandomForestClassifier(n_estimators=10, random_state=12)\n",
        "model3 = LogisticRegression()\n",
        "model4 = GaussianNB()\n",
        "\n",
        "results = []\n",
        "\n",
        "for model, label in zip([model1, model2, model3, model4], ['Tree', 'Random Forest', 'Logistic', 'Naive Bayes']):\n",
        "    model.fit(X_train_trans, y_train)\n",
        "    y_pred = model.predict(X_test_trans)\n",
        "    model = label\n",
        "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
        "    accuracy = sum(y_test == y_pred)/len(y_test)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': label,\n",
        "        'Cohen Kappa Score': cohen_kappa,\n",
        "        'Accuracy': accuracy\n",
        "    })\n",
        "\n",
        "display(pd.DataFrame(results))"
      ],
      "id": "model-select",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In pre testing, it was found that a Random Forest Model performed the best on this dataset. Therefore, this model type is used in all models."
      ],
      "id": "aa489570"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: def-best-model\n",
        "#Create model from most accurate, RandomForest\n",
        "model = RandomForestClassifier(n_estimators=10)\n",
        "model.fit(X_train_trans, y_train)\n",
        "\n",
        "#predict on test data\n",
        "y_pred = model.predict(X_test_trans)"
      ],
      "id": "def-best-model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An initial Random Forest Model run with the selected features returns the following Cohen Kappa score and accuracy:"
      ],
      "id": "d649a162"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: CKS-Accuracy\n",
        "#Print results\n",
        "print(\"Cohen Kappa Score: \", cohen_kappa_score(y_test, y_pred))\n",
        "print(\"Accuracy: \", sum(y_test == y_pred)/len(y_test))"
      ],
      "id": "CKS-Accuracy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: confus-matrix-1\n",
        "#| include: false\n",
        "#| output: false\n",
        "ConfusionMatrixDisplay(\n",
        "    confusion_matrix = confusion_matrix(\n",
        "        y_test, y_pred, \n",
        "        labels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-500']\n",
        "        ), \n",
        "        display_labels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-500']\n",
        "        ).plot(xticks_rotation='vertical')"
      ],
      "id": "confus-matrix-1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure 12459999: Confusion Matrix For Bin Size 1](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/confusion_matrix_1.png?raw=true)\n",
        "\n",
        "Looking only at Cohen Kappa score and accuracy, this seems like a good prediction model. However, the confusion matrix shows the data's skew, centered around the 0-100 range. Therefore, more bins and bin size combinations are tested in order to provide a more meaningful prediction.\n"
      ],
      "id": "f9548671"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: bins-2\n",
        "\n",
        "\n",
        "bins = [0, 25, 50, 75, 100, 200, 500]\n",
        "labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']\n",
        "\n",
        "ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year\n",
        "ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month\n",
        "\n",
        "#dependent variable aqi, all others (except aqi category and aqi definition vars) as independent variables\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[['city',\n",
        "        'population',\n",
        "        'density',\n",
        "        'area',\n",
        "        'park_acreage',\n",
        "        'temp',\n",
        "        'pressure',\n",
        "        'humidity',\n",
        "        'wind_speed',\n",
        "        'num_busses', \n",
        "        'revenue', \n",
        "        'operating_expense', \n",
        "        'passenger_trips', \n",
        "        'operating_hours', \n",
        "        'passenger_miles', \n",
        "        'operating_miles',\n",
        "        'cattle',\n",
        "        'hogs',\n",
        "        'sheep',\n",
        "        'year',\n",
        "        'month'\n",
        "        ]]\n",
        "\n",
        "#Split into train and test datasets, define model and encoder\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')\n",
        "\n",
        "#define transformer with encoder and standardscaler\n",
        "\n",
        "transformer = ColumnTransformer(\n",
        "        [\n",
        "            ('categories', encoder, ['city','year','month']),\n",
        "            ('scaled_air_quality', StandardScaler(), [\n",
        "                'population',\n",
        "                'density',\n",
        "                'area',\n",
        "                'park_acreage',\n",
        "                'temp',\n",
        "                'pressure',\n",
        "                'humidity',\n",
        "                'wind_speed',\n",
        "                'num_busses', \n",
        "                'revenue', \n",
        "                'operating_expense', \n",
        "                'passenger_trips', \n",
        "                'operating_hours', \n",
        "                'passenger_miles', \n",
        "                'operating_miles',\n",
        "                'cattle',\n",
        "                'hogs',\n",
        "                'sheep'\n",
        "                ]\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop', verbose_feature_names_out=False)\n",
        "#fit transformer\n",
        "transformer.fit(X_train, y_train)"
      ],
      "id": "bins-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: feature-select-2\n",
        "#feature selection\n",
        "feature_selector = SelectKBest(k=10)\n",
        "X_train_trans = transformer.transform(X_train)\n",
        "X_train_trans_df = pd.DataFrame(\n",
        "    X_train_trans, \n",
        "    columns = transformer.get_feature_names_out(),\n",
        "    index = X_train.index)\n",
        "feature_selector.fit(X_train_trans_df, y_train)\n",
        "feature_selector.get_support()\n",
        "feature_selector.scores_[feature_selector.get_support()]\n",
        "X_train_trans_df.columns[feature_selector.get_support()]"
      ],
      "id": "feature-select-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: use-features-2\n",
        "#Use selected features\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[[\n",
        "    'city', 'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'\n",
        "    ]]\n",
        "#Create train/test data with selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "X_train_enc = pd.DataFrame(encoder.fit_transform(X_train[['city']]),\n",
        "                           columns = encoder.get_feature_names_out(), index = X_train.index)\n",
        "X_train_trans = X_train[[\n",
        "    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'\n",
        "    ]].merge(X_train_enc, left_index = True, right_index = True)\n",
        "X_test_enc = pd.DataFrame(encoder.fit_transform(X_test[['city']]),\n",
        "                           columns = encoder.get_feature_names_out(), index = X_test.index)\n",
        "X_test_trans = X_test[[\n",
        "    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'\n",
        "    ]].merge(X_test_enc, left_index = True, right_index = True)"
      ],
      "id": "use-features-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: def-best-model-2\n",
        "#Create model from most accurate, RandomForest\n",
        "model = RandomForestClassifier(n_estimators=10)\n",
        "model.fit(X_train_trans, y_train)\n",
        "\n",
        "#predict on test data\n",
        "y_pred = model.predict(X_test_trans)\n",
        "\n",
        "print(\"Cohen Kappa Score: \", cohen_kappa_score(y_test, y_pred))\n",
        "print(\"Accuracy: \", sum(y_test == y_pred)/len(y_test))"
      ],
      "id": "def-best-model-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: confus-matrix2\n",
        "#| include: false\n",
        "#| output: false\n",
        "ConfusionMatrixDisplay(\n",
        "    confusion_matrix = confusion_matrix(\n",
        "        y_test, y_pred, \n",
        "        labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']\n",
        "        ), \n",
        "        display_labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']\n",
        "        ).plot(xticks_rotation='vertical')"
      ],
      "id": "confus-matrix2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After further testing the following bins were decided on:\n",
        "\n",
        "- 0-25\n",
        "- 26-50\n",
        "- 51-75\n",
        "- 76-100\n",
        "- 101-200\n",
        "- 201-500\n",
        "\n",
        "As bins are made smaller, predictions become far less accurate. This bin size serves as a good balance between having enough bins to generate conclusions and maintaining decent accuracy. Different bin sizes also lead to different features being selected. The following features will be present in the final model:\n",
        "\n",
        "- City\n",
        "- Population\n",
        "- City Area\n",
        "- Park Area\n",
        "- Temperature\n",
        "- Humidity\n",
        "- Cattle\n"
      ],
      "id": "d0c74d0e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: redo-bins-2\n",
        "\n",
        "\n",
        "bins = [0, 25, 50, 75, 100, 200, 500]\n",
        "labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']\n",
        "\n",
        "ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year\n",
        "ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month\n",
        "\n",
        "#dependent variable aqi, all others (except aqi category and aqi definition vars) as independent variables\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[['city',\n",
        "        'population',\n",
        "        'area',\n",
        "        'park_acreage',\n",
        "        'temp',\n",
        "        'humidity',\n",
        "        'cattle',\n",
        "        ]]\n",
        "\n",
        "#Split into train and test datasets, define model and encoder\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')\n"
      ],
      "id": "redo-bins-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: improve-transformer\n",
        "#redefined transformer\n",
        "\n",
        "transformer = ColumnTransformer(\n",
        "        [\n",
        "            ('categories', encoder, ['city']),\n",
        "            ('scaled_air_quality', StandardScaler(), [\n",
        "                'population', \n",
        "                'area', \n",
        "                'park_acreage', \n",
        "                'temp', \n",
        "                'humidity', \n",
        "                'cattle'\n",
        "                ]\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop', verbose_feature_names_out=False)\n",
        "#Pipeline for simplification\n",
        "\n",
        "classification_pipeline = Pipeline([('aqi_transformer', transformer),\n",
        "                                    ('RF_model', RandomForestClassifier())\n",
        "                                    ])\n",
        "classification_pipeline.fit(X_train, y_train)\n"
      ],
      "id": "improve-transformer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameter optimization will be done to further improve the model. A randomized search is run with 100 iterations. The following hyperparameters are optimized:\n",
        "\n",
        "- Max Categories\n",
        "- Min Frequency\n",
        "- Max Depth\n",
        "- Max Features\n",
        "- Min Samples Leaf\n",
        "- Min Samples Split\n",
        "- Num Estimators\n",
        "- Bootstrap"
      ],
      "id": "069cf3e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: hyper-parameters-optim\n",
        "#hyperparameter optimization\n",
        "classification_pipeline.get_params()"
      ],
      "id": "hyper-parameters-optim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| label: def-parameters\n",
        "parameters = {\n",
        "    'aqi_transformer__categories__max_categories': randint(3,30),\n",
        "    'aqi_transformer__categories__min_frequency': randint(2,20),\n",
        "    'RF_model__max_depth': randint(3, 30),\n",
        "    'RF_model__max_features': [None, 'sqrt', 'log2'],\n",
        "    'RF_model__min_samples_leaf': randint(2, 10),\n",
        "    'RF_model__min_samples_split': randint(2, 10),\n",
        "    'RF_model__n_estimators': randint(50, 200),\n",
        "    'RF_model__bootstrap': [True, False]\n",
        "}\n",
        "n_iter_search = 100\n",
        "random_search = RandomizedSearchCV(classification_pipeline, param_distributions=parameters,\n",
        "                                   n_iter=n_iter_search, n_jobs=-1, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(random_search.best_score_)"
      ],
      "id": "def-parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Table 9999 below shows the selected hyperparameters."
      ],
      "id": "7bc16dd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: randsearch-get-params\n",
        "random_search.best_params_\n",
        "#random_search.best_estimator_.get_params()\n",
        "params_df = pd.DataFrame(list(random_search.best_params_.items()), columns=['Parameter', 'Value'])\n",
        "\n",
        "# Print the DataFrame\n",
        "display(params_df)"
      ],
      "id": "randsearch-get-params",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Table 9999: Selected Hyperparameters following hyperparamenter optimization\n",
        "\n",
        "Using these hyperparameters, we reach an accuracy of over 70%."
      ],
      "id": "b9239356"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| label: predict-X_test-final\n",
        "y_pred=random_search.predict(X_test)\n",
        "print(\"Cohen Kappa Score: \", cohen_kappa_score(y_test, y_pred))\n",
        "print(\"Accuracy: \", sum(y_pred == y_test)/len(y_test))"
      ],
      "id": "predict-X_test-final",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| label: conf-matrix-final\n",
        "ConfusionMatrixDisplay(\n",
        "    confusion_matrix = confusion_matrix(\n",
        "        y_test, y_pred, \n",
        "        labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']\n",
        "        ), \n",
        "        display_labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']\n",
        "        ).plot(xticks_rotation='vertical')"
      ],
      "id": "conf-matrix-final",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure 12445: Final Model Confusion Matrix](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/confusion_matrix_final.png?raw=true)\n",
        "\n",
        "\n",
        "We can use this model for the prediction of AQI, exploring the features that predict it, and use that to generate conclusions for what variables or systems should be addressed.\n",
        "\n",
        "Below, Table 111111 shows a classification report of the model. Support represents the amount of data points total in that bin. We can see that AQI groups with more data tend to be more accurate (higher f1-score). This makes sense as the model has more of this data to train on. To make the model more accurate, more data from AQI values outside this 26-200 range will need to be collected.\n"
      ],
      "id": "3edc3db9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| label: classification-report\n",
        "report = classification_report(y_test, y_pred, target_names=classification_pipeline.classes_, output_dict=True)\n",
        "\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "df_report = df_report[0:6]\n",
        "df_report = df_report.reindex(['0-25', '26-50', '51-75', '76-100', '101-200', '201-500'])\n",
        "display(df_report)"
      ],
      "id": "classification-report",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Table 111111: Final Model Classification Report)\n",
        "\n",
        "We can also take a look at which features end up being most influential in the model. Below figure 1414 shows Temperature and Humidity being by far the most influential variables in the model. However, temperature and humidity are not variables we can directly impact easily, and therefore, the four other variables are where we should put our efforts."
      ],
      "id": "795270d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| label: feature-importance\n",
        "\n",
        "features = pd.DataFrame(\n",
        "   dict(\n",
        "      variable = model.feature_names_in_,\n",
        "      importance = model.feature_importances_\n",
        "   )\n",
        ")\n",
        "\n",
        "features = features[~features['variable'].str.contains('city_', case=False, na=False)]\n",
        "features = features.sort_values(by='importance', ascending=False)\n",
        "\n",
        "\n",
        "features.replace({\n",
        "    'population': 'Population', \n",
        "    'area': 'Area', \n",
        "    'park_acreage': 'Park Land' ,\n",
        "    'temp': 'Temp',\n",
        "    'humidity': 'Humidity',\n",
        "    'cattle': 'Cattle'\n",
        "    }, inplace=True)\n",
        "\n",
        "\n",
        "plt.bar(features.loc[:,\"variable\"], features.loc[:,\"importance\"], color='skyblue')\n",
        "\n",
        "plt.title('Most Important Features')\n",
        "plt.ylabel('Feature Importance')\n",
        "\n",
        "plt.show()"
      ],
      "id": "feature-importance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Figure 1414: Feature Importance of Selected Variables in Final Model)\n",
        "\n",
        "\n",
        "# Machine Learning SARIMAX"
      ],
      "id": "b0e3a25c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: import-lib\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "import-lib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: read-data2\n",
        "import pandas as pd\n",
        "df = pd.read_pickle('/data/df.pkl')"
      ],
      "id": "read-data2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: define-df-aqi\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "df_aqi = df[['date','aqi']]\n",
        "df_aqi = df_aqi.set_index('date')\n",
        "\n",
        "df_aqi = df_aqi.resample('ME').mean()\n",
        "df_aqi.ffill(inplace=True)\n",
        "#df_aqi.plot(figsize=(15,8))"
      ],
      "id": "define-df-aqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Any time series is composed of three things:\n",
        "\n",
        "- Seasonality\n",
        "- Trends\n",
        "- Residuals\n",
        "\n",
        "By the help of `statsmodel` package we can break the time series into its seasonal pattern and trends.\n",
        "This will helps us to understand the data clearly and will help us to make more sense of the data.\n",
        "\n",
        "# Decomposing the Time Series With Additive Method"
      ],
      "id": "1347e8b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: additive-decomp\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "decomposition = sm.tsa.seasonal_decompose(df_aqi, model='additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ],
      "id": "additive-decomp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are three components to a time series: \n",
        "\n",
        "1. `Trend`: Trend tells you how things are overall changing  \n",
        "2. `Seasonality`: Seasonality shows you how things change within a given period (e.g. year,month, week, day)  \n",
        "3. `Residual`: The Error/residual/irregular activity are the anomalies whitch cannot be explained by the trend or the seasonal value  \n",
        "\n",
        "In a additive time series, the components add together to make the time series. If you have an increasing trend, you still\n",
        "see roughly the same size peaks and troughs throughout the time series. This is often seen in indexed time series where the\n",
        "absolute value is growing but changes stay relative.\n",
        "\n",
        "\n",
        "## Time Series Prediction\n",
        "For this project, we have used an extended version of *ARIMA* model knows as *SARIMAX* model as we have explained in the methods section.\n",
        "The *SARIMAX* model is used when the data sets have seasonal cycles. In our dataset concerning air quality/AQI there is a seasonal pattern\n",
        "which we can see in the above visualization.\n",
        "\n",
        "We need to find the right p,d and q parameters to correctly forecast and predict the AQI value.\n",
        "\n",
        "- *p* is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model.\n",
        "- *d* is the integrated part of the model. This includes terms in the model that incorporate the amount of diferencing (the number of past time points to subtract from the current value) to apply the time series.\n",
        "- *q* is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.  \n",
        "\n",
        "We use a tuning technique called grid search method that attempts to compute the optimum values of hyperparameters. We are trying to find the right p,d,q values that would be given as an input to the SARIMAX time series model."
      ],
      "id": "586a888d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: grid-search\n",
        "#| output: false\n",
        "import itertools\n",
        "\n",
        "# Define the p, d, q parameters to take any value between 0 to 1\n",
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q)) # generate all possible combinations of p,d,q\n",
        "\n",
        "# This creates combinations of seasonal parameters with a seasonality period of 12 (e.g., monthly data).\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in pdq] \n",
        "\n",
        "# Outer Loop: Iterates over all combinations of pdq.\n",
        "# Inner Loop: Iterates over all combinations of seasonal_pdq.\n",
        "#  Try Block:\n",
        "#  Creates a SARIMAX model with the current combination of parameters.\n",
        "#  Fits the model to the data.\n",
        "#  Prints the AIC (Akaike Information Criterion) for the model, which is a measure of model quality.\n",
        "# Except Block:\n",
        "#  Catches and prints any errors that occur during model fitting, allowing the loop to continue with the next set of parameters.\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            mod = sm.tsa.statespace.SARIMAX(df_aqi,\n",
        "                                            order=param,\n",
        "                                            seasonal_order=param_seasonal,\n",
        "                                            enforce_stationarity=False,\n",
        "                                            enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
        "        except Exception as e:\n",
        "            print(f\"Error with parameters {param} and {param_seasonal}: {e}\")\n",
        "            continue"
      ],
      "id": "grid-search",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have to find the lowest AIC values which would have the best corresponding p,d,q values to have the best forecast of AQI values.  \n",
        "\n",
        "# Summary of SARIMAX\n",
        "## Print the summary which includes AIC "
      ],
      "id": "39348020"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mod-fit\n",
        "mod = sm.tsa.statespace.SARIMAX(df_aqi,order=(1, 1, 1),seasonal_order=(0,1, 1, 12),enforce_stationarity=False,enforce_invertibility=False)\n",
        "results = mod.fit()\n",
        "print(results.summary().tables[1])"
      ],
      "id": "mod-fit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How Fit the SARIMAX model"
      ],
      "id": "74882c81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Extract-AIC-value-plot\n",
        "aic_value = results.aic\n",
        "print(f\"The AIC value is: {aic_value}\")"
      ],
      "id": "Extract-AIC-value-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#results.plot_diagnostics(figsize=(10, 8))"
      ],
      "id": "15d9b62d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Plot Diagnostics](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/plot-diag.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "Once the model is created, predicted values are generated using the .get_prediction() method, with datetime as input"
      ],
      "id": "4caf42dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: check-prediction\n",
        "pred = results.get_prediction(start=pd.to_datetime('2023-01-01 00:00:00'), dynamic=False)\n",
        "pred_ci = pred.conf_int()"
      ],
      "id": "check-prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph indicates overlapping patterns in the testing and training data, suggesting strong potential for the forecasting model's performance."
      ],
      "id": "6cb379ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plot-prediction\n",
        "ax = df_aqi['2015-01-31 00:00:00':].plot(label='Observed') # plot the observed data \n",
        "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n",
        "\n",
        "ax.fill_between(pred_ci.index,pred_ci.iloc[:, 0],pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
        "\n",
        "ax.set_xlabel('Days')\n",
        "ax.set_ylabel('AQI index')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "plot-prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To facilitate comparison of true and predicted test values, we will create a separate DataFrame. \n",
        "Mean Error Estimation will be used for analysis."
      ],
      "id": "5071a153"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: define-forecast-truth\n",
        "y_forecasted = pred.predicted_mean\n",
        "y_truth = df_aqi['2022-12-31 00:00:00':]"
      ],
      "id": "define-forecast-truth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate model performance, we calculate the MSE."
      ],
      "id": "4fdc71b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: calculate-mse\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = np.sqrt(mean_squared_error(y_truth, y_forecasted))   \n",
        "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
      ],
      "id": "calculate-mse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecasting Future Values\n",
        "\n",
        "As we conclude our modeling process, we generate predictions for the next 30 data points:\n",
        "\n",
        "1. **Model Information**: The `result` variable contains our fitted model's details.\n",
        "\n",
        "2. **Forecasting Method**: We use the `.get_forecast()` method on our model results.\n",
        "\n",
        "3. **Prediction Generation**: This method analyzes observed patterns in our data to project future values.\n",
        "\n",
        "4. **Output**: We obtain forecasts for the next 30 time points, representing predicted air quality levels.\n",
        "\n",
        "This step transforms our analytical work into actionable insights for air quality management."
      ],
      "id": "9fc277fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: recheck-forecast\n",
        "pred_uc = results.get_forecast(steps=30)\n",
        "pred_ci = pred_uc.conf_int()"
      ],
      "id": "recheck-forecast",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: show tab-pred\n",
        "#| output: false\n",
        "# Get the forecasted values\n",
        "forecasted_values = pred_uc.predicted_mean\n",
        "\n",
        "# Get the forecasted dates\n",
        "forecasted_dates = forecasted_values.index\n",
        "\n",
        "# Create a DataFrame with the dates and forecasted values\n",
        "forecast_df = pd.DataFrame({\n",
        "    'Date': forecasted_dates,\n",
        "    'Forecasted Value': forecasted_values\n",
        "})\n",
        "\n",
        "# Print the DataFrame as a table\n",
        "#print(forecast_df.head(30).to_markdown(index=False))"
      ],
      "id": "show-tab-pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Our Results: The Culmination of Our Analysis\n",
        "\n",
        "The final and crucial step of our project is the creation of a comprehensive plot that encapsulates our complex analysis. This visualization serves as the key to understanding and interpreting our findings.\n",
        "\n",
        "### Interpreting the Forecast Plot\n",
        "\n",
        "Our plot consists of several key elements:\n",
        "\n",
        "1. **Observed Values (Blue Line)**\n",
        "   - Represents the actual, historical air quality measurements\n",
        "   - Provides a baseline for comparing our predictions\n",
        "\n",
        "2. **Forecasted Values (Orange Line)**\n",
        "   - Depicts the future air quality levels predicted by our SARIMAX Time Series Model\n",
        "   - Allows us to visualize potential trends and patterns in air quality\n",
        "\n",
        "3. **Confidence Interval (Shaded Region)**\n",
        "   - The shaded area around the forecast line represents the 95% Confidence Interval (CI)\n",
        "   - Indicates the range within which we can be 95% confident that the true future values will fall\n",
        "   - Wider intervals suggest greater uncertainty in the prediction\n",
        "\n",
        "This visual representation not only summarizes our extensive data analysis but also provides a powerful tool for understanding potential future air quality trends. It bridges the gap between complex statistical models and actionable insights, making our findings accessible and meaningful to a broader audience."
      ],
      "id": "5b82f236"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plot-forecast\n",
        "ax = df_aqi.plot(label='Observed', figsize=(14, 7))\n",
        "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
        "ax.fill_between(pred_ci.index,pred_ci.iloc[:, 0],pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
        "ax.set_xlabel('Days')\n",
        "ax.set_ylabel('AQI index')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "plot-forecast",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "After conducting our thorough research, we have landed on these specific recommendations. We found there are high seasonal trends where late summer/early fall tends to have the worst air quality. These numbers consistently show up each year, exasperated by the dry heat and lack of rainfall. As climate change raises temperatures and water sources dry up, wildfire season will continue to get worse over time. We must be aware of the nature of air quality and how it differs at different parts of the year. We should understand how the AQI works, and avoid being outside for too long when it reaches more dangerous levels.\n",
        "\n",
        "Air quality is significantly affected by various natural and unpredictable elements, including:\n",
        "\n",
        "- Weather conditions\n",
        "- Wind speed and direction\n",
        "- Temperature fluctuations\n",
        "- Humidity levels\n",
        "- Atmospheric pressure\n",
        "- Solar radiation intensity\n",
        "\n",
        "With multiple factors we have no control over, it is important to do what we can for those factors we can impact.\n",
        "\n",
        "We must focus on the variables that have the greatest impact on the AQI. These are city, temperature, humidity, CO, NO2, O3, PM10, and PM2.5. City, temperature, and humidity for the most part are out of our control. That leaves us with three criteria gasses and all particulate matter. The largest source of carbon monoxide, nitrogen dioxide, and ozone is the cars, trucks, and other vehicles we use daily (Environmental Protection Agency). We can lower our reliance on personal vehicles by utilizing public transportation, carpooling, walking, biking, increasing work from home to lower commutes when available, and overall be more considerate about if driving a car is necessary. We often drive unnecessarily, out of convenience or impatience. \n",
        "\n",
        "For many Americans, personal vehicles are required. Many urban and suburban areas lack proper public transportation, or existing public transportation systems are inadequate, unreliable, and infrequent. Many towns and cities in the United States were designed for cars instead of for people. Fixing this will require a substantial overhaul, necessitating millions or billions of dollars in spending. This is not to say that we shouldn’t bother---any investment that increases the public transportation system’s usage decreases the amount of cars on the road. A small change is the first step in addressing the personal vehicle issue.\n",
        "\n",
        "Industrial manufacturing processes and agriculture are significant polluters of the environment. We should invest in the research of more environmentally friendly manufacturing methods, working with materials that require less combustion, or are recyclable. Agricultural reduction starts with less of a reliance on red meat and the dairy industry, mainstays of the American diet. This will be a huge shift, taking combined efforts of the citizens, government, and food industry. As red meat and dairy consumption goes down, possibly due to the replacement with artificial or lab grown meat, less livestock will need to be kept, and less feeding crops will need to be grown (Congressional Budget Office). It will be a difficult transition but a necessary one.\n",
        "\n",
        "Wildfires not only increase the particle matter in the air, but burn forests, causing long term damage to the soil. Particulate matter in the air makes it more difficult to breathe, which is reflected by the increased AQI levels. Not all forest fires are started by man made sources, but many are. Therefore, when in the woods, one should always obey fire restrictions, especially in the middle of the summer when it’s most dry. If fires are allowed, they should always be watched and never left unattended. They must always be properly extinguished and all embers must be cool to the touch before leaving. Campsites should be properly cleaned, and all tools used correctly. One should also stay on marked trails, avoiding trampling vegetation which can increase the risk of wildfire spreading (Oregon Wildfire Response and Recovery).\n",
        "\n",
        "One of the largest limitations is Algorithm Dependence. This is the reliability of forecasts which are inherently tied to the chosen predictive algorithms. Different models may yield varying results, emphasizing the importance of algorithm selection and validation.\n",
        "\n",
        "We must also be wary of geographical considerations. Our data analysis couldn't fully quantify the unique geographical features of Portland and the broader Willamette Valley region. In further analysis we should seek to understand: \n",
        "\n",
        "- The protective influence of surrounding mountain ranges\n",
        "- The impact on wind patterns and air circulation\n",
        "- The potential effects of wildfires on air quality\n",
        "\n",
        "These geographical factors play a significant role in local air quality dynamics but were beyond the scope of our current data set. The impact of the geological features can be seen in the image below illustrates how the surrounding mountains create a basin effect, trapping pollutants and contributing to higher air pollution levels. \n",
        "The concentrated red and orange areas, particularly on right side of the Willamette Valley.Do the mountains in the image seem to create barriers to air flow, potentially trapping pollutants in certain areas?\n",
        "![AirNow Interactive Map](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/AirNow-Map.png?raw=true) \n",
        "\n",
        "For more information the EPA AirNow website, please visit the ![Interactive Map of Air Quality](https://gispub.epa.gov/airnow/?monitors=none&contours=pm25&boundaries=tribal%2Fnps%2Fcounty&xmin=-14017111.250925817&xmax=-13198317.803935051&ymin=5220588.626956775&ymax=6039993.570173823)\n",
        "\n",
        "By recognizing these limitations, we can better interpret and apply our forecasting results, while also identifying areas for future research and data collection to enhance prediction accuracy.\n",
        "\n",
        "Ultimately, we have a responsibility to take care of our planet and combat climate change. The worse climate conditions get, the more wildfires will spread, and the worse the air quality will become. As time goes on, with worsening air conditions, more people will catch and even die from preventable conditions sparked by poor air quality.  Water, pollution, food, and financial problems will get worse. One should look at what they can do to make a difference, support those who vouch to make larger changes, and encourage people they know to do the same. While the situation may seem dire, there is hope for progress through concerted and informed efforts.\n",
        "\n",
        "\n",
        "# Appendix\n",
        "## Bibliography\n",
        "Airly. (n.d.). *How does humidity affect air quality? All you need to know*. [airly.org/en/how-does-humidity-affect-air-quality-all-you-need-to-know/](https://airly.org/en/how-does-humidity-affect-air-quality-all-you-need-to-know/)\n",
        "\n",
        "AirNow. (n.d.). *Using air quality index*. [www.airnow.gov/aqi/aqi-basics/using-air-quality-index](https://www.airnow.gov/aqi/aqi-basics/using-air-quality-index)\n",
        "\n",
        "American Lung Association. (n.d.). *Key findings: State of the air*. [www.lung.org/research/sota/key-findings](https://www.lung.org/research/sota/key-findings)\n",
        "\n",
        "California Air Resources Board. (n.d.). *Carbon monoxide & health*. [ww2.arb.ca.gov/resources/carbon-monoxide-and-health](https://ww2.arb.ca.gov/resources/carbon-monoxide-and-health)\n",
        "\n",
        "Centers for Disease Control and Prevention, Agency for Toxic Substances and Disease Registry. (2014, October 21). *Medical management guidelines for sulfur dioxide*. [wwwn.cdc.gov/TSP/MMG/MMGDetails.aspx?mmgid=249&toxid=46](https://wwwn.cdc.gov/TSP/MMG/MMGDetails.aspx?mmgid=249&toxid=46)\n",
        "\n",
        "Centers for Disease Control and Prevention, Agency for Toxic Substances and Disease Registry. (2023, April 12). *Toxic substances portal - Nitrogen oxides*. [wwwn.cdc.gov/TSP/ToxFAQs/ToxFAQsDetails.aspx?faqid=396&toxid=69](https://wwwn.cdc.gov/TSP/ToxFAQs/ToxFAQsDetails.aspx?faqid=396&toxid=69)\n",
        "\n",
        "Congressional Budget Office. (2023, May). *Reducing emissions from transportation*. [www.cbo.gov/publication/60030](https://www.cbo.gov/publication/60030)\n",
        "\n",
        "Environmental Protection Agency. (n.d.-a). *Evolution of the Clean Air Act*. [www.epa.gov/clean-air-act-overview/evolution-clean-air-act](https://www.epa.gov/clean-air-act-overview/evolution-clean-air-act)\n",
        "\n",
        "Environmental Protection Agency. (n.d.-b). *Health effects of ozone pollution*. [www.epa.gov/ground-level-ozone-pollution/health-effects-ozone-pollution](https://www.epa.gov/ground-level-ozone-pollution/health-effects-ozone-pollution)\n",
        "\n",
        "Environmental Protection Agency. (n.d.-c). *Particulate matter (PM) basics*. [www.epa.gov/pm-pollution/particulate-matter-pm-basics](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics)\n",
        "\n",
        "Federal Transit Administration. (n.d.). *NTD data tables*. American Public Transportation Association. [www.apta.com/research-technical-resources/transit-statistics/ntd-data-tables/](https://www.apta.com/research-technical-resources/transit-statistics/ntd-data-tables/)\n",
        "\n",
        "Fuller, R., Landrigan, P. J., Balakrishnan, K., Bathan, G., Bose-O'Reilly, S., Brauer, M., Caravanos, J., Chiles, T., Cohen, A., Corra, L., Cropper, M., Ferraro, G., Hanna, J., Hanrahan, D., Hu, H., Hunter, D., Janata, G., Kupka, R., Lanphear, B., . . . Yadama, G. N. (2022). Pollution and health: A progress update. *The Lancet Planetary Health*, *6*(6), e535-e547. [www.thelancet.com/journals/lanplh/article/PIIS2542-5196(22)00090-0/fulltext](https://www.thelancet.com/journals/lanplh/article/PIIS2542-5196(22)00090-0/fulltext)\n",
        "\n",
        "Jacobs, E. T., Burgess, J. L., & Abbott, M. B. (2018). The Donora smog revisited: 70 years after the event that inspired the clean air act. *American Journal of Public Health*, *108*(S2), S85-S88. [www.ncbi.nlm.nih.gov/pmc/articles/PMC5922205/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5922205/)\n",
        "\n",
        "Manisalidis, I., Stavropoulou, E., Stavropoulos, A., & Bezirtzoglou, E. (2020). Environmental and health impacts of air pollution: A review. *Frontiers in Public Health*, *8*, 14. [www.ncbi.nlm.nih.gov/pmc/articles/PMC7044178/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7044178/)\n",
        "\n",
        "National Oceanic and Atmospheric Administration. (n.d.). *What are a nautical mile and a knot?* [oceanservice.noaa.gov/facts/nautical-mile-knot.html](https://oceanservice.noaa.gov/facts/nautical-mile-knot.html)\n",
        "\n",
        "National Weather Service. (n.d.). *Pressure and winds*. [www.weather.gov/source/zhu/ZHU_Training_Page/winds/pressure_winds/Pressure.htm](https://www.weather.gov/source/zhu/ZHU_Training_Page/winds/pressure_winds/Pressure.htm)\n",
        "\n",
        "Oregon Wildfire Response and Recovery. (n.d.). *Wildfire prevention*. [wildfire.oregon.gov/prevention](https://wildfire.oregon.gov/prevention)\n",
        "\n",
        "World Health Organization. (2014, March 25). *7 million premature deaths annually linked to air pollution*. [www.who.int/news/item/25-03-2014-7-million-premature-deaths-annually-linked-to-air-pollution](https://www.who.int/news/item/25-03-2014-7-million-premature-deaths-annually-linked-to-air-pollution)\n",
        "\n",
        "World Health Organization. (2022, October 31). *Lead poisoning and health*. [www.who.int/news-room/fact-sheets/detail/lead-poisoning-and-health](https://www.who.int/news-room/fact-sheets/detail/lead-poisoning-and-health)\n",
        "\n",
        "Zhang, Y., Cooper, O. R., Gaudel, A., Thompson, A. M., Nédélec, P., Ogino, S. Y., & West, J. J. (2018). Tropospheric ozone change from 1980 to 2010 dominated by equatorward redistribution of emissions. *Nature Geoscience*, *11*, 637-644. [acp.copernicus.org/articles/18/15003/2018/](https://acp.copernicus.org/articles/18/15003/2018/)\n",
        "\n",
        "\n",
        "\n",
        "## Additional Resources - EDA"
      ],
      "id": "349dfdf8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: import-package\n",
        "from sqlalchemy import create_engine, text\n",
        "import dotenv\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import missingno as msno\n",
        "from summarytools import dfSummary\n",
        "import sweetviz as sv\n",
        "from ydata_profiling import ProfileReport"
      ],
      "id": "import-package",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis\n",
        "We have new dataset metro_1mil.csv. This file was generated using a SQL statement that joins all relevant tables, filtering for metropolitan areas with populations less than or equal to 1 million. This approach limits our EDA to mid-sized metropolitan cities, such as Portland, Oregon."
      ],
      "id": "2ffa6f25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: read-data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')"
      ],
      "id": "read-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the AQI data distribution"
      ],
      "id": "9286eb08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plot-aqi\n",
        "#| output: false\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['aqi'], kde=True)\n",
        "\n",
        "plt.title('Distribution of Air Quality Index')\n",
        "plt.xlabel('AQI')\n",
        "plt.show()"
      ],
      "id": "plot-aqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: describe-shape\n",
        "num_rows, num_columns = df.shape"
      ],
      "id": "describe-shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The DataFrame contains `{python} num_rows` rows and `{python} num_columns` columns."
      ],
      "id": "34bb53b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: filter-df\n",
        "df = df[df['state'] == 'Oregon']\n",
        "num_rows_oregon = df.shape[0]"
      ],
      "id": "filter-df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By filtering our Dataframe for Oregon state, our DataFrame contains `{python} num_rows_oregon` rows.\n",
        "\n",
        "### Features Engineering"
      ],
      "id": "66b65439"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: convert-dt\n",
        "df['date'] = pd.to_datetime(df['date'])"
      ],
      "id": "convert-dt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: df-drop-col\n",
        "df = df.drop(['aqi_range', 'st_abbv'], axis=1)\n",
        "df = df.drop(['mean_lead_micrograms_per_cubic_meter', 'max_lead_micrograms_per_cubic_meter'], axis=1)\n",
        "df = df.drop(['mean_pm100_micrograms_per_cubic_meter', 'max_pm100_micrograms_per_cubic_meter'], axis=1)\n",
        "df = df.drop(['mean_pm25_micrograms_per_cubic_meter', 'max_pm25_micrograms_per_cubic_meter'], axis=1)"
      ],
      "id": "df-drop-col",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: df-ampute\n",
        "columns_to_impute = [\n",
        "    'mean_temperature_fahrenheit', 'max_temperature_fahrenheit',\n",
        "    'mean_pressure_millibars', 'max_pressure_millibars',\n",
        "    'mean_humidity_percent_relative_humidity', 'max_humidity_percent_relative_humidity',\n",
        "    'mean_wind_knots', 'max_wind_knots',\n",
        "    'mean_co_ppm', 'max_co_ppm',\n",
        "    'mean_no2_ppb', 'max_no2_ppb',\n",
        "    'mean_ozone_ppm', 'max_ozone_ppm',\n",
        "    'mean_so2_ppb', 'max_so2_ppb'\n",
        "]\n",
        "\n",
        "means = {col: df[col].mean() for col in columns_to_impute}\n",
        "df = df.assign(**{col: df[col].fillna(means[col]) for col in columns_to_impute})"
      ],
      "id": "df-ampute",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Date Column Preprocessing:\n",
        "\n",
        "- Converted the date column to DateTime objects for easier manipulation and analysis.\n",
        "- Extracted additional time-based features: year, month, day of week, and quarter.\n",
        "\n",
        "Feature Selection:\n",
        "\n",
        "- Removed irrelevant columns to focus the analysis on pertinent variables.\n",
        "- Retained features: pollutant, aqi, wind\n",
        "\n",
        "Missing Value Treatment:\n",
        "\n",
        "- Identified columns with missing values: most all of them\n",
        "- Applied mean() imputation method for numerical columns.\n",
        "- For categorical columns: n/a\n",
        "\n",
        "Data Types and Memory Usage:\n",
        "\n",
        "- Optimized data types to reduce memory usage (e.g., using categories for low-cardinality strings, int8/int16 for small integers).\n",
        "\n",
        "Basic Statistics:\n",
        "\n",
        "- Generated summary statistics for numerical columns using df.describe().\n",
        "- Calculated frequency distributions for categorical variables.\n",
        "\n",
        "Distribution Analysis:\n",
        "\n",
        "- Plotted histograms and kernel density estimates for main numerical features.\n",
        "\n",
        "Time Series Components:\n",
        "\n",
        "- Decomposed time series data into trend, seasonality, and residual components for relevant variables.\n",
        "\n",
        "### Sweetviz Data Report"
      ],
      "id": "15d6717a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: sweetvis-report\n",
        "#| output: false\n",
        "my_report = sv.analyze(df)\n",
        "my_report.show_html()"
      ],
      "id": "sweetvis-report",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have generated Sweetviz statistical report confirming the quality of EDA steps.\n",
        "\n",
        "### Advanced Data Analysis\n",
        "We have also employed the ydata-profiling package, a powerful Time Series Analysis EDA package that offers more detailed analysis.\n",
        "\n",
        "We have unlocked time series-specific features using ydata-profiling:\n",
        "We ensure our DataFrame is sorted or specify the sortby parameter, setting tsmode=True when creating the ProfileReport to allow Time Series Feature Identification\n",
        "\n",
        "The ydata-profiling identifies time-dependent features using autocorrelation analysis.  \n",
        "For recognized time series features, the histograms are replaced with line plots and feature details include new autocorrelation and partial autocorrelation plots.\n",
        "\n",
        "To handle Multi-Entity Time Series Data, In our case, with category_id, each pollutants represents a distinct time series.\n",
        "For optimal analysis, we filter and profile each pollutant separately"
      ],
      "id": "9e600b47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ydata-report\n",
        "#for group in df.groupby(\"category_id\"):\n",
        "#    # Running 1 profile per station\n",
        "#    profile = ProfileReport(\n",
        "#        group[1],\n",
        "#        minimal=True,\n",
        "#        sortby=\"date\",\n",
        "#        # title=f\"Air Quality profiling - Site Num: {group[0]}\"\n",
        "#    )\n",
        "\n",
        " #   profile.to_file(f\"Ts_Profile_{group[0]}.html\")"
      ],
      "id": "ydata-report",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ydata-render\n",
        "#profile = ProfileReport(\n",
        "#    group[1],\n",
        "#    tsmode=True,\n",
        "#    sortby=\"date\",\n",
        "#    # title=f\"Air Quality profiling - Site Num: {group[0]}\"\n",
        "#)\n",
        "#profile.to_file(\"your_report2.html\")"
      ],
      "id": "ydata-render",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: quarto-cross-ref pickle\n",
        "#df.to_pickle('/data/df.pkl')"
      ],
      "id": "quarto-cross-ref-pickle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To conclude our exploratory data analysis (EDA) process consisted of two complementary approaches:\n",
        "\n",
        "- *Manual Investigation*: We conducted an in-depth, hands-on examination of the dataset.\n",
        "- *Automated Analysis*: We leveraged two powerful EDA packages: Sweetviz For quick, visual data summaries, ydata-profiling for more detailed, customizable reports  \n",
        "\n",
        "These methods allowed us to thoroughly evaluate key data quality aspects, including:\n",
        "\n",
        "- Class balance in categorical variables\n",
        "- Presence and distribution of missing values (NaN)\n",
        "- Feature distributions and correlations\n",
        "- Potential time-series characteristics\n",
        "\n",
        "This multi-faceted approach ensures a robust understanding of our dataset's structure, quality, and potential challenges before proceeding with further analysis."
      ],
      "id": "f39acf9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: df-aqi\n",
        "df_aqi = df[['date', 'aqi']]\n",
        "df_aqi = df_aqi.set_index('date')"
      ],
      "id": "df-aqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time Series Visualization in Portland\n",
        "#### Carbon Monoxyde (CO), Wind and Air Quality Index (AQI)\n",
        "CO pollutant refers to carbon monoxide, which is a colorless, odorless, and tasteless gas that can be harmful to human health and the environment. Here's some key information about CO as a pollutant:\n",
        "\n",
        "Primarily produced by incomplete combustion of carbon-containing fuels\n",
        "Major sources include vehicle exhaust, industrial processes, and some natural sources like volcanoes\n",
        "\n",
        "- Slightly less dense than air\n",
        "- Highly flammable"
      ],
      "id": "047d4e08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ts-wind-co-aqi\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# Prepare wind data\n",
        "df_wind = df[['date', 'mean_wind_knots', 'max_wind_knots']]\n",
        "df_wind = df_wind.set_index('date')\n",
        "\n",
        "df_wind_year = df_wind.resample('YE').mean().assign(Resample='Year')\n",
        "df_wind_month = df_wind.resample('ME').mean().assign(Resample='Month')\n",
        "df_wind_day = df_wind.resample('D').mean().assign(Resample='Day')\n",
        "\n",
        "df_wind_combined = pd.concat([df_wind_year, df_wind_month, df_wind_day])\n",
        "df_wind_combined.ffill(inplace=True)\n",
        "df_wind_combined.reset_index(inplace=True)\n",
        "\n",
        "# Prepare CO data\n",
        "df_co = df[['date', 'mean_co_ppm', 'max_co_ppm']]\n",
        "df_co = df_co.set_index('date')\n",
        "\n",
        "df_co_year = df_co.resample('YE').mean().assign(Resample='Year')\n",
        "df_co_month = df_co.resample('ME').mean().assign(Resample='Month')\n",
        "df_co_day = df_co.resample('D').mean().assign(Resample='Day')\n",
        "\n",
        "df_co_combined = pd.concat([df_co_year, df_co_month, df_co_day])\n",
        "df_co_combined.ffill(inplace=True)\n",
        "df_co_combined.reset_index(inplace=True)\n",
        "\n",
        "# Prepare AQI data\n",
        "df_aqi = df[['date','aqi']]\n",
        "df_aqi = df_aqi.set_index('date')\n",
        "\n",
        "df_aqi_year = df_aqi.resample('YE').mean().assign(Resample='Year')\n",
        "df_aqi_month = df_aqi.resample('ME').mean().assign(Resample='Month')\n",
        "df_aqi_day = df_aqi.resample('D').mean().assign(Resample='Day')\n",
        "\n",
        "df_aqi_combined = pd.concat([df_aqi_year, df_aqi_month, df_aqi_day])\n",
        "df_aqi_combined.ffill(inplace=True)\n",
        "df_aqi_combined.reset_index(inplace=True)\n",
        "\n",
        "# Merge the three DataFrames\n",
        "df_combined = pd.merge(df_wind_combined, df_co_combined, on=['date', 'Resample'], suffixes=('_wind', '_co'))\n",
        "df_combined = pd.merge(df_combined, df_aqi_combined, on=['date', 'Resample'])\n",
        "\n",
        "# Melt the DataFrame for FacetGrid\n",
        "df_melted = df_combined.melt(id_vars=['date', 'Resample'], value_vars=['mean_wind_knots', 'mean_co_ppm', 'aqi'], \n",
        "                             var_name='Variable', value_name='Value')\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 18))\n",
        "\n",
        "# Plot using seaborn\n",
        "g = sns.FacetGrid(df_melted, row='Resample', col='Variable', hue='Resample', sharex=True, sharey=False, height=4, aspect=3)\n",
        "g.map(sns.lineplot, 'date', 'Value')\n",
        "\n",
        "# Adjust the plot\n",
        "g.add_legend()\n",
        "g.set_axis_labels('Date', 'Value')\n",
        "plt.subplots_adjust(top=0.9)\n",
        "g.fig.suptitle('Mean Wind Knots, CO PPM, and AQI Resampled by Year, Month, and Day')\n",
        "plt.show()"
      ],
      "id": "ts-wind-co-aqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Nitrogen Dioxide (NO2), Sulfur Dioxyde (SO2) and Ozone (O₃)\n",
        "NO2 (nitrogen dioxide) is an important air pollutant.\n",
        "- Reddish-brown gas with a pungent odor\n",
        "- Part of a group of pollutants known as nitrogen oxides (NOx)\n",
        "\n",
        "SO2 (sulfur dioxide) is an important air pollutant.\n",
        "- Colorless gas with a sharp, pungent odor\n",
        "- Highly soluble in water\n",
        "\n",
        "Ozone (O₃) as a pollutant is a complex topic, as it can be both beneficial and harmful depending on its location in the atmosphere. \n",
        "- Colorless to pale blue gas with a distinctive smell\n",
        "- Highly reactive molecule composed of three oxygen atoms"
      ],
      "id": "7d5af060"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ts-so2-no2-ozone\n",
        "#| warning: false\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# Filter and resample SO2 data\n",
        "df_so2 = df[['date', 'mean_so2_ppb', 'max_so2_ppb']]\n",
        "df_so2 = df_so2.set_index('date')\n",
        "\n",
        "df_so2_year = df_so2.resample('YE').mean().assign(Resample='Year')\n",
        "df_so2_month = df_so2.resample('ME').mean().assign(Resample='Month')\n",
        "df_so2_day = df_so2.resample('D').mean().assign(Resample='Day')\n",
        "\n",
        "df_so2_combined = pd.concat([df_so2_year, df_so2_month, df_so2_day])\n",
        "df_so2_combined.ffill(inplace=True)\n",
        "df_so2_combined.reset_index(inplace=True)\n",
        "\n",
        "# Filter and resample NO2 data\n",
        "df_no2 = df[['date', 'mean_no2_ppb', 'max_no2_ppb']]\n",
        "df_no2 = df_no2.set_index('date')\n",
        "\n",
        "df_no2_year = df_no2.resample('YE').mean().assign(Resample='Year')\n",
        "df_no2_month = df_no2.resample('ME').mean().assign(Resample='Month')\n",
        "df_no2_day = df_no2.resample('D').mean().assign(Resample='Day')\n",
        "\n",
        "df_no2_combined = pd.concat([df_no2_year, df_no2_month, df_no2_day])\n",
        "df_no2_combined.ffill(inplace=True)\n",
        "df_no2_combined.reset_index(inplace=True)\n",
        "\n",
        "# Filter and resample ozone data\n",
        "df_ozone = df[['date', 'mean_ozone_ppm', 'max_ozone_ppm']]\n",
        "df_ozone = df_ozone.set_index('date')\n",
        "\n",
        "df_ozone_year = df_ozone.resample('YE').mean().assign(Resample='Year')\n",
        "df_ozone_month = df_ozone.resample('ME').mean().assign(Resample='Month')\n",
        "df_ozone_day = df_ozone.resample('D').mean().assign(Resample='Day')\n",
        "\n",
        "df_ozone_combined = pd.concat([df_ozone_year, df_ozone_month, df_ozone_day])\n",
        "df_ozone_combined.ffill(inplace=True)\n",
        "df_ozone_combined.reset_index(inplace=True)\n",
        "\n",
        "# Merge the three DataFrames\n",
        "df_combined = pd.merge(df_so2_combined, df_no2_combined, on=['date', 'Resample'], suffixes=('_so2', '_no2'))\n",
        "df_combined = pd.merge(df_combined, df_ozone_combined, on=['date', 'Resample'])\n",
        "\n",
        "# Melt the DataFrame for FacetGrid\n",
        "df_melted = df_combined.melt(id_vars=['date', 'Resample'], \n",
        "                             value_vars=['mean_so2_ppb', 'mean_no2_ppb', 'mean_ozone_ppm'], \n",
        "                             var_name='Variable', value_name='Value')\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 20))\n",
        "\n",
        "# Plot using seaborn\n",
        "g = sns.FacetGrid(df_melted, row='Resample', col='Variable', hue='Resample',sharex=True, sharey=False, height=6, aspect=2)\n",
        "g.map(sns.lineplot, 'date', 'Value')\n",
        "\n",
        "# Adjust the plot\n",
        "g.add_legend()\n",
        "#g.set_axis_labels('Date', 'Value (PPB/PPM)')\n",
        "plt.subplots_adjust(top=0.9)\n",
        "g.fig.suptitle('Mean SO2, NO2, and Ozone Resampled by Year, Month, and Day')\n",
        "plt.show()"
      ],
      "id": "ts-so2-no2-ozone",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Resources - Machine Learning Metrics Table\n",
        "| Technique/Metric | Description | Purpose/Formula | Scenario: Cancer prediction |\n",
        "|------------------|-------------|-----------------|-------------------|\n",
        "| 1. Train-Test Split | Split the dataset into training and testing subsets | Assess model performance on unseen data to detect overfitting and ensure generalizability | Always used; crucial for unbiased evaluation of model performance |\n",
        "| 2. Cross-Validation | Divide data into k subsets and train the model k times, using a different subset as test set each time | Provides robust estimate of model performance by averaging results over multiple splits | Useful for smaller datasets or when data collection is expensive (e.g., rare cancer types) |\n",
        "| 3. Confusion Matrix | Table comparing predicted and actual values in classification | Metrics: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN) | Fundamental for understanding model performance in classification tasks, like cancer detection |\n",
        "| 4. Accuracy | Ratio of correctly predicted instances to total instances | $\\frac{TP + TN}{TP + TN + FP + FN}$ | Used when classes are balanced; less suitable for rare cancer detection due to class imbalance |\n",
        "| 5a. Precision | Ratio of correctly predicted positive observations to total predicted positives | $\\frac{TP}{TP + FP}$ | Important when false positives are costly (e.g., unnecessary biopsies or treatments) |\n",
        "| 5b. Recall (Sensitivity) | Ratio of correctly predicted positive observations to all actual positive observations | $\\frac{TP}{TP + FN}$ | Critical in cancer detection to minimize false negatives (missed cancer cases) |\n",
        "| 5c. F1-Score | Harmonic mean of Precision and Recall | $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Balances precision and recall; useful when seeking a compromise between false positives and false negatives |\n",
        "| 6. ROC Curve and AUC | ROC: Graph of true positive rate vs false positive rate at various thresholds. AUC: Area under ROC curve | Higher AUC indicates better model performance | Useful for comparing models and choosing optimal threshold, especially in diagnostic tests |\n",
        "| 7. Mean Absolute Error (MAE) | Average of absolute differences between predicted and actual values | $\\frac{1}{n} \\sum_{i=1}^{n} \\|y_i - \\hat{y}_i\\|$ | Used in regression tasks, e.g., predicting survival time; less sensitive to outliers than MSE |\n",
        "| 8a. Mean Squared Error (MSE) | Average of squared differences between predicted and actual values | $\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ | Used in regression; penalizes large errors more, suitable when large errors are particularly undesirable |\n",
        "| 8b. Root Mean Squared Error (RMSE) | Square root of MSE | $\\sqrt{\\text{MSE}}$ | Same as MSE, but in the original unit of the target variable, making it more interpretable |\n",
        "| 9. R-squared | Proportion of variance in dependent variable predictable from independent variables | $1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$ | Used in regression to assess overall fit; indicates how well the model explains the variance in the data |\n",
        "| 10a. Akaike Information Criterion (AIC) | Measures relative quality of statistical model for given data | $2k - 2\\ln(L)$ where $k$ is number of parameters and $L$ is likelihood | Used for model selection; helps prevent overfitting by penalizing complex models |\n",
        "| 10b. Bayesian Information Criterion (BIC) | Similar to AIC but with stronger penalty term for number of parameters | $k\\ln(n) - 2\\ln(L)$ where $n$ is number of observations | Also used for model selection; tends to favor simpler models compared to AIC |\n"
      ],
      "id": "9de385df"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}