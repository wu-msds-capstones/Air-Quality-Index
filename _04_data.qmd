---
title: "Data"
format: html
#editor: visual
jupyter: python3
execute:
  echo: true
  output: true
---

```{python}
from sqlalchemy import create_engine, text
import dotenv
import datetime
import time
import os
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import missingno as msno
from summarytools import dfSummary
import sweetviz as sv
from ydata_profiling import ProfileReport

#%pip install ydata-profiling
#%pip install missingno
```

# Exploratory Data Analysis (EDA) - Initial

We have a new dataset named metro_1mil.csv. This file was created using a SQL statement that joins all relevant tables, filtering for metropolitan areas with populations less than or equal to 1 million. This approach limits our EDA to mid-sized metropolitan cities, such as Portland, Oregon.

```{python}
df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')
```

```{python}
print(df.head(10))
```

# Describe Dataset numerical features
```{python}
print(df.describe(include=['float64', 'int64']))
```

# Describe Dataset Objects features
```{python}
print(df.describe(include=['object']))
```

# Size of the dataframe: rows, columns
```{python}
num_rows, num_columns = df.shape
print(f"The dataset contains {num_rows} rows and {num_columns} columns.")
```

# To start our EDA, we focus on Portland OR 
```{python}
df = df[df['state'] == 'Oregon']
df.shape[0]
```

# Features Engineering
## We need to have proper datatype for the time series analysis and clean data without NaN values

```{python}
# date is an object, we must convert to datetime for time series analysis
df['date'] = pd.to_datetime(df['date'])
```

# Let's confirm the range of data in our DataFrame
```{python}
# range of the date column
print(f"The date range is from {df['date'].min()} to {df['date'].max()}.")
```

# Let's confirm the datatype is correct!
```{python}
print(df['date'].info())
```


# Unique Values in df
```{python}
for column in df.columns:
    unique_count = df[column].nunique()
    print(f"Number of unique values in column '{column}': {unique_count}")
```

# Check our Dtypes in df
```{python}
print(df.dtypes) # this will return the data type of each column
```

# Check Missing Values by columns in df
```{python}
print(df.isnull().sum().sort_values(ascending=True))
```

As you can see, we have many NaN in our DataFrame, so let's work on it with amputation.

# Amputation

```{python}
#| echo: false
#| error: false
# Calculate the mean of the 'mean_temperature_fahrenheit' column
mean_temp = df['mean_temperature_fahrenheit'].mean()

# Replace missing values in the 'mean_temperature_fahrenheit' column with the calculated mean
df['mean_temperature_fahrenheit'].fillna(mean_temp, inplace=True)

# Calculate the mean of the 'max_temperature_fahrenheit' column
mean_maxtemp = df['max_temperature_fahrenheit'].mean()

# Replace missing values in the 'max_temperature_fahrenheit' column with the calculated mean
df['max_temperature_fahrenheit'].fillna(mean_maxtemp, inplace=True)

# Lets do the same thing for mean_pressure_millibars and max_pressure_millibars
mean_pressure = df['mean_pressure_millibars'].mean()
df['mean_pressure_millibars'].fillna(mean_pressure, inplace=True)

mean_maxpressure = df['max_pressure_millibars'].mean()
df['max_pressure_millibars'].fillna(mean_maxpressure, inplace=True)

# same for the mean_humidity_percent_relative_humidity and max_humidity_percent_relative_humidity
mean_humidity = df['mean_humidity_percent_relative_humidity'].mean()
df['mean_humidity_percent_relative_humidity'].fillna(mean_humidity, inplace=True)

mean_maxhumidity = df['max_humidity_percent_relative_humidity'].mean()
df['max_humidity_percent_relative_humidity'].fillna(mean_maxhumidity, inplace=True)

# same for the mean_wind_speed_mph and max_wind_speed_mph
mean_wind = df['mean_wind_knots'].mean()
df['mean_wind_knots'].fillna(mean_wind, inplace=True)

mean_maxwind = df['max_wind_knots'].mean()
df['max_wind_knots'].fillna(mean_maxwind, inplace=True)

# same for the mean_co_ppm and max_co_ppm
mean_co = df['mean_co_ppm'].mean()
df['mean_co_ppm'].fillna(mean_co, inplace=True)

mean_maxco = df['max_co_ppm'].mean()
df['max_co_ppm'].fillna(mean_maxco, inplace=True)

# same mean_no2_ppb and max_no2_ppb
mean_no2 = df['mean_no2_ppb'].mean()
df['mean_no2_ppb'].fillna(mean_no2, inplace=True)

mean_maxno2 = df['max_no2_ppb'].mean()
df['max_no2_ppb'].fillna(mean_maxno2, inplace=True)

# mean_ozone_ppm and max_ozone_ppm
mean_ozone = df['mean_ozone_ppm'].mean()
df['mean_ozone_ppm'].fillna(mean_ozone, inplace=True)

mean_maxozone = df['max_ozone_ppm'].mean()
df['max_ozone_ppm'].fillna(mean_maxozone, inplace=True)

# mean_so2_ppb and max_so2_ppb
mean_so2 = df['mean_so2_ppb'].mean()
df['mean_so2_ppb'].fillna(mean_so2, inplace=True)

mean_maxso2 = df['max_so2_ppb'].mean()
df['max_so2_ppb'].fillna(mean_maxso2, inplace=True)
```


Missing Values Plot Chart
```{python}
# Generate a color map
cmap = plt.get_cmap('viridis')
colors = cmap(np.linspace(0, 1, df.shape[1]))

# Plot missing values with custom colors
ax = msno.bar(df, color=colors)

# fig size 
plt.figure(figsize=(8, 5))

# Set the title for the plot
plt.title('Missing values in the dataset')

# Show the plot
plt.show()

```

The EDA is completed, but let's check the statistical summary,data distribution sing Sweetviz Report.

```{python}
my_report = sv.analyze(df)
my_report.show_html()
```

Bonus : We have the ydata profiling report as well!

The support to time series can be enabled by passing the parameter tsmode=True to the ProfileReport when its enabled, pandas profiling will try to identify time-dependent features using the feature's autocorrelation, which requires a sorted DataFrame or the definition of the `sortby` parameter.

When a feature is identified as time series will trigger the following changes:
   - the histogram will be replaced by a line plot
   - the feature details will have a new tab with autocorrelation and partial autocorrelation plots
   - two new warnings: `NON STATIONARY` and `SEASONAL` (which indicates that the series may have seasonality)

In cases where the data has multiple entities,  as in this example, where we have different meteorological stations, each station can be interpreted as a time series, its necessary to filter the entities and profile each station separately.

```{python}
# Return the profile per category_id
for group in df.groupby("category_id"):
    # Running 1 profile per station
    profile = ProfileReport(
        group[1],
        minimal=True,
        sortby="date",
        # title=f"Air Quality profiling - Site Num: {group[0]}"
    )

    profile.to_file(f"Ts_Profile_{group[0]}.html")
```

```{python}
profile = ProfileReport(
    group[1],
    tsmode=True,
    sortby="date",
    # title=f"Air Quality profiling - Site Num: {group[0]}"
)

profile.to_file("your_report2.html")
```

We first performed `manual` EDA analysis, and generated automatic EDA packages reports using  the Sweetviz and ydata to further confirm the quality of our data such as unbalance, missing value (NaN).

I have identified the following :