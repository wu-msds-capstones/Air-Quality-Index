---
title: "Data"
format: html
#editor: visual
jupyter: python3
execute:
  echo: true
  output: true
---

# Data Explaination

The data for this project was initially scattered across multiple sources and required significant organization and compilation. The focus of this project is on the air quality in Portland, Oregon, so various data sources were aggregated and processed to compare a variety of air quality indicators.

## Air Quality Data:
Air quality data, specifically AQI values, were obtained from the United States Environmental Protection Agency (EPA) pre-generated data files. The AQI values are calculated daily, based on a variety of factors including criteria gasses and measured pollutant concentrations, and measures how harmful breathing the air is. AQI is classified into one of six categories from ‘good’ to ‘hazardous’, each having long term health effects associated with it. The files were given daily on a county wide basis, separated into different files by year. 

## Meteorological Data:
Historical weather data was also sourced from the EPA database, measured by thousands of weather stations across the country. Measurements tracked include temperature, wind speed, air pressure, and humidity. Temperature is measured in degrees fahrenheit. Wind speed is measured in knots, which are defined as one nautical mile per hour (equivalent to approximately 1.15mph). Wind speed is important in air quality as winds can blow different pollutants around and move and spread wildfires. Pressure is measured in millibars, where 1013.25 millibars is the standard atmospheric pressure (Earth’s pressure at mean sea level). Finally, humidity is measured in percent relative humidity. This is the amount of water vapor in the air as a percentage of the maximum amount of water vapor possible at a given temperature. Humidity can make it more difficult to breathe and sweat, make the air feel hotter than it is, and prevent air pollutants from dispersing as easily. Indoors, high humidity can trap air, leading to the growth of mold and harmful bacteria. This data was given daily by city, separated into different files by year. Measurements were taken hourly, but pre-calculated in the source database, giving an average value over the twenty four hours and a maximum value.

## Pollution Source Data:
Pollution data was again sourced from the EPA database, separated by criteria gasses (CO, NO2, O3, SO2), Toxins (lead), and particulate matter (PM2.5 and PM10). Criteria gas Carbon Monoxide is measured in parts per million, and is especially dangerous as it is both colorless and odorless. CO binds to hemoglobin in the blood, making the transportation of oxygen around the body more difficult. Nitrogen Dioxide is dangerous to breathe in at high levels. It can cause swelling in the throat, burning, reduced oxygenation of body tissues, and fluid build up in the lungs. It is released in many common combustion reactions including in cars, coal plants, and cigarettes. It is measured in parts per billion. Ozone can harm our ability to breathe, especially in older people, children, and people with asthma. It is measured in parts per million. Sulfur Dioxide, measured in parts per billion, can irritate the eyes, mucous membranes, skin, and respiratory tract. Lead is a toxin which can increase the risk of high blood pressure, cardiovascular problems, and complications during pregnancy. While exposure has gone down significantly in the recent decades after use in gasoline, it still remains a dangerous toxin to breathe in. It is measured in micrograms per cubic meter. PM2.5 and PM10 are particulate matter, small inhalable particles with diameters of 2.5 microns or smaller, and 10 microns or smaller respectively. PM2.5 includes all sorts of common particles, metals, and organic compounds. PM10 includes dust, pollen, molds, and other larger (but still very small) particles. Due to the variability of particles included in the PM classification, there are a wide range of negative health impacts that come from breathing in these particles. PM2.5 and PM10 are measured in micrograms per cubic meter.

This data was given daily by city, separated into different files by year. They are sourced from thousands of individual sources, which measure various selections of these pollution sources. Because of the variety of different pollutants being measured, there was a significant amount of missing data, especially from small towns. Measurements were taken hourly, pre-compiled into a daily average and maximum. 

## Transit Data:
Information on motor buses taken from the National Transit Database, produced by the Federal Transit Administration. Includes information of bus systems and ridership by city, separated by year. Data is recorded yearly, encompassing annual totals for information such as number of buses, total revenue, passengers, and miles driven for the respective city transit systems. Information was given in yearly CSVs, separated by the city transit system. For cities with multiple systems, data was combined. Only motorbus data was used, which may not be reflective of cities with other large methods of public transportation, such as the New York subway system.

## Population data:
Data on population and population density sourced from the Simplemaps United States Cities database, which is built from multiple sources including the U.S. Geological Survey and the U.S. Census Bureau. Data is updated as of May 6, 2024, reflecting very up to date information. 

# Data Processing

The data was downloaded in R. For information given in yearly CSV files, data was stacked vertically to include all years in our time frame. In all tables, relevant columns were selected and renamed, reducing the information being brought into our initial SQL database. R was connected and imported to PostgreSQL using the RPostgres package, and used to read, stack, select columns, and rename columns before being written into a PostgreSQL database. 

## Data Organization

Given the raw data available, the table structure was simplified compared to the original data sources. Data was organized in a star schema centered on the air_quality fact table. This table tracks AQI, pollutant, weather and toxin data daily for each location. The first dimension table is the dates table, a serialized list of dates from January 1st, 2015 to December 31st, 2022. Next, we have a locations dimension table, a serialized list of over 1400 cities and towns from around the country. These are labeled by the state, county, and city name, as well as the population and population density, allowing connection to information based on what is given. The aqi_category dimension table is a short list of AQI value categories (Good, Unhealthy, Hazardous, etc.) with their respective AQI value range as minimum and maximum values. Finally, the yearly_transit dimension table gives the information for the transit system of the respective city during the specified year attached in the fact table. This table seems counterproductive to not include the location or year of the specified line or even a reference id, but in keeping with star schema, it was decided that this was the best way to reference this information. Understanding the context of a specified line requires joining the table back to the fact table, and joining the location and date tables to that as well. 

Each table has a unique serialized primary key, and all dimension tables are connected via foreign key. Several additional indexes are included on columns that will be queried often. Finally, constraints have been added to limit unusual or impossible data.

Tracking these identifiers independently allows for accurate analysis of changes over time and across different areas, and allows adding new information should we need to update the database. (Figure 1) illustrates the resulting ERD structure using drawSQL.


Figure 1.
!()[]


```{python}
from sqlalchemy import create_engine, text
import dotenv
import datetime
import time
import os
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import missingno as msno
from summarytools import dfSummary
import sweetviz as sv
from ydata_profiling import ProfileReport

#%pip install ydata-profiling
#%pip install missingno
```

# Exploratory Data Analysis (EDA) - Initial

We have a new dataset named metro_1mil.csv. This file was created using a SQL statement that joins all relevant tables, filtering for metropolitan areas with populations less than or equal to 1 million. This approach limits our EDA to mid-sized metropolitan cities, such as Portland, Oregon.

```{python}
df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')
```

```{python}
print(df.head(10))
```

# Describe Dataset numerical features
```{python}
print(df.describe(include=['float64', 'int64']))
```

# Describe Dataset Objects features
```{python}
print(df.describe(include=['object']))
```

# Size of the dataframe: rows, columns
```{python}
num_rows, num_columns = df.shape
print(f"The dataset contains {num_rows} rows and {num_columns} columns.")
```

# To start our EDA, we focus on Portland OR 
```{python}
df = df[df['state'] == 'Oregon']
df.shape[0]
```

# Features Engineering
## We need to have proper datatype for the time series analysis and clean data without NaN values

```{python}
# date is an object, we must convert to datetime for time series analysis
df['date'] = pd.to_datetime(df['date'])
```

# Let's confirm the range of data in our DataFrame
```{python}
# range of the date column
print(f"The date range is from {df['date'].min()} to {df['date'].max()}.")
```

# Let's confirm the datatype is correct!
```{python}
print(df['date'].info())
```


# Unique Values in df
```{python}
for column in df.columns:
    unique_count = df[column].nunique()
    print(f"Number of unique values in column '{column}': {unique_count}")
```

# Check our Dtypes in df
```{python}
print(df.dtypes) # this will return the data type of each column
```

# Check Missing Values by columns in df
```{python}
print(df.isnull().sum().sort_values(ascending=True))
```

As you can see, we have many NaN in our DataFrame, so let's work on it with amputation.

# Amputation

```{python}
#| echo: false
#| error: false
# Calculate the mean of the 'mean_temperature_fahrenheit' column
mean_temp = df['mean_temperature_fahrenheit'].mean()

# Replace missing values in the 'mean_temperature_fahrenheit' column with the calculated mean
df['mean_temperature_fahrenheit'].fillna(mean_temp, inplace=True)

# Calculate the mean of the 'max_temperature_fahrenheit' column
mean_maxtemp = df['max_temperature_fahrenheit'].mean()

# Replace missing values in the 'max_temperature_fahrenheit' column with the calculated mean
df['max_temperature_fahrenheit'].fillna(mean_maxtemp, inplace=True)

# Lets do the same thing for mean_pressure_millibars and max_pressure_millibars
mean_pressure = df['mean_pressure_millibars'].mean()
df['mean_pressure_millibars'].fillna(mean_pressure, inplace=True)

mean_maxpressure = df['max_pressure_millibars'].mean()
df['max_pressure_millibars'].fillna(mean_maxpressure, inplace=True)

# same for the mean_humidity_percent_relative_humidity and max_humidity_percent_relative_humidity
mean_humidity = df['mean_humidity_percent_relative_humidity'].mean()
df['mean_humidity_percent_relative_humidity'].fillna(mean_humidity, inplace=True)

mean_maxhumidity = df['max_humidity_percent_relative_humidity'].mean()
df['max_humidity_percent_relative_humidity'].fillna(mean_maxhumidity, inplace=True)

# same for the mean_wind_speed_mph and max_wind_speed_mph
mean_wind = df['mean_wind_knots'].mean()
df['mean_wind_knots'].fillna(mean_wind, inplace=True)

mean_maxwind = df['max_wind_knots'].mean()
df['max_wind_knots'].fillna(mean_maxwind, inplace=True)

# same for the mean_co_ppm and max_co_ppm
mean_co = df['mean_co_ppm'].mean()
df['mean_co_ppm'].fillna(mean_co, inplace=True)

mean_maxco = df['max_co_ppm'].mean()
df['max_co_ppm'].fillna(mean_maxco, inplace=True)

# same mean_no2_ppb and max_no2_ppb
mean_no2 = df['mean_no2_ppb'].mean()
df['mean_no2_ppb'].fillna(mean_no2, inplace=True)

mean_maxno2 = df['max_no2_ppb'].mean()
df['max_no2_ppb'].fillna(mean_maxno2, inplace=True)

# mean_ozone_ppm and max_ozone_ppm
mean_ozone = df['mean_ozone_ppm'].mean()
df['mean_ozone_ppm'].fillna(mean_ozone, inplace=True)

mean_maxozone = df['max_ozone_ppm'].mean()
df['max_ozone_ppm'].fillna(mean_maxozone, inplace=True)

# mean_so2_ppb and max_so2_ppb
mean_so2 = df['mean_so2_ppb'].mean()
df['mean_so2_ppb'].fillna(mean_so2, inplace=True)

mean_maxso2 = df['max_so2_ppb'].mean()
df['max_so2_ppb'].fillna(mean_maxso2, inplace=True)
```


Missing Values Plot Chart
```{python}
# Generate a color map
cmap = plt.get_cmap('viridis')
colors = cmap(np.linspace(0, 1, df.shape[1]))

# Plot missing values with custom colors
ax = msno.bar(df, color=colors)

# fig size 
plt.figure(figsize=(8, 5))

# Set the title for the plot
plt.title('Missing values in the dataset')

# Show the plot
plt.show()

```

The EDA is completed, but let's check the statistical summary,data distribution sing Sweetviz Report.

```{python}
my_report = sv.analyze(df)
my_report.show_html()
```

Bonus : We have the ydata profiling report as well!

The support to time series can be enabled by passing the parameter tsmode=True to the ProfileReport when its enabled, pandas profiling will try to identify time-dependent features using the feature's autocorrelation, which requires a sorted DataFrame or the definition of the `sortby` parameter.

When a feature is identified as time series will trigger the following changes:
   - the histogram will be replaced by a line plot
   - the feature details will have a new tab with autocorrelation and partial autocorrelation plots
   - two new warnings: `NON STATIONARY` and `SEASONAL` (which indicates that the series may have seasonality)

In cases where the data has multiple entities,  as in this example, where we have different meteorological stations, each station can be interpreted as a time series, its necessary to filter the entities and profile each station separately.

```{python}
# Return the profile per category_id
for group in df.groupby("category_id"):
    # Running 1 profile per station
    profile = ProfileReport(
        group[1],
        minimal=True,
        sortby="date",
        # title=f"Air Quality profiling - Site Num: {group[0]}"
    )

    profile.to_file(f"Ts_Profile_{group[0]}.html")
```

```{python}
profile = ProfileReport(
    group[1],
    tsmode=True,
    sortby="date",
    # title=f"Air Quality profiling - Site Num: {group[0]}"
)

profile.to_file("your_report2.html")
```

We first performed `manual` EDA analysis, and generated automatic EDA packages reports using  the Sweetviz and ydata to further confirm the quality of our data such as unbalance, missing value (NaN).

I have identified the following :