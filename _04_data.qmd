---
title: "Data"
format: html
#editor: visual
jupyter: python3
execute:
  echo: true
  output: true
---

# Data Explaination
The data for this project was initially scattered across multiple sources and required significant organization and compilation. The focus of this project is on the air quality in Portland, Oregon, so various data sources were aggregated and processed to compare a variety of air quality indicators.

## Air Quality Data:
Air quality data, specifically AQI values, were obtained from the United States Environmental Protection Agency (EPA) pre-generated data files. The AQI values are calculated daily, based on a variety of factors including criteria gasses and measured pollutant concentrations, and measures how harmful breathing the air is. AQI is classified into one of six categories from ‘good’ to ‘hazardous’, each having long term health effects associated with it. The files were given daily on a county wide basis, separated into different files by year. 

## Meteorological Data:
Historical weather data was also sourced from the EPA database, measured by thousands of weather stations across the country. Measurements tracked include temperature, wind speed, air pressure, and humidity. Temperature is measured in degrees fahrenheit. Wind speed is measured in knots, which are defined as one nautical mile per hour (equivalent to approximately 1.15mph). Wind speed is important in air quality as winds can blow different pollutants around and move and spread wildfires. Pressure is measured in millibars, where 1013.25 millibars is the standard atmospheric pressure (Earth’s pressure at mean sea level). Finally, humidity is measured in percent relative humidity. This is the amount of water vapor in the air as a percentage of the maximum amount of water vapor possible at a given temperature. Humidity can make it more difficult to breathe and sweat, make the air feel hotter than it is, and prevent air pollutants from dispersing as easily. Indoors, high humidity can trap air, leading to the growth of mold and harmful bacteria. This data was given daily by city, separated into different files by year. Measurements were taken hourly, but pre-calculated in the source database, giving an average value over the twenty four hours and a maximum value.

## Pollution Source Data:
Pollution data was again sourced from the EPA database, separated by criteria gasses (CO, NO2, O3, SO2), Toxins (lead), and particulate matter (PM2.5 and PM10). Criteria gas Carbon Monoxide is measured in parts per million, and is especially dangerous as it is both colorless and odorless. CO binds to hemoglobin in the blood, making the transportation of oxygen around the body more difficult. Nitrogen Dioxide is dangerous to breathe in at high levels. It can cause swelling in the throat, burning, reduced oxygenation of body tissues, and fluid build up in the lungs. It is released in many common combustion reactions including in cars, coal plants, and cigarettes. It is measured in parts per billion. Ozone can harm our ability to breathe, especially in older people, children, and people with asthma. It is measured in parts per million. Sulfur Dioxide, measured in parts per billion, can irritate the eyes, mucous membranes, skin, and respiratory tract. Lead is a toxin which can increase the risk of high blood pressure, cardiovascular problems, and complications during pregnancy. While exposure has gone down significantly in the recent decades after use in gasoline, it still remains a dangerous toxin to breathe in. It is measured in micrograms per cubic meter. PM2.5 and PM10 are particulate matter, small inhalable particles with diameters of 2.5 microns or smaller, and 10 microns or smaller respectively. PM2.5 includes all sorts of common particles, metals, and organic compounds. PM10 includes dust, pollen, molds, and other larger (but still very small) particles. Due to the variability of particles included in the PM classification, there are a wide range of negative health impacts that come from breathing in these particles. PM2.5 and PM10 are measured in micrograms per cubic meter.

This data was given daily by city, separated into different files by year. They are sourced from thousands of individual sources, which measure various selections of these pollution sources. Because of the variety of different pollutants being measured, there was a significant amount of missing data, especially from small towns. Measurements were taken hourly, pre-compiled into a daily average and maximum. 

## Transit Data:
Information on motor buses taken from the National Transit Database, produced by the Federal Transit Administration. Includes information of bus systems and ridership by city, separated by year. Data is recorded yearly, encompassing annual totals for information such as number of buses, total revenue, passengers, and miles driven for the respective city transit systems. Information was given in yearly CSVs, separated by the city transit system. For cities with multiple systems, data was combined. Only motorbus data was used, which may not be reflective of cities with other large methods of public transportation, such as the New York subway system.

## Population Data:
Data on population and population density sourced from the Simplemaps United States Cities database, which is built from multiple sources including the U.S. Geological Survey and the U.S. Census Bureau. Data is updated as of May 6, 2024, reflecting very up to date information. 

# Data Processing
The data was downloaded in R. For information given in yearly CSV files, data was stacked vertically to include all years in our time frame. In all tables, relevant columns were selected and renamed, reducing the information being brought into our initial SQL database. R was connected and imported to PostgreSQL using the RPostgres package, and used to read, stack, select columns, and rename columns before being written into a PostgreSQL database. 

## Data Organization
Given the raw data available, the table structure was simplified compared to the original data sources. Data was organized in a star schema centered on the air_quality fact table. This table tracks AQI, pollutant, weather and toxin data daily for each location. The first dimension table is the dates table, a serialized list of dates from January 1st, 2015 to December 31st, 2022. Next, we have a locations dimension table, a serialized list of over 1400 cities and towns from around the country. These are labeled by the state, county, and city name, as well as the population and population density, allowing connection to information based on what is given. The aqi_category dimension table is a short list of AQI value categories (Good, Unhealthy, Hazardous, etc.) with their respective AQI value range as minimum and maximum values. 

Finally, the yearly_transit dimension table gives the information for the transit system of the respective city during the specified year attached in the fact table. This table seems counterproductive to not include the location or year of the specified line or even a reference id, but in keeping with star schema, it was decided that this was the best way to reference this information. Understanding the context of a specified line requires joining the table back to the fact table, and joining the location and date tables to that as well. 

Each table has a unique serialized primary key, and all dimension tables are connected via foreign key. Several additional indexes are included on columns that will be queried often. Finally, constraints have been added to limit unusual or impossible data.

Tracking these identifiers independently allows for accurate analysis of changes over time and across different areas, and allows adding new information should we need to update the database. (Figure 1) illustrates the resulting ERD structure using drawSQL.


Figure 1.  

![ERD Diagram](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/aqi_erd_final.png?raw=true)


```{python}
#| label: import-package
from sqlalchemy import create_engine, text
import dotenv
import datetime
import time
import os
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import missingno as msno
from summarytools import dfSummary
import sweetviz as sv
from ydata_profiling import ProfileReport
```

# Initial Exploratory Data Analysis (EDA)
We have a new dataset named metro_1mil.csv. This file was created using a SQL statement that joins all relevant tables, filtering for metropolitan areas with populations less than or equal to 1 million. This approach limits our EDA to mid-sized metropolitan cities, such as Portland, Oregon.

```{python}
#| label: read-data
#| message: false
#| cache: true
df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')
```

```{python}
#| message: false
#| output-location: slide
print(df.head(10))
```

# Describe Dataset Numerical Features
```{python}
print(df.describe(include=['float64', 'int64']))
```

# Describe Dataset Objects Features
```{python}
print(df.describe(include=['object']))
```

# Dataframe Size: rows, columns
```{python}
num_rows, num_columns = df.shape
print(f"The dataset contains {num_rows} rows and {num_columns} columns.")
```

# Exploring Oregon: Initiating Our Exploratory Data Analysis by filtering
```{python}
df = df[df['state'] == 'Oregon']
df.shape[0]
```

# Features Engineering
## Data Preparation: Ensuring Proper Datatypes and Handling Missing Values for Time Series Analysis
```{python}
# date is an object, we must convert to datetime for time series analysis
df['date'] = pd.to_datetime(df['date'])
```

# Validating Data Range: Examining the Temporal Scope of Our DataFram
```{python}
# range of the date column
print(f"The date range is from {df['date'].min()} to {df['date'].max()}.")
```

# Verifying Data Types: Ensuring Correct Format for Analysis
```{python}
print(df['date'].info())
```

# Identifying Distinct Values: Analyzing Uniqueness in the Datase
```{python}
for column in df.columns:
    unique_count = df[column].nunique()
    print(f"Number of unique values in column '{column}': {unique_count}")
```

# Examining Data Structure: Assessing Column Datatypes in the DataFrame
```{python}
print(df.dtypes)
```

# Check Missing Values by columns in df
```{python}
print(df.isnull().sum().sort_values(ascending=True))
```

# Data Quality Assessment: Addressing Missing Values through Imputation Techniques

```{python}
#| echo: false
#| error: false
# List of columns to impute
columns_to_impute = [
    'mean_temperature_fahrenheit', 'max_temperature_fahrenheit',
    'mean_pressure_millibars', 'max_pressure_millibars',
    'mean_humidity_percent_relative_humidity', 'max_humidity_percent_relative_humidity',
    'mean_wind_knots', 'max_wind_knots',
    'mean_co_ppm', 'max_co_ppm',
    'mean_no2_ppb', 'max_no2_ppb',
    'mean_ozone_ppm', 'max_ozone_ppm',
    'mean_so2_ppb', 'max_so2_ppb'
]

# Calculate means and fill missing values
means = {col: df[col].mean() for col in columns_to_impute}
df = df.assign(**{col: df[col].fillna(means[col]) for col in columns_to_impute})
```


# Comprehensive Data Overview: Generating Statistical Summary and Distribution Analysis with Sweetviz.

```{python}
my_report = sv.analyze(df)
my_report.show_notebook()
```

# Advanced Exploratory Data Analysis: Leveraging ydata-profiling for Time Series Insights

## Introduction to Enhanced EDA Tool
We're employing ydata-profiling, a powerful automatic EDA package that offers more detailed analysis than Sweetviz.
Enabling Time Series Analysis To unlock time series-specific features in ydata-profiling:

- Set tsmode=True when creating the ProfileReport
- Ensure your DataFrame is sorted or specify the sortby parameter
- Time Series Feature Identification

The ydata identifies time-dependent features using autocorrelation analysis. For recognized time series features:
- Histograms are replaced with line plots
- Feature details include new autocorrelation and partial autocorrelation plots
- Two additional warnings may appear: "NON STATIONARY" and "SEASONAL"

Handling Multi-Entity Time Series Data, In our case, with category_id:

Each pollutants represents a distinct time series
For optimal analysis, we filter and profile each pollutant separately

```{python}
# Return the profile per category_id
for group in df.groupby("category_id"):
    # Running 1 profile per station
    profile = ProfileReport(
        group[1],
        minimal=True,
        sortby="date",
        # title=f"Air Quality profiling - Site Num: {group[0]}"
    )

    profile.to_file(f"Ts_Profile_{group[0]}.html")
```

```{python}
profile = ProfileReport(
    group[1],
    tsmode=True,
    sortby="date",
    # title=f"Air Quality profiling - Site Num: {group[0]}"
)

profile.to_file("your_report2.html")
```

# Persisting Data: Exporting DataFrame for Quarto Cross-Document Reference
```{python}
df.to_pickle('df.pkl')
```

We first performed `manual` EDA analysis, and generated automatic EDA packages reports using  the Sweetviz and ydata to further confirm the quality of our data such as unbalance, missing value (NaN).