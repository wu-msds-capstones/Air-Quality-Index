{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Result Machine Learning\"\n",
        "format: html\n",
        "#editor: visual\n",
        "jupyter: python3\n",
        "execute:\n",
        "  echo: true\n",
        "  output: true\n",
        "---\n",
        "\n",
        "# Machine Learning using SKlearn\n",
        "Ultimately, we want to see which variables have the greatest impact on AQI. To do this we must perform a machine learning analysis and create a prediction algorithm. \n"
      ],
      "id": "226f945b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint"
      ],
      "id": "79b1525f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Starting by looking at the head to see all of our columns. Immediately, we can some missing data, which must be addressed before any machine learning algorithms can be conducted.\n",
        "\n",
        "## Import the Data"
      ],
      "id": "bdd2c2a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil.csv')"
      ],
      "id": "00eb0a9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(df)"
      ],
      "id": "3b0755df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see all the missing data in each column. There are some very large numbers we must address. The entire dataset is only 147039 rows long. Some columnshave more than half the data missing.\n"
      ],
      "id": "870493ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Check out missing data\n",
        "df.isna().sum()"
      ],
      "id": "30d415ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first steps to addressing the missing data will be to drop the columns that will definitely not be needed. This includes the id columns used for joining within the SQL database, as well as the maximum columns as we already have the mean data. We also rename the columns to make it easier to understand and reference.\n"
      ],
      "id": "bca26d0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Drop columns definitely not needed, no ids, no maximums\n",
        "met = df[[\n",
        "    'date',\n",
        "    'state',\n",
        "    'county',\n",
        "    'city',\n",
        "    'population',\n",
        "    'density',\n",
        "    'aqi',\n",
        "    'category',\n",
        "    'mean_temperature_fahrenheit',\n",
        "    'mean_pressure_millibars',\n",
        "    'mean_humidity_percent_relative_humidity',\n",
        "    'mean_wind_knots',\n",
        "    'mean_co_ppm',\n",
        "    'mean_no2_ppb',\n",
        "    'mean_ozone_ppm',\n",
        "    'mean_so2_ppb',\n",
        "    'mean_pm100_micrograms_per_cubic_meter',\n",
        "    'mean_pm25_micrograms_per_cubic_meter',\n",
        "    'mean_lead_micrograms_per_cubic_meter',\n",
        "    'num_busses',\n",
        "    'revenue',\n",
        "    'operating_expense',\n",
        "    'passenger_trips',\n",
        "    'operating_hours',\n",
        "    'passenger_miles',\n",
        "    'operating_miles'\n",
        "    ]]\n",
        "\n",
        "#rename columns to be easier to reference\n",
        "met = met.rename(columns={'mean_temperature_fahrenheit': 'temp', \n",
        "                          'mean_pressure_millibars': 'pressure', \n",
        "                          'mean_humidity_percent_relative_humidity': 'humidity',\n",
        "                          'mean_wind_knots': 'wind_speed', \n",
        "                          'mean_co_ppm': 'co', \n",
        "                          'mean_no2_ppb': 'no2',\n",
        "                          'mean_ozone_ppm': 'o3', \n",
        "                          'mean_so2_ppb': 'so2', \n",
        "                          'mean_pm100_micrograms_per_cubic_meter': 'pm100',\n",
        "                          'mean_pm25_micrograms_per_cubic_meter': 'pm25', \n",
        "                          'mean_lead_micrograms_per_cubic_meter': 'lead'})"
      ],
      "id": "a29349ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the missing data looks like this, it is a little better, but we can still do so much more.\n"
      ],
      "id": "b8ca1dd2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#See missing data\n",
        "met.isna().sum()"
      ],
      "id": "64d04677",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will group data by week. This will take an average count for the entire week for each variable for each city. This will eliminate any missing data that was not captured everyday, but only some days. For example, the pollutants (PM10 and PM2.5) were captured once every three days. \n"
      ],
      "id": "f5861fd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Set datatime type and index\n",
        "met['date'] = pd.to_datetime(met['date'])\n",
        "met.set_index('date', inplace=True)\n",
        "#group by week, drop lead for too much missing data\n",
        "met = met.groupby([pd.Grouper(freq='W'), 'state', 'county', 'city']).agg({\n",
        "    'population': 'first',\n",
        "    'density': 'first',\n",
        "    'aqi': 'mean',\n",
        "    'temp': 'mean',\n",
        "    'pressure': 'mean',\n",
        "    'humidity': 'mean',\n",
        "    'wind_speed': 'mean',\n",
        "    'co': 'mean',\n",
        "    'no2': 'mean',\n",
        "    'o3': 'mean',\n",
        "    'so2': 'mean',\n",
        "    'pm100': 'mean',\n",
        "    'pm25': 'mean',\n",
        "    'num_busses': 'mean',\n",
        "    'revenue': 'mean',\n",
        "    'operating_expense': 'mean',\n",
        "    'passenger_trips': 'mean',\n",
        "    'operating_hours': 'mean',\n",
        "    'passenger_miles': 'mean',\n",
        "    'operating_miles': 'mean'\n",
        "}).reset_index()"
      ],
      "id": "06630b50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see that the city Virginia Beach has almost no data, so it will be easiest to drop that city. \n"
      ],
      "id": "eb8a07d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#drop virginia beach due to large amount of missing data\n",
        "met = met[met['city'] != 'Virginia Beach']"
      ],
      "id": "d079a05c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, since we will be measuring and predicting AQI, we will drop all rows missing AQI data.\n"
      ],
      "id": "8278dd07"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#drop null aqi\n",
        "met = met.dropna(subset=['aqi'])"
      ],
      "id": "15034baf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Missing data is better, but there is still work to do.\n"
      ],
      "id": "c5f41176"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Check missing data\n",
        "met.isna().sum()"
      ],
      "id": "f56b7475",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data collected has separate information for the city of New York City. NYC is divided into five borroughs, each within its own county. We will group and average out these values to make NYC have the same amount of datapoints as every other city. This will also address data present in some New York borroughs but not others. \n"
      ],
      "id": "d0f6866b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#group nyc together into one city, ignoring county\n",
        "nyc = met[met['city'] == 'New York']\n",
        "columns = ['date', 'aqi', 'temp', 'pressure','humidity',\n",
        "           'wind_speed','co','no2','o3',\n",
        "           'so2','pm100','pm25', 'num_busses',\n",
        "           'revenue', 'operating_expense', \n",
        "           'passenger_trips', 'operating_hours', \n",
        "           'passenger_miles', 'operating_miles']\n",
        "\n",
        "nyc = nyc[columns].groupby('date').mean().reset_index()\n",
        "#Add data that was dropped\n",
        "nyc['state'] = 'New York'\n",
        "nyc['county'] = 'Multiple'\n",
        "nyc['city'] = 'New York City'\n",
        "nyc['population'] = 18908608\n",
        "nyc['density'] = 11080.3\n",
        "\n",
        "nyc = nyc[['date', 'state', 'county', 'city', 'population', \n",
        "     'density', 'aqi', 'temp', 'pressure','humidity',\n",
        "     'wind_speed','co','no2','o3', 'so2','pm100',\n",
        "     'pm25', 'num_busses', 'revenue', 'operating_expense', \n",
        "     'passenger_trips', 'operating_hours', \n",
        "     'passenger_miles', 'operating_miles']]\n",
        "\n",
        "print(nyc)"
      ],
      "id": "be445078",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#replace existing nyc data in met dataframe\n",
        "met = met[met['city'] != 'New York']\n",
        "met = pd.concat([met, nyc], ignore_index=True)"
      ],
      "id": "885037d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, Kansas City is located in two states, and thus in two counties. We will do the same thing to contract and standardize the data.\n"
      ],
      "id": "74e6fb8f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Do same with Kansas City\n",
        "kc = met[met['city'] == 'Kansas City']\n",
        "columns = ['date', 'aqi', 'temp', 'pressure','humidity',\n",
        "           'wind_speed','co','no2','o3',\n",
        "           'so2','pm100','pm25', 'num_busses',\n",
        "           'revenue', 'operating_expense', \n",
        "           'passenger_trips', 'operating_hours', \n",
        "           'passenger_miles', 'operating_miles']\n",
        "\n",
        "kc = kc[columns].groupby('date').mean().reset_index()\n",
        "\n",
        "columns = ['temp', 'pressure','humidity',\n",
        "           'wind_speed','co','no2','o3',\n",
        "           'so2','pm100','pm25', 'num_busses',\n",
        "           'revenue', 'operating_expense', \n",
        "           'passenger_trips', 'operating_hours', \n",
        "           'passenger_miles', 'operating_miles']\n",
        "met[columns] = met[columns].fillna(met.groupby('city')[columns].transform('mean'))\n",
        "\n",
        "kc['state'] = 'Missouri'\n",
        "kc['county'] = 'Multiple'\n",
        "kc['city'] = 'Kansas City'\n",
        "kc['population'] = 1689556\n",
        "kc['density'] = 620.7\n",
        "\n",
        "kc = kc[['date', 'state', 'county', 'city', 'population', \n",
        "     'density', 'aqi', 'temp', 'pressure','humidity',\n",
        "     'wind_speed','co','no2','o3', 'so2','pm100',\n",
        "     'pm25', 'num_busses', 'revenue', 'operating_expense', \n",
        "     'passenger_trips', 'operating_hours', \n",
        "     'passenger_miles', 'operating_miles']]\n",
        "\n",
        "met = met[met['city'] != 'Kansas City']\n",
        "met = pd.concat([met, kc], ignore_index=True)"
      ],
      "id": "285acae0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After doing this, we drop the rest of the rows with missing data, leaving us with a total of 17 cities with metropolitan area populations greater than one million.\n"
      ],
      "id": "99807d5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#count cities after dropping na rows, left with 17 cities\n",
        "ds = met.dropna()\n",
        "\n",
        "ds.groupby('city').size().reset_index(name='count')"
      ],
      "id": "ce8b1889",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform a ML prediction algorithm, the predicted variable (AQI) must be discrete. To achieve this, we bin AQI data into discrete groups. Initially, we thought to bin them based on the existing AQI categories, but found that most of the data is grouped in the sub 100 range. Therefore, we expanded the bins, focusing the prediction on outcomes in the double digits. The bins chosen are shown below:\n"
      ],
      "id": "eeb35124"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#bin AQI data for ML\n",
        "bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 150, 500]\n",
        "\n",
        "labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "ds.head()"
      ],
      "id": "bf6a0258",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "ds.head()"
      ],
      "id": "a7037430",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year\n",
        "ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month"
      ],
      "id": "5dc4055c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To set up the ML prediction, we must specify the variable being predicted (aqi_discrete, the aqi separated into bins), and the independent prediction variables. We will use a feature selector to select the most accurate predictors to try to optimize the model. Therefore, we initially choose every other variable to be our independent variables.\n"
      ],
      "id": "f2815e71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#dependent variable aqi, all others (except aqi category) as independent variables\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[['city',\n",
        "        'population',\n",
        "        'density',\n",
        "        'temp',\n",
        "        'pressure',\n",
        "        'humidity',\n",
        "        'wind_speed',\n",
        "        'co',\n",
        "        'no2',\n",
        "        'o3',\n",
        "        'so2',\n",
        "        'pm100',\n",
        "        'pm25',\n",
        "        'num_busses', \n",
        "        'revenue', \n",
        "        'operating_expense', \n",
        "        'passenger_trips', \n",
        "        'operating_hours', \n",
        "        'passenger_miles', \n",
        "        'operating_miles',\n",
        "        'year',\n",
        "        'month'\n",
        "        ]]"
      ],
      "id": "5251a479",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We must split the data into training and testing datasets. We also define an encoder to transform the discrete predictors into many binary variables for each category.\n"
      ],
      "id": "658ce615"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Split into train and test datasets, define model and encoder\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')"
      ],
      "id": "dfe18ce1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A transformer is used to quickly transform variables into the necessary form for modeling.\n"
      ],
      "id": "e01dde48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#define transformer with encoder and standardscaler\n",
        "\n",
        "transformer = ColumnTransformer(\n",
        "        [\n",
        "            ('categories', encoder, ['city','year','month']),\n",
        "            ('scaled_air_quality', StandardScaler(), [\n",
        "                'population',\n",
        "                'density',\n",
        "                'temp',\n",
        "                'pressure',\n",
        "                'humidity',\n",
        "                'wind_speed',\n",
        "                'co',\n",
        "                'no2',\n",
        "                'o3',\n",
        "                'so2',\n",
        "                'pm100',\n",
        "                'pm25',\n",
        "                'num_busses', \n",
        "                'revenue', \n",
        "                'operating_expense', \n",
        "                'passenger_trips', \n",
        "                'operating_hours', \n",
        "                'passenger_miles', \n",
        "                'operating_miles'\n",
        "                ]\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop', verbose_feature_names_out=False)\n",
        "#fit transformer\n",
        "transformer.fit(X_train, y_train)"
      ],
      "id": "52d2711c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature selection is done on the data. From this we are given the following variables: City, Temperature, Humidity, Carbon Monoxide, Nitrogen Dioxide, Ozone, PM10, and PM2.5.\n"
      ],
      "id": "4c25bc5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#feature selection\n",
        "feature_selector = SelectKBest(k=10)\n",
        "X_train_trans = transformer.transform(X_train)\n",
        "X_train_trans_df = pd.DataFrame(\n",
        "    X_train_trans, \n",
        "    columns = transformer.get_feature_names_out(),\n",
        "    index = X_train.index)\n",
        "feature_selector.fit(X_train_trans_df, y_train)\n",
        "feature_selector.get_support()\n",
        "feature_selector.scores_[feature_selector.get_support()]\n",
        "X_train_trans_df.columns[feature_selector.get_support()]"
      ],
      "id": "feeb3e61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using these selected features, we will run our models.\n"
      ],
      "id": "8646835d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Use selected features\n",
        "y = ds['aqi_discrete']\n",
        "X = ds[[\n",
        "    'city', 'temp', 'humidity', 'co', 'no2', 'o3', 'pm100', 'pm25'\n",
        "    ]]\n",
        "#Create train/test data with selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "X_train_enc = pd.DataFrame(encoder.fit_transform(X_train[['city']]),\n",
        "                           columns = encoder.get_feature_names_out(), index = X_train.index)\n",
        "X_train_trans = X_train[[\n",
        "    'temp', 'humidity', 'co', 'no2','o3', 'pm100', 'pm25'\n",
        "    ]].merge(X_train_enc, left_index = True, right_index = True)\n",
        "X_test_enc = pd.DataFrame(encoder.fit_transform(X_test[['city']]),\n",
        "                           columns = encoder.get_feature_names_out(), index = X_test.index)\n",
        "X_test_trans = X_test[[\n",
        "    'temp', 'humidity', 'co', 'no2','o3', 'pm100', 'pm25'\n",
        "    ]].merge(X_test_enc, left_index = True, right_index = True)"
      ],
      "id": "9a3b8526",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will test various models with default parameters to see which is the most accurate at predicting AQI from this dataset. The five models selected are: K nearest neighbors, Tree model, Random Forest model, Logistic Regression, and Naive Bayes.Running the models shows us that the Random Forest model is the most accurate.\n"
      ],
      "id": "22de904b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#model selection\n",
        "model1 = KNeighborsClassifier(n_neighbors=5)\n",
        "model2 = tree.DecisionTreeClassifier()\n",
        "model3 = RandomForestClassifier(n_estimators=10, random_state=12)\n",
        "model4 = LogisticRegression()\n",
        "model5 = GaussianNB()\n",
        "\n",
        "for model, label in zip([model1, model2, model3, model4, model5], ['KNN', 'Tree', 'Random Forest', 'Logistic', 'naive Bayes']):\n",
        "    model.fit(X_train_trans, y_train)\n",
        "    y_pred = model.predict(X_test_trans)\n",
        "    print(\"Model: \", label)\n",
        "    print(\"Cohen Kappa Score: \", cohen_kappa_score(y_test, y_pred))\n",
        "    print(\"Accuracy: \", sum(y_test == y_pred)/len(y_test))\n",
        "    print(\"\\n\")"
      ],
      "id": "d701236c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running a basic Random Forest model gives us the following cohen kappa score and accuracy. This will serve as the baseline to be compared to.\n"
      ],
      "id": "f918a981"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Create model from most accurate, RandomForest\n",
        "model = RandomForestClassifier(n_estimators=10)\n",
        "model.fit(X_train_trans, y_train)\n",
        "\n",
        "#predict on test data\n",
        "y_pred = model.predict(X_test_trans)\n",
        "\n",
        "#Print results\n",
        "print(\"cohen kappa score: \", cohen_kappa_score(y_test, y_pred))\n",
        "print(\"accuracy: \", sum(y_test == y_pred)/len(y_test))\n",
        "ConfusionMatrixDisplay(\n",
        "    confusion_matrix = confusion_matrix(\n",
        "        y_test, y_pred, \n",
        "        labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "        ), \n",
        "        display_labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "        ).plot(xticks_rotation='vertical')"
      ],
      "id": "3d0dcdc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will further simplify this with a transformer to be used in a pipeline.\n"
      ],
      "id": "26a4a625"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#redefined transformer\n",
        "transformer = ColumnTransformer(\n",
        "        [\n",
        "            ('categories', encoder, ['city']),\n",
        "            ('scaled_air_quality', StandardScaler(), [\n",
        "                'temp',\n",
        "                'humidity',\n",
        "                'co',\n",
        "                'no2',\n",
        "                'o3',\n",
        "                'pm100',\n",
        "                'pm25'\n",
        "                ]\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop', verbose_feature_names_out=False)\n",
        "#Pipeline for simplification\n",
        "\n",
        "classification_pipeline = Pipeline([('aqi_transformer', transformer),\n",
        "                                    ('RF_model', RandomForestClassifier())\n",
        "                                    ])\n",
        "classification_pipeline.fit(X_train, y_train)"
      ],
      "id": "512ebece",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will optimize hyperparameters to make the model more accurate. \n"
      ],
      "id": "017b64e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#hyperparameter optimization\n",
        "classification_pipeline.get_params()"
      ],
      "id": "1d2b0cda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following parameters are selected, each with a range of possible values. A large 100 iterations are run to find the highest score.\n"
      ],
      "id": "95face29"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameters = {\n",
        "    'aqi_transformer__categories__max_categories': randint(3,30),\n",
        "    'aqi_transformer__categories__min_frequency': randint(2,20),\n",
        "    'RF_model__max_depth': randint(3, 30),\n",
        "    'RF_model__max_features': [None, 'sqrt', 'log2'],\n",
        "    'RF_model__min_samples_leaf': randint(2, 10),\n",
        "    'RF_model__min_samples_split': randint(2, 10),\n",
        "    'RF_model__n_estimators': randint(50, 200),\n",
        "    'RF_model__min_samples_split': randint(2, 10),\n",
        "    'RF_model__bootstrap': [True, False]\n",
        "}\n",
        "n_iter_search = 100\n",
        "random_search = RandomizedSearchCV(classification_pipeline, param_distributions=parameters,\n",
        "                                   n_iter=n_iter_search, n_jobs=-1, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(random_search.best_score_)"
      ],
      "id": "33541cc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see which hyperparameters are selected.\n"
      ],
      "id": "52ff4c44"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "random_search.best_estimator_.get_params()"
      ],
      "id": "c743e095",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using these hyperparameters, we reach an accuracy of about 63%.\n"
      ],
      "id": "75973e55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred=random_search.predict(X_test)\n",
        "print(\"cohen kappa score: \", cohen_kappa_score(y_test, y_pred))\n",
        "print(\"accuracy: \", sum(y_pred == y_test)/len(y_test))\n",
        "ConfusionMatrixDisplay(\n",
        "    confusion_matrix = confusion_matrix(\n",
        "        y_test, y_pred, \n",
        "        labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "        ), \n",
        "        display_labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']\n",
        "        ).plot(xticks_rotation='vertical')"
      ],
      "id": "a24729c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this model for the prediction of AQI, exploring the features that predict it, and use that to generate conclusions for what particles to reduce or systems to increase.\n",
        "\n",
        "\n",
        "#  ML Time Series with SARIMAX\n",
        "## Decomposing the Time Series Additive Method"
      ],
      "id": "72cc0199"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: read-data\n",
        "import pandas as pd\n",
        "df = pd.read_pickle('/data/df.pkl')"
      ],
      "id": "read-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head()"
      ],
      "id": "3cc06758",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_aqi = df[['date','aqi']]\n",
        "df_aqi = df_aqi.set_index('date')\n",
        "\n",
        "df_aqi = df_aqi.resample('ME').mean()\n",
        "df_aqi.ffill(inplace=True)\n",
        "df_aqi.plot(figsize=(15, 6))"
      ],
      "id": "e20e7154",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: additive-decomp\n",
        "from matplotlib import pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "decomposition = sm.tsa.seasonal_decompose(df_aqi, model='additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ],
      "id": "additive-decomp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decomposing the Time Series Multiplicative Method"
      ],
      "id": "e41e5a6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: multiplicative-decomp\n",
        "from matplotlib import pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "decomposition = sm.tsa.seasonal_decompose(df_aqi, model='multiplicative')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ],
      "id": "multiplicative-decomp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Prediction"
      ],
      "id": "407d9c38"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: grid-search\n",
        "import statsmodels.api as sm\n",
        "import itertools\n",
        "\n",
        "# Define the p, d, q parameters to take any value between 0 and 2\n",
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "\n",
        "# Define the seasonal p, d, q parameters to take any value between 0 and 2, and the seasonality period (e.g., 12 for monthly data)\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in pdq]\n",
        "\n",
        "# Iterate over all combinations of pdq and seasonal_pdq\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            mod = sm.tsa.statespace.SARIMAX(df_aqi,\n",
        "                                            order=param,\n",
        "                                            seasonal_order=param_seasonal,\n",
        "                                            enforce_stationarity=False,\n",
        "                                            enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
        "        except Exception as e:\n",
        "            print(f\"Error with parameters {param} and {param_seasonal}: {e}\")\n",
        "            continue"
      ],
      "id": "grid-search",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mod-fit\n",
        "mod = sm.tsa.statespace.SARIMAX(df_aqi,order=(1, 1, 1),seasonal_order=(0,1, 1, 12),enforce_stationarity=False,enforce_invertibility=False)\n",
        "results = mod.fit()\n",
        "print(results.summary().tables[1])"
      ],
      "id": "mod-fit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fit-summary-AIC\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Fit the SARIMAX model\n",
        "mod = sm.tsa.statespace.SARIMAX(df_aqi, order=(1, 1, 1), seasonal_order=(0, 1, 1, 12), enforce_stationarity=False, enforce_invertibility=False)\n",
        "results = mod.fit()\n",
        "\n",
        "# Print the summary which includes AIC\n",
        "print(results.summary())\n",
        "\n",
        "# Extract AIC value\n",
        "aic_value = results.aic\n",
        "print(f\"AIC: {aic_value}\")\n",
        "\n",
        "# Additional performance metrics can be calculated as needed"
      ],
      "id": "fit-summary-AIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plot-diag\n",
        "results.plot_diagnostics(figsize=(15, 12))"
      ],
      "id": "plot-diag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train and Test\n",
        "The final critical phase in our research methodology is the rigorous validation and empirical testing of the newly developed model. To ensure the robustness and generalizability of our results, it is imperative to implement a careful data partitioning strategy. Specifically, we will employ a train-test split methodology to mitigate the risk of data leakage, which could otherwise lead to overly optimistic performance estimates and compromise the validity of our findings.\n",
        "By segregating our dataset into distinct training and testing subsets, we create an environment that closely simulates real-world scenarios where the model encounters previously unseen data. This approach serves multiple crucial purposes:\n",
        "\n",
        "- *Model Validation*: It allows us to assess the model's performance on an independent dataset, providing a more reliable estimate of its predictive capabilities in practical applications.\n",
        "- *Overfitting Detection*: By comparing performance metrics on both training and testing sets, we can identify potential overfitting issues where the model may have learned noise or idiosyncrasies specific to the training data.\n",
        "Generalization Assessment: The test set serves as a proxy for new, unseen data, enabling us to evaluate the model's ability to generalize beyond the specific instances it was trained on.\n",
        "Reliability Quantification: Through this approach, we can compute confidence intervals and other statistical measures to quantify the reliability and stability of our model's predictions.\n",
        "Iterative Refinement: The insights gained from this validation process can inform potential model adjustments or hyperparameter tuning, leading to improved performance and robustness.\n",
        "\n",
        "This methodological step is crucial in establishing the credibility and practical utility of our predictive model, ensuring that our research conclusions are both scientifically sound and applicable in real-world contexts.\n",
        "\n",
        "To split the data, we follow the recommended 70:30 ratio, 70% of the data is the training data, and 30% of the data is the testing data."
      ],
      "id": "dc4defa2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check the minimum date in the 'date' column\n",
        "print(f\"Start date of the data:\", df_aqi.index.min())"
      ],
      "id": "bbe937a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check the max date in the 'date' column\n",
        "print(f\"End date of the data:\", df_aqi.index.max())"
      ],
      "id": "5d971019",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pred = results.get_prediction(start=pd.to_datetime('2016-01-01'), dynamic=False)\n",
        "pred_ci = pred.conf_int()"
      ],
      "id": "b0e2687f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = df_aqi['2015-01-31 00:00:00':].plot(label='Observed') # plot the observed data \n",
        "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n",
        "\n",
        "ax.fill_between(pred_ci.index,pred_ci.iloc[:, 0],pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
        "\n",
        "ax.set_xlabel('Days')\n",
        "ax.set_ylabel('AQI index')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "7091bac4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "y_forecasted = pred.predicted_mean\n",
        "y_truth = df_aqi['2022-12-31 00:00:00':]"
      ],
      "id": "ca14a015",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = np.sqrt(mean_squared_error(y_truth, y_forecasted))   \n",
        "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
      ],
      "id": "b1ffb99f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pred_uc = results.get_forecast(steps=7)\n",
        "pred_ci = pred_uc.conf_int()"
      ],
      "id": "833c83c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = df_aqi.plot(label='Observed', figsize=(14, 7))\n",
        "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
        "ax.fill_between(pred_ci.index,pred_ci.iloc[:, 0],pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
        "ax.set_xlabel('Days')\n",
        "ax.set_ylabel('AQI index')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "5a866e1e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}