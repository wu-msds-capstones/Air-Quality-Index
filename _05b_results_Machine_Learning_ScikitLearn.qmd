# Results Machine Learning Scikit-Learn
Ultimately, we want to see which variables have the greatest impact on AQI. To do this we perform a machine learning analysis and create a prediction algorithm. As the AQI is defined by the four criteria gasses and particulate matter (PM10 and PM2.5), those should not be included in the algorithm. Thus we start with all the other variables.

```{python}
#| echo: false
#| label: import-library

import numpy as np
import pandas as pd
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sklearn.metrics import classification_report
```

```{python}
#| include: false 
#| label: import-data
df = pd.read_csv('https://raw.githubusercontent.com/wu-msds-capstones/Air-Quality-Index/main/data/metro_1mil_v2.csv')
```

```{python}
#| include: false
len(df)
df.head()
```

```{python}
#| label: met-dataframe
#| echo: false

met = df[[
    'date',
    'state',
    'county',
    'city',
    'population',
    'density',
    'city_area_km2',
    'park_acreage',
    'aqi',
    'category',
    'mean_temperature_fahrenheit',
    'mean_pressure_millibars',
    'mean_humidity_percent_relative_humidity',
    'mean_wind_knots',
    'mean_co_ppm',
    'mean_no2_ppb',
    'mean_ozone_ppm',
    'mean_so2_ppb',
    'mean_pm100_micrograms_per_cubic_meter',
    'mean_pm25_micrograms_per_cubic_meter',
    'mean_lead_micrograms_per_cubic_meter',
    'num_busses',
    'revenue',
    'operating_expense',
    'passenger_trips',
    'operating_hours',
    'passenger_miles',
    'operating_miles',
    'cattle',
    'hogs',
    'sheep'
    ]]

met = met.rename(columns={'city_area_km2': 'area',
                          'mean_temperature_fahrenheit': 'temp', 
                          'mean_pressure_millibars': 'pressure', 
                          'mean_humidity_percent_relative_humidity': 'humidity',
                          'mean_wind_knots': 'wind_speed', 
                          'mean_co_ppm': 'co', 
                          'mean_no2_ppb': 'no2',
                          'mean_ozone_ppm': 'o3', 
                          'mean_so2_ppb': 'so2', 
                          'mean_pm100_micrograms_per_cubic_meter': 'pm100',
                          'mean_pm25_micrograms_per_cubic_meter': 'pm25', 
                          'mean_lead_micrograms_per_cubic_meter': 'lead'})
```


```{python}
#| include: false
#| label: check-NaN

met.isna().sum()
```

```{python}
#| include: false
#| label: set-index-ts

met['date'] = pd.to_datetime(met['date'])
met.set_index('date', inplace=True)
#group by week, drop lead for too much missing data
met = met.groupby([pd.Grouper(freq='W'), 'state', 'county', 'city']).agg({
    'population': 'first',
    'density': 'first',
    'area': 'first',
    'park_acreage': 'first',
    'aqi': 'mean',
    'temp': 'mean',
    'pressure': 'mean',
    'humidity': 'mean',
    'wind_speed': 'mean',
    'co': 'mean',
    'no2': 'mean',
    'o3': 'mean',
    'so2': 'mean',
    'pm100': 'mean',
    'pm25': 'mean',
    'num_busses': 'mean',
    'revenue': 'mean',
    'operating_expense': 'mean',
    'passenger_trips': 'mean',
    'operating_hours': 'mean',
    'passenger_miles': 'mean',
    'operating_miles': 'mean',
    'cattle': 'first',
    'hogs': 'first',
    'sheep': 'first'
}).reset_index()
```

```{python}
#| include: false
#| label: drop-missNaN-col
met = met[met['city'] != 'Virginia Beach']
```

```{python}
#| echo: false
#| label: drop-null-aqi
met = met.dropna(subset=['aqi'])
```

```{python}
#| include: false
#| label: check-NaN-again
met.isna().sum()
```

```{python}
#| include: false
#| label: group-nyc-1-city
nyc = met[met['city'] == 'New York']
columns = ['date', 'area', 'park_acreage', 'aqi',
           'temp', 'pressure','humidity',
           'wind_speed','co','no2','o3',
           'so2','pm100','pm25', 'num_busses',
           'revenue', 'operating_expense', 
           'passenger_trips', 'operating_hours', 
           'passenger_miles', 'operating_miles',
           'cattle', 'hogs', 'sheep']

nyc = nyc[columns].groupby('date').mean().reset_index()
#Add data that was dropped
nyc['state'] = 'New York'
nyc['county'] = 'Multiple'
nyc['city'] = 'New York City'
nyc['population'] = 18908608
nyc['density'] = 11080.3

nyc = nyc[['date', 'state', 'county', 'city', 'population', 
     'density', 'area', 'park_acreage','aqi', 
     'temp', 'pressure','humidity',
     'wind_speed','co','no2','o3', 'so2','pm100',
     'pm25', 'num_busses', 'revenue', 'operating_expense', 
     'passenger_trips', 'operating_hours', 
     'passenger_miles', 'operating_miles',
     'cattle', 'hogs', 'sheep']]

print(nyc)
```

```{python}
#| echo: false
#| label: replace-nyc-in-met
met = met[met['city'] != 'New York']
met = pd.concat([met, nyc], ignore_index=True)
```

```{python}
#| echo: false
#| label: dataframe-Kansas-City
#Do same with Kansas City
kc = met[met['city'] == 'Kansas City']
columns = ['date', 'area', 'park_acreage', 'aqi',
           'temp', 'pressure','humidity',
           'wind_speed','co','no2','o3',
           'so2','pm100','pm25', 'num_busses',
           'revenue', 'operating_expense', 
           'passenger_trips', 'operating_hours', 
           'passenger_miles', 'operating_miles',
           'cattle', 'hogs', 'sheep']


kc = kc[columns].groupby('date').mean().reset_index()

columns = ['date', 'area', 'park_acreage', 'aqi',
           'temp', 'pressure','humidity',
           'wind_speed','co','no2','o3',
           'so2','pm100','pm25', 'num_busses',
           'revenue', 'operating_expense', 
           'passenger_trips', 'operating_hours', 
           'passenger_miles', 'operating_miles',
           'cattle', 'hogs', 'sheep']

met[columns] = met[columns].fillna(met.groupby('city')[columns].transform('mean'))

kc['state'] = 'Missouri'
kc['county'] = 'Multiple'
kc['city'] = 'Kansas City'
kc['population'] = 1689556
kc['density'] = 620.7

kc = kc[['date', 'state', 'county', 'city', 'population', 
     'density', 'area', 'park_acreage','aqi', 
     'temp', 'pressure','humidity',
     'wind_speed','co','no2','o3', 'so2','pm100',
     'pm25', 'num_busses', 'revenue', 'operating_expense', 
     'passenger_trips', 'operating_hours', 
     'passenger_miles', 'operating_miles',
     'cattle', 'hogs', 'sheep']]

met = met[met['city'] != 'Kansas City']
met = pd.concat([met, kc], ignore_index=True)
```

```{python}
#| include: false
#| label: count-cities
ds = met.dropna()

ds.groupby('city').size().reset_index(name='count')
```

```{python}
#| include: false
#| label: AQI-bin-ML
#bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 150, 500]
#labels = ['0-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-150', '151+']

bins = [0, 50, 100, 150, 200, 300, 500]
labels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-500']

ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)

ds.head()
```

```{python}
#| echo: false
#| warning: false
#| label: convert-year-month-datetime
ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year
ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month
```

After data is cleaned, we are left with a total of eighteen cities across the country with a total metropolitan area population of greater than one million. To perform the ML prediction algorithm, AQI will be predicted. We will use existing AQI cateogories as classifiers. A feature selector is run on the set of all variables except for the six that define AQI (CO, NO2, O3, SO2, PM10, PM2.5). This chooses the best predictors of the dependent variable AQI.

```{python}
#| echo: false
#| label: aqi-X-y
#dependent variable aqi, all others (except aqi category and aqi definition vars) as independent variables
y = ds['aqi_discrete']
X = ds[['city',
        'population',
        'density',
        'area',
        'park_acreage',
        'temp',
        'pressure',
        'humidity',
        'wind_speed',
        'num_busses', 
        'revenue', 
        'operating_expense', 
        'passenger_trips', 
        'operating_hours', 
        'passenger_miles', 
        'operating_miles',
        'cattle',
        'hogs',
        'sheep',
        'year',
        'month'
        ]]

```

```{python}
#| echo: false
#| label: split-train
#Split into train and test datasets, define model and encoder
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')
```

```{python}
#| include: false
#| label: def-transformer
#define transformer with encoder and standardscaler

transformer = ColumnTransformer(
        [
            ('categories', encoder, ['city','year','month']),
            ('scaled_air_quality', StandardScaler(), [
                'population',
                'density',
                'area',
                'park_acreage',
                'temp',
                'pressure',
                'humidity',
                'wind_speed',
                'num_busses', 
                'revenue', 
                'operating_expense', 
                'passenger_trips', 
                'operating_hours', 
                'passenger_miles', 
                'operating_miles',
                'cattle',
                'hogs',
                'sheep'
                ]
            )
        ],
        remainder='drop', verbose_feature_names_out=False)
#fit transformer
transformer.fit(X_train, y_train)
```

Features Selected in Initial Model:

- City
- Month
- Population
- City Area
- Park Area
- Temperature
- Humidity
- Cattle


```{python}
#| include: false
#| label: feature-select
#feature selection
feature_selector = SelectKBest(k=10)
X_train_trans = transformer.transform(X_train)
X_train_trans_df = pd.DataFrame(
    X_train_trans, 
    columns = transformer.get_feature_names_out(),
    index = X_train.index)
feature_selector.fit(X_train_trans_df, y_train)
feature_selector.get_support()
feature_selector.scores_[feature_selector.get_support()]
X_train_trans_df.columns[feature_selector.get_support()]
```



```{python}
#| echo: false
#| label: use-features
#Use selected features
y = ds['aqi_discrete']
X = ds[[
    'city', 'month', 'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'
    ]]
#Create train/test data with selected features
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
X_train_enc = pd.DataFrame(encoder.fit_transform(X_train[['city','month']]),
                           columns = encoder.get_feature_names_out(), index = X_train.index)
X_train_trans = X_train[[
    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'
    ]].merge(X_train_enc, left_index = True, right_index = True)
X_test_enc = pd.DataFrame(encoder.fit_transform(X_test[['city','month']]),
                           columns = encoder.get_feature_names_out(), index = X_test.index)
X_test_trans = X_test[[
    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'
    ]].merge(X_test_enc, left_index = True, right_index = True)
```

```{python}
#| include: false
#| warning: false
#| label: model-select
model1 = tree.DecisionTreeClassifier()
model2 = RandomForestClassifier(n_estimators=10, random_state=12)
model3 = LogisticRegression()
model4 = GaussianNB()

results = []

for model, label in zip([model1, model2, model3, model4], ['Tree', 'Random Forest', 'Logistic', 'Naive Bayes']):
    model.fit(X_train_trans, y_train)
    y_pred = model.predict(X_test_trans)
    model = label
    cohen_kappa = cohen_kappa_score(y_test, y_pred)
    accuracy = sum(y_test == y_pred)/len(y_test)
    
    results.append({
        'Model': label,
        'Cohen Kappa Score': cohen_kappa,
        'Accuracy': accuracy
    })

display(pd.DataFrame(results))
```

In pre testing, it was found that a Random Forest Model performed the best on this dataset. Therefore, this model type is used in all models.

```{python}
#| include: false
#| label: def-best-model
#Create model from most accurate, RandomForest
model = RandomForestClassifier(n_estimators=10)
model.fit(X_train_trans, y_train)

#predict on test data
y_pred = model.predict(X_test_trans)
```

An initial Random Forest Model run with the selected features returns the following Cohen Kappa score and accuracy:
```{python}
#| echo: false
#| label: CKS-Accuracy
#Print results
print("Cohen Kappa Score: ", cohen_kappa_score(y_test, y_pred))
print("Accuracy: ", sum(y_test == y_pred)/len(y_test))
```

```{python}
#| label: confus-matrix-1
#| include: false
#| output: false
ConfusionMatrixDisplay(
    confusion_matrix = confusion_matrix(
        y_test, y_pred, 
        labels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-500']
        ), 
        display_labels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-500']
        ).plot(xticks_rotation='vertical')
```

![Figure 11: Confusion Matrix For Bin Size 1](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/confusion_matrix_1.png?raw=true)

Looking only at Cohen Kappa score and accuracy, this seems like a good prediction model. However, the confusion matrix shows the data's skew, centered around the 0-100 range. Therefore, more bins and bin size combinations are tested in order to provide a more meaningful prediction.

```{python}
#| include: false
#| label: bins-2


bins = [0, 25, 50, 75, 100, 200, 500]
labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']

ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)

ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year
ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month

#dependent variable aqi, all others (except aqi category and aqi definition vars) as independent variables
y = ds['aqi_discrete']
X = ds[['city',
        'population',
        'density',
        'area',
        'park_acreage',
        'temp',
        'pressure',
        'humidity',
        'wind_speed',
        'num_busses', 
        'revenue', 
        'operating_expense', 
        'passenger_trips', 
        'operating_hours', 
        'passenger_miles', 
        'operating_miles',
        'cattle',
        'hogs',
        'sheep',
        'year',
        'month'
        ]]

#Split into train and test datasets, define model and encoder
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')

#define transformer with encoder and standardscaler

transformer = ColumnTransformer(
        [
            ('categories', encoder, ['city','year','month']),
            ('scaled_air_quality', StandardScaler(), [
                'population',
                'density',
                'area',
                'park_acreage',
                'temp',
                'pressure',
                'humidity',
                'wind_speed',
                'num_busses', 
                'revenue', 
                'operating_expense', 
                'passenger_trips', 
                'operating_hours', 
                'passenger_miles', 
                'operating_miles',
                'cattle',
                'hogs',
                'sheep'
                ]
            )
        ],
        remainder='drop', verbose_feature_names_out=False)
#fit transformer
transformer.fit(X_train, y_train)
```

```{python}
#| include: false
#| label: feature-select-2
#feature selection
feature_selector = SelectKBest(k=10)
X_train_trans = transformer.transform(X_train)
X_train_trans_df = pd.DataFrame(
    X_train_trans, 
    columns = transformer.get_feature_names_out(),
    index = X_train.index)
feature_selector.fit(X_train_trans_df, y_train)
feature_selector.get_support()
feature_selector.scores_[feature_selector.get_support()]
X_train_trans_df.columns[feature_selector.get_support()]
```

```{python}
#| echo: false
#| label: use-features-2
#Use selected features
y = ds['aqi_discrete']
X = ds[[
    'city', 'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'
    ]]
#Create train/test data with selected features
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
X_train_enc = pd.DataFrame(encoder.fit_transform(X_train[['city']]),
                           columns = encoder.get_feature_names_out(), index = X_train.index)
X_train_trans = X_train[[
    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'
    ]].merge(X_train_enc, left_index = True, right_index = True)
X_test_enc = pd.DataFrame(encoder.fit_transform(X_test[['city']]),
                           columns = encoder.get_feature_names_out(), index = X_test.index)
X_test_trans = X_test[[
    'population', 'area', 'park_acreage', 'temp', 'humidity', 'cattle'
    ]].merge(X_test_enc, left_index = True, right_index = True)
```

```{python}
#| include: false
#| label: def-best-model-2
#Create model from most accurate, RandomForest
model = RandomForestClassifier(n_estimators=10)
model.fit(X_train_trans, y_train)

#predict on test data
y_pred = model.predict(X_test_trans)

print("Cohen Kappa Score: ", cohen_kappa_score(y_test, y_pred))
print("Accuracy: ", sum(y_test == y_pred)/len(y_test))
```

```{python}
#| label: confus-matrix2
#| include: false
#| output: false
ConfusionMatrixDisplay(
    confusion_matrix = confusion_matrix(
        y_test, y_pred, 
        labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']
        ), 
        display_labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']
        ).plot(xticks_rotation='vertical')
```

After further testing the following bins were decided on:

- 0-25
- 26-50
- 51-75
- 76-100
- 101-200
- 201-500

As bins are made smaller, predictions become far less accurate. This bin size serves as a good balance between having enough bins to generate conclusions and maintaining decent accuracy. Different bin sizes also lead to different features being selected. The following features will be present in the final model:

- City
- Population
- City Area
- Park Area
- Temperature
- Humidity
- Cattle

```{python}
#| include: false
#| label: redo-bins-2


bins = [0, 25, 50, 75, 100, 200, 500]
labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']

ds.loc[:,'aqi_discrete'] = pd.cut(ds['aqi'], bins=bins, labels=labels, right=True)

ds.loc[:,'year'] = pd.to_datetime(ds['date']).dt.year
ds.loc[:,'month'] = pd.to_datetime(ds['date']).dt.month

#dependent variable aqi, all others (except aqi category and aqi definition vars) as independent variables
y = ds['aqi_discrete']
X = ds[['city',
        'population',
        'area',
        'park_acreage',
        'temp',
        'humidity',
        'cattle',
        ]]

#Split into train and test datasets, define model and encoder
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
encoder = OneHotEncoder(sparse_output=False, min_frequency=5, handle_unknown='infrequent_if_exist')
```

```{python}
#| include: false
#| label: improve-transformer
#redefined transformer

transformer = ColumnTransformer(
        [
            ('categories', encoder, ['city']),
            ('scaled_air_quality', StandardScaler(), [
                'population', 
                'area', 
                'park_acreage', 
                'temp', 
                'humidity', 
                'cattle'
                ]
            )
        ],
        remainder='drop', verbose_feature_names_out=False)
#Pipeline for simplification

classification_pipeline = Pipeline([('aqi_transformer', transformer),
                                    ('RF_model', RandomForestClassifier())
                                    ])
classification_pipeline.fit(X_train, y_train)
```

Hyperparameter optimization will be done to further improve the model. A randomized search is run with 100 iterations. The following hyperparameters are optimized:

- Max Categories
- Min Frequency
- Max Depth
- Max Features
- Min Samples Leaf
- Min Samples Split
- Num Estimators
- Bootstrap

```{python}
#| include: false
#| label: hyper-parameters-optim
#hyperparameter optimization
classification_pipeline.get_params()
```

```{python}
#| include: false
#| label: def-parameters
parameters = {
    'aqi_transformer__categories__max_categories': randint(3,30),
    'aqi_transformer__categories__min_frequency': randint(2,20),
    'RF_model__max_depth': randint(3, 30),
    'RF_model__max_features': [None, 'sqrt', 'log2'],
    'RF_model__min_samples_leaf': randint(2, 10),
    'RF_model__min_samples_split': randint(2, 10),
    'RF_model__n_estimators': randint(50, 200),
    'RF_model__bootstrap': [True, False]
}
n_iter_search = 100
random_search = RandomizedSearchCV(classification_pipeline, param_distributions=parameters,
                                   n_iter=n_iter_search, n_jobs=-1, cv=5)
random_search.fit(X_train, y_train)
print(random_search.best_score_)
```
Table 1 below shows the selected hyperparameters.

```{python}
#| echo: false
#| label: randsearch-get-params
random_search.best_params_
#random_search.best_estimator_.get_params()
params_df = pd.DataFrame(list(random_search.best_params_.items()), columns=['Parameter', 'Value'])

# Print the DataFrame
display(params_df)
```
Table 2 below shows the selected hyperparameters having hyperparamenter optimization

Using these hyperparameters, we reach an accuracy of over 70%.

```{python}
#| echo: false
#| warning: false
#| label: predict-X_test-final
y_pred=random_search.predict(X_test)
print("Cohen Kappa Score: ", cohen_kappa_score(y_test, y_pred))
print("Accuracy: ", sum(y_pred == y_test)/len(y_test))
```

```{python}
#| echo: false
#| output: false
#| label: conf-matrix-final
ConfusionMatrixDisplay(
    confusion_matrix = confusion_matrix(
        y_test, y_pred, 
        labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']
        ), 
        display_labels = ['0-25', '26-50', '51-75', '76-100', '101-200', '201-500']
        ).plot(xticks_rotation='vertical')
```
![Figure 12: Final Model Confusion Matrix](https://github.com/wu-msds-capstones/Air-Quality-Index/blob/main/images/confusion_matrix_final.png?raw=true)

We can use this model for the prediction of AQI, exploring the features that predict it, and use that to generate conclusions for what variables or systems should be addressed.

Below, the table 3 shows a classification report of the model. Support represents the amount of data points total in that bin. We can see that AQI groups with more data tend to be more accurate (higher f1-score). This makes sense as the model has more of this data to train on. To make the model more accurate, more data from AQI values outside this 26-200 range will need to be collected.

```{python}
#| echo: false
#| warning: false
#| label: classification-report
report = classification_report(y_test, y_pred, target_names=classification_pipeline.classes_, output_dict=True)

df_report = pd.DataFrame(report).transpose()

df_report = df_report[0:6]
df_report = df_report.reindex(['0-25', '26-50', '51-75', '76-100', '101-200', '201-500'])
display(df_report)
```
Table 3: Final Model Classification Report)

We can also take a look at which features end up being most influential in the model. Below figure 13 shows Temperature and Humidity being by far the most influential variables in the model. However, temperature and humidity are not variables we can directly impact easily, and therefore, the four other variables are where we should put our efforts.

```{python}
#| echo: false
#| warning: false
#| label: feature-importance

features = pd.DataFrame(
   dict(
      variable = model.feature_names_in_,
      importance = model.feature_importances_
   )
)

features = features[~features['variable'].str.contains('city_', case=False, na=False)]
features = features.sort_values(by='importance', ascending=False)


features.replace({
    'population': 'Population', 
    'area': 'Area', 
    'park_acreage': 'Park Land' ,
    'temp': 'Temp',
    'humidity': 'Humidity',
    'cattle': 'Cattle'
    }, inplace=True)


plt.bar(features.loc[:,"variable"], features.loc[:,"importance"], color='skyblue')

plt.title('Most Important Features')
plt.ylabel('Feature Importance')

plt.show()
```
Figure 13: Feature Importance of Selected Variables in Final Model